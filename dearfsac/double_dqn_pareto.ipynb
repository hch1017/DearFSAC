{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # federated arguments\n",
    "    #RL的训练轮次\n",
    "    parser.add_argument('--epochs', type=int, default=500, help=\"rounds of training\")\n",
    "    \n",
    "    #嵌入向量的训练轮次\n",
    "    parser.add_argument('--emb_train_epochs', type=int, default=5, help=\"rounds of training\")\n",
    "    parser.add_argument('--emb', default=True)\n",
    "    \n",
    "    #验证RL和Fedavg哪个更好的验证轮次\n",
    "    parser.add_argument('--validation_epochs', type=int, default=50, help=\"rounds of training\")\n",
    "    parser.add_argument('--divide', default=True)\n",
    "    parser.add_argument('--reset_flag', type=int, default=20, help=\"reset flag\")\n",
    "    \n",
    "    #将训练集分为几份\n",
    "    parser.add_argument('--divide_num', type=int, default=2, help=\"divide number\")\n",
    "    \n",
    "    #有多少个local client\n",
    "    parser.add_argument('--num_users', type=int, default=100, help=\"number of users: K\")\n",
    "    parser.add_argument('--k', type=int, default=10, help=\"k\")\n",
    "    \n",
    "    #每次选多少个local client参与训练\n",
    "    parser.add_argument('--frac', type=float, default=0.1, help=\"the fraction of clients: C\")\n",
    "    parser.add_argument('--train_frac', type=float, default=1, help=\"the fraction of training: C\")\n",
    "    \n",
    "    parser.add_argument('--collect_ep', type=int, default=1000, help=\"rounds of training\")\n",
    "    \n",
    "    #local client自己本地训练的轮次\n",
    "    parser.add_argument('--local_emb_ep', type=int, default=1, help=\"the number of local epochs: E\")\n",
    "    parser.add_argument('--local_ep', type=int, default=1, help=\"the number of local epochs: E\")\n",
    "    parser.add_argument('--local_chosen_ep', type=int, default=2, help=\"the number of local epochs: E\")\n",
    "    \n",
    "    #验证环节的local clinet本地训练轮次\n",
    "    parser.add_argument('--local_validation_ep', type=int, default=1, help=\"the number of local epochs: E\")\n",
    "    \n",
    "    #local client本地训练的batchsize\n",
    "    parser.add_argument('--local_bs', type=int, default=10, help=\"local batch size: B\")\n",
    "    parser.add_argument('--bs', type=int, default=128, help=\"test batch size\")\n",
    "    \n",
    "    #RL的学习率和衰减率\n",
    "    parser.add_argument('--lr', type=float, default=0.01, help=\"learning rate (default: 0.01)\")\n",
    "    parser.add_argument('--lr_decay', type=float, default=1, help=\"lr decay\")\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, help=\"SGD momentum (default: 0.5)\")\n",
    "    parser.add_argument('--split', type=str, default='user', help=\"train-test split type, user or sample\")\n",
    "\n",
    "    # model arguments\n",
    "    \n",
    "    #使用的client 模型\n",
    "    parser.add_argument('--model', type=str, default='cnn', help='model name')\n",
    "    parser.add_argument('--kernel_num', type=int, default=9, help='number of each kind of kernel')\n",
    "    parser.add_argument('--kernel_sizes', type=str, default='3,4,5',\n",
    "                        help='comma-separated kernel size to use for convolution')\n",
    "    parser.add_argument('--norm', type=str, default='batch_norm', help=\"batch_norm, layer_norm, or None\")\n",
    "    parser.add_argument('--num_filters', type=int, default=32, help=\"number of filters for conv nets\")\n",
    "    parser.add_argument('--max_pool', type=str, default='True',\n",
    "                        help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    \n",
    "    #使用的数据集\n",
    "    parser.add_argument('--dataset', type=str, default='mnist', help=\"name of dataset\")\n",
    "    \n",
    "    #数据集的划分是否满足独立同分布\n",
    "    parser.add_argument('--iid', action='store_true', help='whether i.i.d or not')\n",
    "    \n",
    "    #输出的分类个数\n",
    "    parser.add_argument('--num_classes', type=int, default=10, help=\"number of classes\")\n",
    "    \n",
    "    #输入的图片的通道数\n",
    "    parser.add_argument('--num_channels', type=int, default=1, help=\"number of channels of imges\")\n",
    "    parser.add_argument('--gpu', type=int, default=-1, help=\"GPU ID, -1 for CPU\")\n",
    "    parser.add_argument('--stopping_rounds', type=int, default=10, help='rounds of early stopping')\n",
    "    parser.add_argument('--verbose', action='store_true', help='verbose print')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='random seed (default: 1)')\n",
    "    parser.add_argument('--all_clients', action='store_true', help='aggregation over all clients')\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid, mnist_iid_drl_local_divide, cifar_iid_drl_local_divide, mnist_noniid_drl_local_divide\n",
    "from models.Update import LocalUpdate\n",
    "from models.Update_divide import LocalUpdate_divide\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, CNNCifarEmb, CNNCifarEmbReverse, CNNMnistEmb, CNNMnistEmbReverse\n",
    "from models.Fed import FedAvg\n",
    "# from models.test import test_img\n",
    "\n",
    "# parse args\n",
    "args = args_parser()\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.repeat(1,1,1))])\n",
    "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=False, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    args.iid = False\n",
    "    if args.iid:\n",
    "        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    dataset_train = datasets.CIFAR10('./data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('./data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "    args.iid = True\n",
    "    if args.iid:\n",
    "        dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        exit('Error: only consider IID setting in CIFAR10')\n",
    "else:\n",
    "    exit('Error: unrecognized dataset')\n",
    "img_size = dataset_train[0][0].shape\n",
    "\n",
    "# build model\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob = CNNCifar(args=args).to(args.device)\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob = CNNMnist(args=args).to(args.device)\n",
    "elif args.model == 'mlp':\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "print(net_glob)\n",
    "net_glob.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import heapq\n",
    "from torch.autograd import Variable\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.95             # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, parameter_dim, action_dim, args):\n",
    "        super(Net, self).__init__()\n",
    "        self.args = args\n",
    "        self.parameter_dim = parameter_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # N+1 即global的p拼接上所有local的p\n",
    "        self.fc1 = nn.Linear(parameter_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, action_dim)\n",
    "\n",
    "    def forward(self, parameters):\n",
    "        #拼接的p\n",
    "        x = self.fc1(parameters)\n",
    "        q = self.fc2(x)\n",
    "        \n",
    "        #100维\n",
    "        return q\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self, parameter_dim, action_dim, replay_buffer, args):\n",
    "        self.parameter_dim = parameter_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.args = args\n",
    "        self.iter = 0\n",
    "        #self.noise = utils.OrnsteinUhlenbeckActionNoise(self.action_dim)\n",
    "\n",
    "        self.eval_net = Net(self.parameter_dim,  \n",
    "                                 self.action_dim, args).to(args.device)\n",
    "        self.target_net = Net(self.parameter_dim, \n",
    "                                 self.action_dim, args).to(args.device)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), 0.01)\n",
    "#         self.loss_func = F.smooth_l1_loss\n",
    "         \n",
    "    #选q最大的local，直接赋值给global，只用一个local参与训练的目的是加快训练\n",
    "    def choose_action_train(self, parameters):\n",
    "        q = self.eval_net.forward(parameters)\n",
    "        action = q.detach().numpy()[0][0].tolist().index(max(q.detach().numpy()[0][0]))\n",
    "        #这是索引\n",
    "        print(action)\n",
    "        return action\n",
    "    \n",
    "    #选q最大的10个local，再将它们的q用softmax变成权值\n",
    "    def choose_action_run(self, parameters):\n",
    "        q = self.eval_net.forward(parameters)\n",
    "        #l是q的列表形式\n",
    "        l = q.detach().numpy()[0][0].tolist()\n",
    "        \n",
    "        #最大的k个q值\n",
    "        topk = heapq.nlargest(int(self.args.frac * self.args.num_users), l)\n",
    "        \n",
    "        #找索引\n",
    "        choice = []\n",
    "        for i in topk:\n",
    "            choice.append(l.index(i))\n",
    "        \n",
    "        #算权值\n",
    "        topk_sum = np.sum(topk)\n",
    "        for i in range(len(topk)):\n",
    "            topk[i] /= topk_sum\n",
    "        action = topk\n",
    "        \n",
    "        #softmax\n",
    "#         action = F.softmax(topk)\n",
    "        print(choice)\n",
    "        return choice, action\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Samples a random batch from replay memory and performs optimization\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        s1,a1,r1,s2 = self.replay_buffer.sample(10)\n",
    "#         print(s1)\n",
    "#         print(a1)\n",
    "#         print(r1)\n",
    "        s1 = Variable(s1, requires_grad=True)\n",
    "        r1 = Variable(r1, requires_grad=True)\n",
    "        s2 = Variable(s2, requires_grad=True)\n",
    "        \n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net.forward(s1) # shape (batch, 1)\n",
    "#         a = Variable(torch.from_numpy(np.array([a1]))).type(torch.LongTensor)\n",
    "#         q_eval = q_eval.gather(0,a).squeeze()\n",
    "        a1 = a1.tolist()    \n",
    "        q_eval_list = q_eval.tolist()[0]\n",
    "        q_eval = []\n",
    "        for i in range(len(q_eval_list)):\n",
    "            q = 0\n",
    "            for j in range(len(a1[i])):\n",
    "                q += q_eval_list[i][int(a1[i][j])]/10000\n",
    "            q_eval.append([q/len(a1[i])])\n",
    "        q_eval = torch.Tensor(q_eval)\n",
    "#         print(q_eval)\n",
    "        \n",
    "        \n",
    "        q_next = self.eval_net.forward(s2).detach()     # detach from graph, don't backpropagate\n",
    "#         print(q_next)\n",
    "        q_next_list = q_next.tolist()[0]\n",
    "        a_next = []\n",
    "        for i in range(len(q_next_list)):\n",
    "            a = []\n",
    "            topk = heapq.nlargest(int(self.args.frac * self.args.num_users), q_next_list[i])\n",
    "            for j in range(len(topk)):\n",
    "                tmp = q_next_list[i].index(topk[j])\n",
    "                a.append(tmp)\n",
    "            a_next.append(a)\n",
    "        \n",
    "        q_target_next = self.target_net(s2).detach()\n",
    "        q_target_next_list = q_target_next.tolist()[0]\n",
    "        q_target_next = []\n",
    "        for i in range(len(q_target_next_list)):\n",
    "            q = 0\n",
    "            for j in range(len(a_next[i])):\n",
    "                q += q_target_next_list[i][a_next[i][j]]/10000\n",
    "            q_target_next.append([q/len(a_next[i])])\n",
    "        q_target_next = torch.Tensor(q_target_next)\n",
    "#         print(q_target_next)\n",
    "        \n",
    "        q_target = r1 + GAMMA * q_target_next  # shape (batch, 1)\n",
    "        loss = F.smooth_l1_loss(q_eval.float(), q_target.float())\n",
    "\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "#         self.soft_update(self.target_net, self.eval_net, 0.001)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.autograd import Variable\n",
    "# x = torch.Tensor(range(10))\n",
    "# print(F.softmax(x,0))\n",
    "# a = Variable(torch.from_numpy(np.array([5]))).type(torch.LongTensor)\n",
    "# print(a)\n",
    "# print(a.unsqueeze(1))\n",
    "# x.gather(0,a).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer: # MemoryBuffer类实现的功能：buffer内采样，往buffer里塞（sars）\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size) #buffer设置为双端队列\n",
    "        self.maxSize = size\n",
    "        self.len = 0\n",
    "        \n",
    "    def state_reco(self, s):\n",
    "        s_1 = [i[0] for i in s]\n",
    "        s_2 = [i[1] for i in s]\n",
    "        s_3 = [i[2] for i in s]\n",
    "        return [torch.cat(s_1),torch.cat(s_2),torch.cat(s_3)]\n",
    "\n",
    "    def sample(self, count):\n",
    "        \"\"\"\n",
    "        samples a random batch from the replay memory buffer\n",
    "        :param count: batch size\n",
    "        :return: batch (numpy array)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "        count = min(count, self.len)\n",
    "        batch = random.sample(self.buffer, count) # 随机取样\n",
    "\n",
    "        s_arr = torch.cat([arr[0] for arr in batch],1)\n",
    "        a_arr = torch.tensor([arr[1] for arr in batch])\n",
    "        r_arr = torch.tensor([arr[2] for arr in batch]).reshape(-1,1)\n",
    "        s1_arr = torch.cat([arr[3] for arr in batch],1)\n",
    "\n",
    "        return s_arr, a_arr, r_arr, s1_arr\n",
    "\n",
    "    def len(self):\n",
    "        return self.len\n",
    "\n",
    "    def add(self, s, a, r, s1):\n",
    "        \"\"\"\n",
    "        adds a particular transaction in the memory buffer\n",
    "        :param s: current state\n",
    "        :param a: action taken\n",
    "        :param r: reward received\n",
    "        :param s1: next state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        reset_flag = 40\n",
    "        add_count = 0\n",
    "        transition = (s,a,r,s1)\n",
    "        self.len += 1\n",
    "        if self.len > self.maxSize:\n",
    "            self.len = self.maxSize\n",
    "        self.buffer.append(transition)\n",
    "        add_count += 1\n",
    "        if add_count == reset_flag:\n",
    "            self.buffer = deque(maxlen=size)\n",
    "            add_count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100\n"
     ]
    }
   ],
   "source": [
    "#每个local的每一层在emb之后拼接起来，再乘以(100+1)，或者用均值。分别对应是400和100\n",
    "parameter_dim = (args.num_users+1) * 100\n",
    "action_dim = args.num_users\n",
    "print(parameter_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss_avg:0.14283256232738495\n",
      "epoch:0, loss_avg:0.4421205520629883\n",
      "epoch:0, loss_avg:0.1828140765428543\n",
      "epoch:0, loss_avg:0.2194962501525879\n",
      "epoch:0, loss_avg:0.167791485786438\n",
      "epoch:0, loss_avg:0.11470726877450943\n",
      "epoch:0, loss_avg:0.21228577196598053\n",
      "epoch:0, loss_avg:0.8082790970802307\n",
      "epoch:0, loss_avg:0.4383396506309509\n",
      "epoch:0, loss_avg:0.19843563437461853\n",
      "epoch:0, loss_avg:0.17731745541095734\n",
      "epoch:0, loss_avg:0.05347681790590286\n",
      "epoch:0, loss_avg:0.06839808076620102\n",
      "epoch:0, loss_avg:0.05507805198431015\n",
      "epoch:0, loss_avg:0.061597757041454315\n",
      "epoch:0, loss_avg:0.04792385920882225\n",
      "epoch:0, loss_avg:0.06587247550487518\n",
      "epoch:0, loss_avg:0.06981831043958664\n",
      "epoch:0, loss_avg:0.0743120014667511\n",
      "epoch:0, loss_avg:0.09482083469629288\n",
      "epoch:0, loss_avg:0.06391963362693787\n",
      "epoch:0, loss_avg:0.06765797734260559\n",
      "epoch:0, loss_avg:0.0837058424949646\n",
      "epoch:0, loss_avg:0.08891676366329193\n",
      "epoch:0, loss_avg:0.05015251040458679\n",
      "epoch:0, loss_avg:0.051111020147800446\n",
      "epoch:0, loss_avg:0.048951491713523865\n",
      "epoch:0, loss_avg:0.0644632875919342\n",
      "epoch:0, loss_avg:0.07336557656526566\n",
      "epoch:0, loss_avg:0.5125558376312256\n",
      "epoch:0, loss_avg:0.04035063087940216\n",
      "epoch:0, loss_avg:0.07103350758552551\n",
      "epoch:0, loss_avg:0.07619195431470871\n",
      "epoch:0, loss_avg:0.04416346922516823\n",
      "epoch:0, loss_avg:0.06047811359167099\n",
      "epoch:0, loss_avg:0.06206127256155014\n",
      "epoch:0, loss_avg:0.06467551738023758\n",
      "epoch:0, loss_avg:0.2682734727859497\n",
      "epoch:0, loss_avg:0.0589897483587265\n",
      "epoch:0, loss_avg:0.05768132209777832\n",
      "epoch:0, loss_avg:0.061246369034051895\n",
      "epoch:0, loss_avg:0.05062942951917648\n",
      "epoch:0, loss_avg:0.04479258134961128\n",
      "epoch:0, loss_avg:0.055596694350242615\n",
      "epoch:0, loss_avg:0.055869802832603455\n",
      "epoch:0, loss_avg:0.05118143931031227\n",
      "epoch:0, loss_avg:0.060586024075746536\n",
      "epoch:0, loss_avg:0.07113666087388992\n",
      "epoch:0, loss_avg:0.06573950499296188\n",
      "epoch:0, loss_avg:0.04214499145746231\n",
      "epoch:0, loss_avg:0.04905792325735092\n",
      "epoch:0, loss_avg:0.04526090621948242\n",
      "epoch:0, loss_avg:0.08195757120847702\n",
      "epoch:0, loss_avg:0.06326545774936676\n",
      "epoch:0, loss_avg:0.060349106788635254\n",
      "epoch:0, loss_avg:0.05976245552301407\n",
      "epoch:0, loss_avg:0.05140009522438049\n",
      "epoch:0, loss_avg:0.05175158753991127\n",
      "epoch:0, loss_avg:0.04977263882756233\n",
      "epoch:0, loss_avg:0.06001044809818268\n",
      "epoch:0, loss_avg:0.06865355372428894\n",
      "epoch:0, loss_avg:0.049497298896312714\n",
      "epoch:0, loss_avg:0.08940389007329941\n",
      "epoch:0, loss_avg:0.0658639669418335\n",
      "epoch:0, loss_avg:0.06219482421875\n",
      "epoch:0, loss_avg:0.04218490421772003\n",
      "epoch:0, loss_avg:0.05162878707051277\n",
      "epoch:0, loss_avg:0.059497371315956116\n",
      "epoch:0, loss_avg:0.050346143543720245\n",
      "epoch:0, loss_avg:0.05336124449968338\n",
      "epoch:0, loss_avg:0.05105315148830414\n",
      "epoch:0, loss_avg:0.05324605479836464\n",
      "epoch:0, loss_avg:0.071361243724823\n",
      "epoch:0, loss_avg:0.07590717822313309\n",
      "epoch:0, loss_avg:0.05376718193292618\n",
      "epoch:0, loss_avg:0.059314608573913574\n",
      "epoch:0, loss_avg:0.06246226280927658\n",
      "epoch:0, loss_avg:0.06193205714225769\n",
      "epoch:0, loss_avg:0.03697381541132927\n",
      "epoch:0, loss_avg:0.06409095972776413\n",
      "epoch:0, loss_avg:0.060380857437849045\n",
      "epoch:0, loss_avg:0.0767652839422226\n",
      "epoch:0, loss_avg:0.048701368272304535\n",
      "epoch:0, loss_avg:0.05222715437412262\n",
      "epoch:0, loss_avg:0.052154622972011566\n",
      "epoch:0, loss_avg:0.06088770925998688\n",
      "epoch:0, loss_avg:0.056894443929195404\n",
      "epoch:0, loss_avg:0.048801593482494354\n",
      "epoch:0, loss_avg:0.08836186677217484\n",
      "epoch:0, loss_avg:0.04143355041742325\n",
      "epoch:0, loss_avg:0.09893816709518433\n",
      "epoch:0, loss_avg:0.07024648785591125\n",
      "epoch:0, loss_avg:0.0709683895111084\n",
      "epoch:0, loss_avg:0.05126478523015976\n",
      "epoch:0, loss_avg:0.06360040605068207\n",
      "epoch:0, loss_avg:0.05205991491675377\n",
      "epoch:0, loss_avg:0.049597129225730896\n",
      "epoch:0, loss_avg:0.058923158794641495\n",
      "epoch:0, loss_avg:0.06375272572040558\n",
      "epoch:0, loss_avg:0.06979189068078995\n",
      "epoch:1, loss_avg:0.04462029039859772\n",
      "epoch:1, loss_avg:0.06477382034063339\n",
      "epoch:1, loss_avg:0.055205050855875015\n",
      "epoch:1, loss_avg:0.08960288763046265\n",
      "epoch:1, loss_avg:0.08141720294952393\n",
      "epoch:1, loss_avg:0.050941694527864456\n",
      "epoch:1, loss_avg:0.05860335752367973\n",
      "epoch:1, loss_avg:0.040981605648994446\n",
      "epoch:1, loss_avg:0.09477810561656952\n",
      "epoch:1, loss_avg:0.0568365678191185\n",
      "epoch:1, loss_avg:0.051671598106622696\n",
      "epoch:1, loss_avg:0.05696007236838341\n",
      "epoch:1, loss_avg:0.08358161151409149\n",
      "epoch:1, loss_avg:0.05558179318904877\n",
      "epoch:1, loss_avg:0.07004706561565399\n",
      "epoch:1, loss_avg:0.045193105936050415\n",
      "epoch:1, loss_avg:0.06048886105418205\n",
      "epoch:1, loss_avg:0.07978594303131104\n",
      "epoch:1, loss_avg:0.042429715394973755\n",
      "epoch:1, loss_avg:0.0533284991979599\n",
      "epoch:1, loss_avg:0.0644216537475586\n",
      "epoch:1, loss_avg:0.04837769269943237\n",
      "epoch:1, loss_avg:0.08851566910743713\n",
      "epoch:1, loss_avg:0.05965488404035568\n",
      "epoch:1, loss_avg:0.04167532920837402\n",
      "epoch:1, loss_avg:0.06050591915845871\n",
      "epoch:1, loss_avg:0.050169438123703\n",
      "epoch:1, loss_avg:0.03910408914089203\n",
      "epoch:1, loss_avg:0.06001739203929901\n",
      "epoch:1, loss_avg:0.045670680701732635\n",
      "epoch:1, loss_avg:0.04748959839344025\n",
      "epoch:1, loss_avg:0.06054685264825821\n",
      "epoch:1, loss_avg:0.06585545837879181\n",
      "epoch:1, loss_avg:0.0837983712553978\n",
      "epoch:1, loss_avg:0.051087357103824615\n",
      "epoch:1, loss_avg:0.0527985543012619\n",
      "epoch:1, loss_avg:0.0791182667016983\n",
      "epoch:1, loss_avg:0.07310420274734497\n",
      "epoch:1, loss_avg:0.06301352381706238\n",
      "epoch:1, loss_avg:0.04513583332300186\n",
      "epoch:1, loss_avg:0.06790211796760559\n",
      "epoch:1, loss_avg:0.046274811029434204\n",
      "epoch:1, loss_avg:0.06953538954257965\n",
      "epoch:1, loss_avg:0.056255489587783813\n",
      "epoch:1, loss_avg:0.07652557641267776\n",
      "epoch:1, loss_avg:0.062270939350128174\n",
      "epoch:1, loss_avg:0.052649639546871185\n",
      "epoch:1, loss_avg:0.055944763123989105\n",
      "epoch:1, loss_avg:0.059685006737709045\n",
      "epoch:1, loss_avg:0.06606747210025787\n",
      "epoch:1, loss_avg:0.05522743612527847\n",
      "epoch:1, loss_avg:0.06597986072301865\n",
      "epoch:1, loss_avg:0.05759085342288017\n",
      "epoch:1, loss_avg:0.05442109331488609\n",
      "epoch:1, loss_avg:0.053307607769966125\n",
      "epoch:1, loss_avg:0.05361291021108627\n",
      "epoch:1, loss_avg:0.04615411162376404\n",
      "epoch:1, loss_avg:0.03942554071545601\n",
      "epoch:1, loss_avg:0.05229644477367401\n",
      "epoch:1, loss_avg:0.05439736694097519\n",
      "epoch:1, loss_avg:0.0536142960190773\n",
      "epoch:1, loss_avg:0.05459361523389816\n",
      "epoch:1, loss_avg:0.06895928829908371\n",
      "epoch:1, loss_avg:0.05334462970495224\n",
      "epoch:1, loss_avg:0.05187831073999405\n",
      "epoch:1, loss_avg:0.06209546700119972\n",
      "epoch:1, loss_avg:0.06516797095537186\n",
      "epoch:1, loss_avg:0.08695406466722488\n",
      "epoch:1, loss_avg:0.051023878157138824\n",
      "epoch:1, loss_avg:0.07540799677371979\n",
      "epoch:1, loss_avg:0.0554002970457077\n",
      "epoch:1, loss_avg:0.07248915731906891\n",
      "epoch:1, loss_avg:0.05874328315258026\n",
      "epoch:1, loss_avg:0.05997334420681\n",
      "epoch:1, loss_avg:0.06189950555562973\n",
      "epoch:1, loss_avg:0.05890408903360367\n",
      "epoch:1, loss_avg:0.05843441188335419\n",
      "epoch:1, loss_avg:0.08047348260879517\n",
      "epoch:1, loss_avg:0.05384727567434311\n",
      "epoch:1, loss_avg:0.052024975419044495\n",
      "epoch:1, loss_avg:0.07777506858110428\n",
      "epoch:1, loss_avg:0.04934303089976311\n",
      "epoch:1, loss_avg:0.0538705438375473\n",
      "epoch:1, loss_avg:0.05458412319421768\n",
      "epoch:1, loss_avg:0.05635013431310654\n",
      "epoch:1, loss_avg:0.050968803465366364\n",
      "epoch:1, loss_avg:0.06410554051399231\n",
      "epoch:1, loss_avg:0.07492095232009888\n",
      "epoch:1, loss_avg:0.06897859275341034\n",
      "epoch:1, loss_avg:0.04807378351688385\n",
      "epoch:1, loss_avg:0.05205918103456497\n",
      "epoch:1, loss_avg:0.04994046688079834\n",
      "epoch:1, loss_avg:0.048552192747592926\n",
      "epoch:1, loss_avg:0.054456524550914764\n",
      "epoch:1, loss_avg:0.05855976417660713\n",
      "epoch:1, loss_avg:0.05245175212621689\n",
      "epoch:1, loss_avg:0.05261905491352081\n",
      "epoch:1, loss_avg:0.05567667633295059\n",
      "epoch:1, loss_avg:0.04906906187534332\n",
      "epoch:1, loss_avg:0.06603740155696869\n",
      "epoch:2, loss_avg:0.05079871416091919\n",
      "epoch:2, loss_avg:0.043617021292448044\n",
      "epoch:2, loss_avg:0.05868595093488693\n",
      "epoch:2, loss_avg:0.054164640605449677\n",
      "epoch:2, loss_avg:0.044859759509563446\n",
      "epoch:2, loss_avg:0.051704179495573044\n",
      "epoch:2, loss_avg:0.03869764506816864\n",
      "epoch:2, loss_avg:0.0910036563873291\n",
      "epoch:2, loss_avg:0.04877453297376633\n",
      "epoch:2, loss_avg:0.05912813916802406\n",
      "epoch:2, loss_avg:0.04079271852970123\n",
      "epoch:2, loss_avg:0.057569511234760284\n",
      "epoch:2, loss_avg:0.05389629304409027\n",
      "epoch:2, loss_avg:0.04441816359758377\n",
      "epoch:2, loss_avg:0.07923902571201324\n",
      "epoch:2, loss_avg:0.049848802387714386\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2, loss_avg:0.051041267812252045\n",
      "epoch:2, loss_avg:0.07095755636692047\n",
      "epoch:2, loss_avg:0.050460733473300934\n",
      "epoch:2, loss_avg:0.05117597430944443\n",
      "epoch:2, loss_avg:0.047284651547670364\n",
      "epoch:2, loss_avg:0.06193218007683754\n",
      "epoch:2, loss_avg:0.055474042892456055\n",
      "epoch:2, loss_avg:0.17277973890304565\n",
      "epoch:2, loss_avg:0.06996461004018784\n",
      "epoch:2, loss_avg:0.08037039637565613\n",
      "epoch:2, loss_avg:0.05788422375917435\n",
      "epoch:2, loss_avg:0.07382740080356598\n",
      "epoch:2, loss_avg:0.04799604415893555\n",
      "epoch:2, loss_avg:0.08644312620162964\n",
      "epoch:2, loss_avg:0.05461160093545914\n",
      "epoch:2, loss_avg:0.06022743880748749\n",
      "epoch:2, loss_avg:0.056088145822286606\n",
      "epoch:2, loss_avg:0.08590653538703918\n",
      "epoch:2, loss_avg:0.04962471127510071\n",
      "epoch:2, loss_avg:0.06538254022598267\n",
      "epoch:2, loss_avg:0.07595651596784592\n",
      "epoch:2, loss_avg:0.045967407524585724\n",
      "epoch:2, loss_avg:0.04919493570923805\n",
      "epoch:2, loss_avg:0.06039029359817505\n",
      "epoch:2, loss_avg:0.05546789988875389\n",
      "epoch:2, loss_avg:0.3128584921360016\n",
      "epoch:2, loss_avg:0.06384139508008957\n",
      "epoch:2, loss_avg:0.052009474486112595\n",
      "epoch:2, loss_avg:0.04372458904981613\n",
      "epoch:2, loss_avg:0.06588494777679443\n",
      "epoch:2, loss_avg:0.055018894374370575\n",
      "epoch:2, loss_avg:0.0595548041164875\n",
      "epoch:2, loss_avg:0.059540413320064545\n",
      "epoch:2, loss_avg:0.061138663440942764\n",
      "epoch:2, loss_avg:0.05036449432373047\n",
      "epoch:2, loss_avg:0.05732051283121109\n",
      "epoch:2, loss_avg:0.06372801959514618\n",
      "epoch:2, loss_avg:0.05017617717385292\n",
      "epoch:2, loss_avg:0.058540672063827515\n",
      "epoch:2, loss_avg:0.04536733403801918\n",
      "epoch:2, loss_avg:0.050750963389873505\n",
      "epoch:2, loss_avg:0.07556749135255814\n",
      "epoch:2, loss_avg:0.057382792234420776\n",
      "epoch:2, loss_avg:0.08843939006328583\n",
      "epoch:2, loss_avg:0.059423964470624924\n",
      "epoch:2, loss_avg:0.07362166047096252\n",
      "epoch:2, loss_avg:0.048648905009031296\n",
      "epoch:2, loss_avg:0.049408718943595886\n",
      "epoch:2, loss_avg:0.05271999165415764\n",
      "epoch:2, loss_avg:0.043295178562402725\n",
      "epoch:2, loss_avg:0.0715068057179451\n",
      "epoch:2, loss_avg:0.0612017959356308\n",
      "epoch:2, loss_avg:0.08061479032039642\n",
      "epoch:2, loss_avg:0.050852321088314056\n",
      "epoch:2, loss_avg:0.056517355144023895\n",
      "epoch:2, loss_avg:0.05951566621661186\n",
      "epoch:2, loss_avg:0.04987400025129318\n",
      "epoch:2, loss_avg:0.04953445866703987\n",
      "epoch:2, loss_avg:0.06430275738239288\n",
      "epoch:2, loss_avg:0.057583749294281006\n",
      "epoch:2, loss_avg:0.05091690272092819\n",
      "epoch:2, loss_avg:0.04849793761968613\n",
      "epoch:2, loss_avg:0.0713217556476593\n",
      "epoch:2, loss_avg:0.05359780788421631\n",
      "epoch:2, loss_avg:0.052136726677417755\n",
      "epoch:2, loss_avg:0.057281989604234695\n",
      "epoch:2, loss_avg:0.0702725350856781\n",
      "epoch:2, loss_avg:0.05618860200047493\n",
      "epoch:2, loss_avg:0.08767598122358322\n",
      "epoch:2, loss_avg:0.0516953319311142\n",
      "epoch:2, loss_avg:0.06650863587856293\n",
      "epoch:2, loss_avg:0.055496979504823685\n",
      "epoch:2, loss_avg:0.05853301286697388\n",
      "epoch:2, loss_avg:0.04829397425055504\n",
      "epoch:2, loss_avg:0.04813212901353836\n",
      "epoch:2, loss_avg:0.06327109783887863\n",
      "epoch:2, loss_avg:0.05685083568096161\n",
      "epoch:2, loss_avg:0.04167940095067024\n",
      "epoch:2, loss_avg:0.05979137867689133\n",
      "epoch:2, loss_avg:0.05603473260998726\n",
      "epoch:2, loss_avg:0.0504804328083992\n",
      "epoch:2, loss_avg:0.058895330876111984\n",
      "epoch:2, loss_avg:0.06683064252138138\n",
      "epoch:2, loss_avg:0.059848248958587646\n",
      "epoch:3, loss_avg:0.07738082855939865\n",
      "epoch:3, loss_avg:0.04781067371368408\n",
      "epoch:3, loss_avg:0.06308714300394058\n",
      "epoch:3, loss_avg:0.057012759149074554\n",
      "epoch:3, loss_avg:0.06857459247112274\n",
      "epoch:3, loss_avg:0.04618891328573227\n",
      "epoch:3, loss_avg:0.044434137642383575\n",
      "epoch:3, loss_avg:0.06402523815631866\n",
      "epoch:3, loss_avg:0.08196762204170227\n",
      "epoch:3, loss_avg:0.0666513666510582\n",
      "epoch:3, loss_avg:0.06651682406663895\n",
      "epoch:3, loss_avg:0.07906495779752731\n",
      "epoch:3, loss_avg:0.061804916709661484\n",
      "epoch:3, loss_avg:0.054772041738033295\n",
      "epoch:3, loss_avg:0.05952326953411102\n",
      "epoch:3, loss_avg:0.047008417546749115\n",
      "epoch:3, loss_avg:0.04375026747584343\n",
      "epoch:3, loss_avg:0.06618726998567581\n",
      "epoch:3, loss_avg:0.05113716796040535\n",
      "epoch:3, loss_avg:0.058913394808769226\n",
      "epoch:3, loss_avg:0.04438455402851105\n",
      "epoch:3, loss_avg:0.048283711075782776\n",
      "epoch:3, loss_avg:0.054334357380867004\n",
      "epoch:3, loss_avg:0.05802559107542038\n",
      "epoch:3, loss_avg:0.04803217947483063\n",
      "epoch:3, loss_avg:0.060108691453933716\n",
      "epoch:3, loss_avg:0.04802770912647247\n",
      "epoch:3, loss_avg:0.05760867893695831\n",
      "epoch:3, loss_avg:0.04680195450782776\n",
      "epoch:3, loss_avg:0.08025334030389786\n",
      "epoch:3, loss_avg:0.045067086815834045\n",
      "epoch:3, loss_avg:0.04443180561065674\n",
      "epoch:3, loss_avg:0.054985180497169495\n",
      "epoch:3, loss_avg:0.05060375854372978\n",
      "epoch:3, loss_avg:0.05491139739751816\n",
      "epoch:3, loss_avg:0.0669591873884201\n",
      "epoch:3, loss_avg:0.06953529268503189\n",
      "epoch:3, loss_avg:0.05308523401618004\n",
      "epoch:3, loss_avg:0.05535006895661354\n",
      "epoch:3, loss_avg:0.10506433248519897\n",
      "epoch:3, loss_avg:0.053694821894168854\n",
      "epoch:3, loss_avg:0.06719271838665009\n",
      "epoch:3, loss_avg:0.05574030801653862\n",
      "epoch:3, loss_avg:0.07491268217563629\n",
      "epoch:3, loss_avg:0.05975206941366196\n",
      "epoch:3, loss_avg:0.05234702304005623\n",
      "epoch:3, loss_avg:0.061209216713905334\n",
      "epoch:3, loss_avg:0.059702858328819275\n",
      "epoch:3, loss_avg:0.085145965218544\n",
      "epoch:3, loss_avg:0.05728534236550331\n",
      "epoch:3, loss_avg:0.05720927566289902\n",
      "epoch:3, loss_avg:0.07944038510322571\n",
      "epoch:3, loss_avg:0.05242463946342468\n",
      "epoch:3, loss_avg:0.06404906511306763\n",
      "epoch:3, loss_avg:0.06994389742612839\n",
      "epoch:3, loss_avg:0.05527890473604202\n",
      "epoch:3, loss_avg:0.052566900849342346\n",
      "epoch:3, loss_avg:0.057998500764369965\n",
      "epoch:3, loss_avg:0.046893808990716934\n",
      "epoch:3, loss_avg:0.06370696425437927\n",
      "epoch:3, loss_avg:0.04709096625447273\n",
      "epoch:3, loss_avg:0.07064150273799896\n",
      "epoch:3, loss_avg:0.04610732942819595\n",
      "epoch:3, loss_avg:0.08117879927158356\n",
      "epoch:3, loss_avg:0.050810009241104126\n",
      "epoch:3, loss_avg:0.05683343857526779\n",
      "epoch:3, loss_avg:0.05876923352479935\n",
      "epoch:3, loss_avg:0.05636397376656532\n",
      "epoch:3, loss_avg:0.05638335272669792\n",
      "epoch:3, loss_avg:0.04599403589963913\n",
      "epoch:3, loss_avg:0.05595908686518669\n",
      "epoch:3, loss_avg:0.0602966845035553\n",
      "epoch:3, loss_avg:0.05388803780078888\n",
      "epoch:3, loss_avg:0.06228715926408768\n",
      "epoch:3, loss_avg:0.05553877353668213\n",
      "epoch:3, loss_avg:0.07507190108299255\n",
      "epoch:3, loss_avg:0.059070736169815063\n",
      "epoch:3, loss_avg:0.0533609502017498\n",
      "epoch:3, loss_avg:0.06892317533493042\n",
      "epoch:3, loss_avg:0.06362639367580414\n",
      "epoch:3, loss_avg:0.0657341331243515\n",
      "epoch:3, loss_avg:0.04193956032395363\n",
      "epoch:3, loss_avg:0.08924584090709686\n",
      "epoch:3, loss_avg:0.07051154226064682\n",
      "epoch:3, loss_avg:0.05775945633649826\n",
      "epoch:3, loss_avg:0.05299178883433342\n",
      "epoch:3, loss_avg:0.05977519229054451\n",
      "epoch:3, loss_avg:0.06660494208335876\n",
      "epoch:3, loss_avg:0.051562465727329254\n",
      "epoch:3, loss_avg:0.05674085021018982\n",
      "epoch:3, loss_avg:0.0768699198961258\n",
      "epoch:3, loss_avg:0.05654754117131233\n",
      "epoch:3, loss_avg:0.08462388813495636\n",
      "epoch:3, loss_avg:0.053069353103637695\n",
      "epoch:3, loss_avg:0.04594828188419342\n",
      "epoch:3, loss_avg:0.05602581053972244\n",
      "epoch:3, loss_avg:0.05333889275789261\n",
      "epoch:3, loss_avg:0.08338592946529388\n",
      "epoch:3, loss_avg:0.050451766699552536\n",
      "epoch:3, loss_avg:0.05402056872844696\n",
      "epoch:4, loss_avg:0.05404657870531082\n",
      "epoch:4, loss_avg:0.070162832736969\n",
      "epoch:4, loss_avg:0.05373179912567139\n",
      "epoch:4, loss_avg:0.0452050119638443\n",
      "epoch:4, loss_avg:0.05038176476955414\n",
      "epoch:4, loss_avg:0.06216299533843994\n",
      "epoch:4, loss_avg:0.06385410577058792\n",
      "epoch:4, loss_avg:0.06862863898277283\n",
      "epoch:4, loss_avg:0.06168324127793312\n",
      "epoch:4, loss_avg:0.05476132035255432\n",
      "epoch:4, loss_avg:0.06533622741699219\n",
      "epoch:4, loss_avg:0.07507568597793579\n",
      "epoch:4, loss_avg:0.05267105996608734\n",
      "epoch:4, loss_avg:0.06957715004682541\n",
      "epoch:4, loss_avg:0.042817022651433945\n",
      "epoch:4, loss_avg:0.05043061077594757\n",
      "epoch:4, loss_avg:0.05094234272837639\n",
      "epoch:4, loss_avg:0.055236443877220154\n",
      "epoch:4, loss_avg:0.06263318657875061\n",
      "epoch:4, loss_avg:0.06054046005010605\n",
      "epoch:4, loss_avg:0.04548819735646248\n",
      "epoch:4, loss_avg:0.052880287170410156\n",
      "epoch:4, loss_avg:0.062296103686094284\n",
      "epoch:4, loss_avg:0.04552068933844566\n",
      "epoch:4, loss_avg:0.05007384717464447\n",
      "epoch:4, loss_avg:0.057371485978364944\n",
      "epoch:4, loss_avg:0.0578862726688385\n",
      "epoch:4, loss_avg:0.0680576041340828\n",
      "epoch:4, loss_avg:0.04445856809616089\n",
      "epoch:4, loss_avg:0.045986108481884\n",
      "epoch:4, loss_avg:0.09517373889684677\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:4, loss_avg:0.04793037474155426\n",
      "epoch:4, loss_avg:0.045182205736637115\n",
      "epoch:4, loss_avg:0.06341404467821121\n",
      "epoch:4, loss_avg:0.06898858398199081\n",
      "epoch:4, loss_avg:0.04478355497121811\n",
      "epoch:4, loss_avg:0.047496844083070755\n",
      "epoch:4, loss_avg:0.044594429433345795\n",
      "epoch:4, loss_avg:0.05144903063774109\n",
      "epoch:4, loss_avg:0.06877158582210541\n",
      "epoch:4, loss_avg:0.06309758126735687\n",
      "epoch:4, loss_avg:0.07231319695711136\n",
      "epoch:4, loss_avg:0.05683460831642151\n",
      "epoch:4, loss_avg:0.06333325803279877\n",
      "epoch:4, loss_avg:0.05965851992368698\n",
      "epoch:4, loss_avg:0.074168361723423\n",
      "epoch:4, loss_avg:0.04958617687225342\n",
      "epoch:4, loss_avg:0.04804511368274689\n",
      "epoch:4, loss_avg:0.08997024595737457\n",
      "epoch:4, loss_avg:0.05051504820585251\n",
      "epoch:4, loss_avg:0.06816818565130234\n",
      "epoch:4, loss_avg:0.04373011738061905\n",
      "epoch:4, loss_avg:0.0604417622089386\n",
      "epoch:4, loss_avg:0.05788732320070267\n",
      "epoch:4, loss_avg:0.048131126910448074\n",
      "epoch:4, loss_avg:0.048563435673713684\n",
      "epoch:4, loss_avg:0.06841757148504257\n",
      "epoch:4, loss_avg:0.05473075807094574\n",
      "epoch:4, loss_avg:0.04592173546552658\n",
      "epoch:4, loss_avg:0.055347736924886703\n",
      "epoch:4, loss_avg:0.05900708585977554\n",
      "epoch:4, loss_avg:0.06732501089572906\n",
      "epoch:4, loss_avg:0.07089117169380188\n",
      "epoch:4, loss_avg:0.0821416974067688\n",
      "epoch:4, loss_avg:0.05415176600217819\n",
      "epoch:4, loss_avg:0.05204390734434128\n",
      "epoch:4, loss_avg:0.042123179882764816\n",
      "epoch:4, loss_avg:0.06391333043575287\n",
      "epoch:4, loss_avg:0.06075572595000267\n",
      "epoch:4, loss_avg:0.05184908211231232\n",
      "epoch:4, loss_avg:0.0395125076174736\n",
      "epoch:4, loss_avg:0.06886614859104156\n",
      "epoch:4, loss_avg:0.05137382820248604\n",
      "epoch:4, loss_avg:0.05448064208030701\n",
      "epoch:4, loss_avg:0.033509545028209686\n",
      "epoch:4, loss_avg:0.07771430909633636\n",
      "epoch:4, loss_avg:0.04760288819670677\n",
      "epoch:4, loss_avg:0.04588228464126587\n",
      "epoch:4, loss_avg:0.056701913475990295\n",
      "epoch:4, loss_avg:0.07279729843139648\n",
      "epoch:4, loss_avg:0.04403607174754143\n",
      "epoch:4, loss_avg:0.07840938121080399\n",
      "epoch:4, loss_avg:0.07644417881965637\n",
      "epoch:4, loss_avg:0.07228626310825348\n",
      "epoch:4, loss_avg:0.06270170956850052\n",
      "epoch:4, loss_avg:0.05725545808672905\n",
      "epoch:4, loss_avg:0.043219394981861115\n",
      "epoch:4, loss_avg:0.07960774004459381\n",
      "epoch:4, loss_avg:0.07184890657663345\n",
      "epoch:4, loss_avg:0.05465064197778702\n",
      "epoch:4, loss_avg:0.05743922293186188\n",
      "epoch:4, loss_avg:0.06377086788415909\n",
      "epoch:4, loss_avg:0.06924661993980408\n",
      "epoch:4, loss_avg:0.05180107057094574\n",
      "epoch:4, loss_avg:0.09007643163204193\n",
      "epoch:4, loss_avg:0.05311669409275055\n",
      "epoch:4, loss_avg:0.05650670826435089\n",
      "epoch:4, loss_avg:0.03909587487578392\n",
      "epoch:4, loss_avg:0.058144792914390564\n",
      "epoch:4, loss_avg:0.04636753350496292\n"
     ]
    }
   ],
   "source": [
    "layer_dict = {}\n",
    "layer_name = []\n",
    "count = 0\n",
    "for name in w_glob.keys():\n",
    "    if count % 2 == 0:\n",
    "        layer_name.append(name.split('.',1)[0])\n",
    "    count += 1\n",
    "    \n",
    "for i in layer_name:\n",
    "#     layer_dict[i] = CNNCifarEmb(torch.cat([w_glob[i+'.weight'].reshape(1,-1), w_glob[i+'.bias'].reshape(1,-1)], 1).numel())\n",
    "    layer_dict[i] = CNNMnistEmb(torch.cat([w_glob[i+'.weight'].reshape(1,-1), w_glob[i+'.bias'].reshape(1,-1)], 1).numel())\n",
    "# emb_reverse = CNNCifarEmbReverse(args)\n",
    "emb_reverse = CNNMnistEmbReverse(args)\n",
    "# print(w_glob[i+'.weight'].reshape(1,-1).numpy()[0])\n",
    "\n",
    "# print(w_glob[i+'.bias'].numpy())\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':layer_dict[layer_name[0]].parameters()},\n",
    "    {'params':layer_dict[layer_name[1]].parameters()},\n",
    "    {'params':layer_dict[layer_name[2]].parameters()},\n",
    "    {'params':layer_dict[layer_name[3]].parameters()},\n",
    "#     {'params':layer_dict[layer_name[4]].parameters()},\n",
    "    {'params':emb_reverse.parameters()}\n",
    "] ,0.01)\n",
    "\n",
    "for iter in range(args.emb_train_epochs):\n",
    "    idxs_users = np.random.choice(range(args.num_users), 100, replace=False)\n",
    "    \n",
    "    for idx in idxs_users:\n",
    "#         local = LocalUpdate_divide(args=args, dataset=dataset_train, idxs=dict_users[0][idx])\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "#         w, loss, loss_list = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1).to(args.device))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1).to(args.device))\n",
    "        avg_emb_feature = emb_feature/4\n",
    "        transform_w = emb_reverse.forward(avg_emb_feature)\n",
    "        loss_w = [sum((w[i].reshape(1,-1) - transform_w[i].reshape(1,-1)) ** 2) for i in w_glob.keys()]\n",
    "        loss_avg = 0\n",
    "        loss_check_dict = {}\n",
    "        for i in range(len(loss_w)):\n",
    "            loss_avg += sum(loss_w[i])/len(loss_w[i])\n",
    "            loss_check_dict[len(loss_w[i])] = loss_w[i]\n",
    "            if loss_avg.item() > 3.0:\n",
    "                print('len:{},w:{}'.format(len(loss_w[i]), loss_check_dict[len(loss_w[i])]))\n",
    "                print('loss_sum:', loss_avg)\n",
    "                print('\\n***************')\n",
    "        optimizer.zero_grad()\n",
    "        loss_avg.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        print('epoch:{}, loss_avg:{}'.format(iter, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = MemoryBuffer(20)\n",
    "dqn = DQN(parameter_dim, action_dim, replay_buffer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedPareto(w, action, choice):\n",
    "    w_chosen = []\n",
    "    for i in choice:\n",
    "        w_chosen.append(w[i])\n",
    "    w_avg = copy.deepcopy(w_chosen[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(0, len(w_chosen)):\n",
    "            if i==0:\n",
    "                w_avg[k] = action[i] * w_chosen[i][k]\n",
    "            else:\n",
    "                w_avg[k] += action[i] * w_chosen[i][k]\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "def test_img(net_g, datatest, args):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=args.bs)\n",
    "    l = len(data_loader)\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        if args.gpu != -1:\n",
    "            data, target = data, target\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if args.verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[30, 85, 11, 24, 62, 36, 66, 57, 64, 87]\n",
      "tensor(9.7367)\n",
      "Round   0, Average loss -0.461\n",
      "saved\n",
      "[66, 85, 62, 36, 30, 24, 47, 53, 77, 88]\n",
      "tensor(10.4417)\n",
      "Round   1, Average loss -0.459\n",
      "[66, 36, 24, 62, 47, 85, 53, 92, 11, 30]\n",
      "tensor(10.7350)\n",
      "Round   2, Average loss -0.458\n",
      "[66, 47, 85, 30, 53, 62, 36, 43, 61, 24]\n",
      "tensor(21.0883)\n",
      "Round   3, Average loss -0.417\n",
      "[66, 85, 24, 30, 43, 47, 36, 53, 62, 11]\n",
      "tensor(25.3050)\n",
      "Round   4, Average loss -0.400\n",
      "[47, 24, 66, 30, 85, 62, 36, 53, 96, 88]\n",
      "tensor(26.6317)\n",
      "Round   5, Average loss -0.394\n",
      "[24, 85, 47, 66, 62, 53, 36, 30, 96, 38]\n",
      "tensor(27.6483)\n",
      "Round   6, Average loss -0.390\n",
      "[85, 24, 47, 62, 53, 66, 30, 36, 88, 61]\n",
      "tensor(25.5983)\n",
      "Round   7, Average loss -0.399\n",
      "[24, 66, 85, 47, 62, 53, 36, 30, 61, 96]\n",
      "tensor(27.3900)\n",
      "Round   8, Average loss -0.391\n",
      "[24, 47, 53, 85, 66, 38, 36, 62, 61, 88]\n",
      "tensor(28.4300)\n",
      "Round   9, Average loss -0.387\n",
      "[24, 62, 66, 85, 36, 88, 47, 11, 61, 30]\n",
      "tensor(27.9617)\n",
      "Round  10, Average loss -0.389\n",
      "saved\n",
      "[24, 47, 36, 66, 85, 11, 62, 30, 61, 53]\n",
      "tensor(29.2550)\n",
      "Round  11, Average loss -0.383\n",
      "[24, 47, 85, 53, 62, 30, 48, 38, 66, 36]\n",
      "tensor(29.0333)\n",
      "Round  12, Average loss -0.384\n",
      "[24, 66, 62, 85, 47, 53, 30, 61, 36, 96]\n",
      "tensor(27.1633)\n",
      "Round  13, Average loss -0.392\n",
      "[24, 85, 66, 47, 62, 36, 53, 88, 30, 92]\n",
      "tensor(28.3683)\n",
      "Round  14, Average loss -0.387\n",
      "[24, 47, 85, 53, 66, 62, 36, 61, 22, 88]\n",
      "tensor(22.1317)\n",
      "Round  15, Average loss -0.413\n",
      "[24, 85, 38, 36, 47, 30, 61, 96, 66, 11]\n",
      "tensor(26.1850)\n",
      "Round  16, Average loss -0.396\n",
      "[24, 66, 47, 36, 30, 85, 62, 88, 61, 53]\n",
      "tensor(26.1817)\n",
      "Round  17, Average loss -0.396\n",
      "[24, 66, 47, 85, 30, 36, 92, 88, 53, 38]\n",
      "tensor(28.3900)\n",
      "Round  18, Average loss -0.387\n",
      "[24, 85, 66, 30, 47, 62, 53, 38, 96, 36]\n",
      "tensor(25.6983)\n",
      "Round  19, Average loss -0.398\n",
      "[62, 30, 36, 66, 87, 85, 24, 11, 47, 53]\n",
      "tensor(6.6633)\n",
      "Round  20, Average loss -0.473\n",
      "saved\n",
      "[66, 24, 85, 36, 47, 62, 30, 69, 88, 11]\n",
      "tensor(10.4417)\n",
      "Round  21, Average loss -0.459\n",
      "[24, 47, 66, 36, 85, 30, 53, 62, 38, 61]\n",
      "tensor(22.2050)\n",
      "Round  22, Average loss -0.413\n",
      "[85, 24, 30, 36, 53, 62, 47, 66, 38, 61]\n",
      "tensor(23.7333)\n",
      "Round  23, Average loss -0.406\n",
      "[24, 30, 47, 85, 66, 62, 53, 88, 38, 61]\n",
      "tensor(23.4467)\n",
      "Round  24, Average loss -0.408\n",
      "[24, 85, 47, 53, 62, 66, 38, 30, 36, 61]\n",
      "tensor(29.8033)\n",
      "Round  25, Average loss -0.381\n",
      "[24, 85, 66, 47, 62, 53, 61, 30, 36, 38]\n",
      "tensor(31.4250)\n",
      "Round  26, Average loss -0.374\n",
      "[24, 85, 66, 47, 30, 53, 36, 38, 62, 61]\n",
      "tensor(33.8350)\n",
      "Round  27, Average loss -0.363\n",
      "[24, 85, 47, 36, 66, 53, 62, 30, 61, 11]\n",
      "tensor(34.3883)\n",
      "Round  28, Average loss -0.361\n",
      "[85, 24, 47, 66, 36, 38, 30, 62, 53, 61]\n",
      "tensor(29.2150)\n",
      "Round  29, Average loss -0.384\n",
      "[24, 85, 30, 47, 66, 62, 38, 36, 88, 92]\n",
      "tensor(30.8383)\n",
      "Round  30, Average loss -0.377\n",
      "saved\n",
      "[47, 24, 66, 85, 88, 62, 61, 36, 30, 53]\n",
      "tensor(35.4283)\n",
      "Round  31, Average loss -0.356\n",
      "[24, 66, 85, 47, 30, 62, 36, 38, 61, 96]\n",
      "tensor(35.9583)\n",
      "Round  32, Average loss -0.354\n",
      "[24, 85, 53, 66, 47, 30, 38, 36, 96, 62]\n",
      "tensor(37.0167)\n",
      "Round  33, Average loss -0.349\n",
      "[85, 24, 47, 62, 66, 36, 38, 53, 11, 30]\n",
      "tensor(29.5567)\n",
      "Round  34, Average loss -0.382\n",
      "[24, 36, 66, 85, 53, 47, 30, 62, 61, 38]\n",
      "tensor(27.8733)\n",
      "Round  35, Average loss -0.389\n",
      "[24, 47, 85, 53, 36, 66, 38, 62, 88, 96]\n",
      "tensor(31.8533)\n",
      "Round  36, Average loss -0.372\n",
      "[24, 85, 62, 30, 47, 53, 43, 11, 66, 36]\n",
      "tensor(26.2017)\n",
      "Round  37, Average loss -0.396\n",
      "[24, 85, 62, 53, 36, 47, 66, 11, 30, 61]\n",
      "tensor(27.6983)\n",
      "Round  38, Average loss -0.390\n",
      "[24, 85, 62, 66, 47, 53, 36, 30, 68, 38]\n",
      "tensor(28.0533)\n",
      "Round  39, Average loss -0.388\n",
      "[62, 87, 47, 85, 57, 11, 24, 66, 30, 28]\n",
      "tensor(10.2183)\n",
      "Round  40, Average loss -0.460\n",
      "saved\n",
      "[24, 30, 66, 62, 85, 36, 53, 47, 69, 38]\n",
      "tensor(9.9150)\n",
      "Round  41, Average loss -0.461\n",
      "[24, 85, 66, 47, 36, 68, 38, 96, 30, 62]\n",
      "tensor(15.5450)\n",
      "Round  42, Average loss -0.439\n",
      "[24, 66, 36, 62, 47, 85, 30, 68, 61, 88]\n",
      "tensor(26.1650)\n",
      "Round  43, Average loss -0.396\n",
      "[24, 47, 85, 66, 30, 62, 53, 68, 43, 36]\n",
      "tensor(28.7033)\n",
      "Round  44, Average loss -0.386\n",
      "[24, 85, 66, 47, 62, 88, 30, 36, 69, 53]\n",
      "tensor(27.5167)\n",
      "Round  45, Average loss -0.391\n",
      "[24, 85, 66, 30, 62, 36, 38, 47, 68, 53]\n",
      "tensor(29.0083)\n",
      "Round  46, Average loss -0.384\n",
      "[24, 85, 62, 47, 66, 30, 36, 61, 22, 68]\n",
      "tensor(27.9933)\n",
      "Round  47, Average loss -0.389\n",
      "[24, 85, 62, 36, 66, 47, 30, 43, 38, 68]\n",
      "tensor(29.5133)\n",
      "Round  48, Average loss -0.382\n",
      "[24, 85, 66, 62, 30, 47, 92, 68, 96, 36]\n",
      "tensor(29.3167)\n",
      "Round  49, Average loss -0.383\n",
      "[24, 47, 85, 66, 30, 36, 62, 96, 53, 92]\n",
      "tensor(29.6033)\n",
      "Round  50, Average loss -0.382\n",
      "saved\n",
      "[24, 85, 62, 30, 53, 66, 47, 68, 38, 36]\n",
      "tensor(30.7950)\n",
      "Round  51, Average loss -0.377\n",
      "[24, 85, 62, 53, 30, 47, 38, 66, 61, 88]\n",
      "tensor(28.3367)\n",
      "Round  52, Average loss -0.387\n",
      "[24, 47, 66, 36, 85, 96, 53, 62, 68, 30]\n",
      "tensor(38.1233)\n",
      "Round  53, Average loss -0.344\n",
      "[85, 62, 24, 47, 66, 30, 36, 92, 53, 88]\n",
      "tensor(29.6033)\n",
      "Round  54, Average loss -0.382\n",
      "[24, 85, 47, 62, 36, 66, 53, 61, 88, 30]\n",
      "tensor(28.6600)\n",
      "Round  55, Average loss -0.386\n",
      "[24, 47, 85, 66, 62, 53, 30, 38, 48, 88]\n",
      "tensor(29.0300)\n",
      "Round  56, Average loss -0.384\n",
      "[24, 66, 85, 30, 47, 53, 11, 36, 62, 48]\n",
      "tensor(28.8250)\n",
      "Round  57, Average loss -0.385\n",
      "[24, 47, 62, 66, 85, 36, 30, 88, 38, 53]\n",
      "tensor(36.0067)\n",
      "Round  58, Average loss -0.354\n",
      "[24, 85, 62, 47, 66, 36, 53, 30, 88, 61]\n",
      "tensor(29.5650)\n",
      "Round  59, Average loss -0.382\n",
      "[66, 85, 30, 53, 57, 11, 87, 96, 62, 47]\n",
      "tensor(13.8367)\n",
      "Round  60, Average loss -0.446\n",
      "saved\n",
      "[24, 47, 85, 53, 36, 30, 62, 66, 96, 11]\n",
      "tensor(27.1567)\n",
      "Round  61, Average loss -0.392\n",
      "[66, 24, 47, 85, 30, 53, 62, 38, 43, 61]\n",
      "tensor(20.9967)\n",
      "Round  62, Average loss -0.418\n",
      "[66, 85, 47, 24, 30, 38, 62, 53, 36, 92]\n",
      "tensor(38.5000)\n",
      "Round  63, Average loss -0.343\n",
      "[24, 85, 66, 30, 47, 53, 38, 62, 96, 92]\n",
      "tensor(34.1567)\n",
      "Round  64, Average loss -0.362\n",
      "[24, 47, 66, 85, 53, 62, 30, 11, 96, 61]\n",
      "tensor(32.4600)\n",
      "Round  65, Average loss -0.369\n",
      "[30, 85, 66, 24, 62, 47, 53, 38, 11, 36]\n",
      "tensor(33.2300)\n",
      "Round  66, Average loss -0.366\n",
      "[24, 85, 47, 66, 62, 53, 30, 36, 43, 68]\n",
      "tensor(32.9300)\n",
      "Round  67, Average loss -0.367\n",
      "[85, 30, 47, 24, 62, 66, 38, 53, 92, 11]\n",
      "tensor(35.1200)\n",
      "Round  68, Average loss -0.358\n",
      "[24, 66, 47, 36, 85, 30, 53, 62, 68, 11]\n",
      "tensor(34.1667)\n",
      "Round  69, Average loss -0.362\n",
      "[47, 24, 85, 62, 53, 66, 61, 30, 68, 36]\n",
      "tensor(29.8617)\n",
      "Round  70, Average loss -0.381\n",
      "saved\n",
      "[24, 85, 30, 66, 47, 11, 53, 62, 43, 61]\n",
      "tensor(29.2150)\n",
      "Round  71, Average loss -0.384\n",
      "[47, 85, 24, 66, 30, 62, 53, 36, 61, 68]\n",
      "tensor(29.4433)\n",
      "Round  72, Average loss -0.383\n",
      "[24, 85, 30, 47, 62, 66, 38, 53, 88, 96]\n",
      "tensor(29.2850)\n",
      "Round  73, Average loss -0.383\n",
      "[24, 66, 36, 85, 62, 47, 38, 53, 30, 88]\n",
      "tensor(31.1950)\n",
      "Round  74, Average loss -0.375\n",
      "[24, 66, 85, 30, 47, 11, 88, 61, 62, 36]\n",
      "tensor(36.1083)\n",
      "Round  75, Average loss -0.353\n",
      "[24, 85, 66, 53, 62, 47, 36, 88, 30, 11]\n",
      "tensor(31.4433)\n",
      "Round  76, Average loss -0.374\n",
      "[24, 66, 47, 85, 62, 30, 53, 48, 96, 61]\n",
      "tensor(28.8083)\n",
      "Round  77, Average loss -0.385\n",
      "[24, 66, 30, 85, 47, 53, 62, 38, 48, 36]\n",
      "tensor(33.7200)\n",
      "Round  78, Average loss -0.364\n",
      "[24, 85, 47, 53, 66, 61, 62, 30, 11, 43]\n",
      "tensor(41.1467)\n",
      "Round  79, Average loss -0.330\n",
      "[30, 66, 87, 62, 36, 24, 55, 47, 53, 82]\n",
      "tensor(10.4067)\n",
      "Round  80, Average loss -0.459\n",
      "saved\n",
      "[62, 66, 47, 85, 24, 36, 96, 88, 43, 53]\n",
      "tensor(9.9150)\n",
      "Round  81, Average loss -0.461\n",
      "[66, 30, 47, 53, 85, 62, 24, 68, 36, 43]\n",
      "tensor(19.5350)\n",
      "Round  82, Average loss -0.424\n",
      "[24, 47, 66, 30, 85, 53, 62, 61, 43, 36]\n",
      "tensor(22.5100)\n",
      "Round  83, Average loss -0.412\n",
      "[66, 24, 30, 88, 85, 62, 47, 38, 68, 36]\n",
      "tensor(32.8200)\n",
      "Round  84, Average loss -0.368\n",
      "[24, 47, 30, 53, 66, 62, 61, 36, 85, 88]\n",
      "tensor(33.8983)\n",
      "Round  85, Average loss -0.363\n",
      "[24, 85, 66, 30, 62, 36, 53, 47, 11, 88]\n",
      "tensor(33.8233)\n",
      "Round  86, Average loss -0.363\n",
      "[24, 85, 66, 62, 30, 61, 11, 53, 38, 47]\n",
      "tensor(29.0017)\n",
      "Round  87, Average loss -0.384\n",
      "[24, 85, 62, 53, 47, 36, 66, 38, 30, 48]\n",
      "tensor(29.0550)\n",
      "Round  88, Average loss -0.384\n",
      "[85, 24, 62, 47, 53, 36, 30, 66, 38, 11]\n",
      "tensor(27.3650)\n",
      "Round  89, Average loss -0.391\n",
      "[24, 85, 66, 38, 47, 36, 53, 30, 61, 62]\n",
      "tensor(29.0550)\n",
      "Round  90, Average loss -0.384\n",
      "saved\n",
      "[24, 85, 47, 66, 62, 53, 36, 30, 68, 88]\n",
      "tensor(32.1350)\n",
      "Round  91, Average loss -0.371\n",
      "[24, 85, 53, 62, 36, 47, 66, 38, 61, 68]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(27.3483)\n",
      "Round  92, Average loss -0.391\n",
      "[24, 47, 30, 36, 85, 61, 53, 66, 38, 62]\n",
      "tensor(26.5350)\n",
      "Round  93, Average loss -0.395\n",
      "[24, 85, 30, 47, 62, 36, 11, 38, 66, 53]\n",
      "tensor(32.6217)\n",
      "Round  94, Average loss -0.369\n",
      "[24, 47, 66, 36, 53, 62, 85, 30, 11, 96]\n",
      "tensor(29.2183)\n",
      "Round  95, Average loss -0.383\n",
      "[24, 85, 66, 47, 53, 36, 62, 30, 96, 11]\n",
      "tensor(27.8900)\n",
      "Round  96, Average loss -0.389\n",
      "[24, 85, 66, 47, 30, 36, 53, 38, 62, 61]\n",
      "tensor(27.5883)\n",
      "Round  97, Average loss -0.390\n",
      "[24, 62, 85, 47, 88, 30, 36, 48, 66, 53]\n",
      "tensor(28.2567)\n",
      "Round  98, Average loss -0.388\n",
      "[24, 85, 66, 47, 62, 30, 36, 61, 53, 88]\n",
      "tensor(31.8017)\n",
      "Round  99, Average loss -0.372\n",
      "[30, 66, 87, 85, 47, 91, 57, 88, 79, 24]\n",
      "tensor(9.9150)\n",
      "Round 100, Average loss -0.461\n",
      "saved\n",
      "[47, 85, 43, 36, 62, 24, 66, 68, 30, 69]\n",
      "tensor(9.9150)\n",
      "Round 101, Average loss -0.461\n",
      "[24, 66, 85, 47, 30, 62, 36, 53, 11, 38]\n",
      "tensor(18.2350)\n",
      "Round 102, Average loss -0.429\n",
      "[24, 30, 85, 66, 62, 36, 47, 88, 61, 43]\n",
      "tensor(24.1083)\n",
      "Round 103, Average loss -0.405\n",
      "[47, 30, 24, 66, 53, 62, 85, 88, 36, 38]\n",
      "tensor(28.0450)\n",
      "Round 104, Average loss -0.388\n",
      "[24, 47, 85, 66, 30, 53, 62, 36, 96, 43]\n",
      "tensor(27.8333)\n",
      "Round 105, Average loss -0.389\n",
      "[85, 24, 47, 53, 66, 30, 62, 69, 96, 38]\n",
      "tensor(30.1200)\n",
      "Round 106, Average loss -0.380\n",
      "[24, 85, 66, 30, 47, 53, 62, 38, 43, 36]\n",
      "tensor(30.4900)\n",
      "Round 107, Average loss -0.378\n",
      "[85, 24, 66, 47, 36, 30, 53, 38, 62, 43]\n",
      "tensor(29.8683)\n",
      "Round 108, Average loss -0.381\n",
      "[24, 66, 85, 47, 30, 36, 53, 62, 38, 43]\n",
      "tensor(35.6367)\n",
      "Round 109, Average loss -0.355\n",
      "[24, 85, 53, 66, 30, 43, 36, 62, 47, 38]\n",
      "tensor(37.5000)\n",
      "Round 110, Average loss -0.347\n",
      "saved\n",
      "[24, 47, 66, 85, 30, 62, 53, 36, 38, 61]\n",
      "tensor(34.6950)\n",
      "Round 111, Average loss -0.360\n",
      "[85, 24, 66, 47, 53, 62, 30, 88, 11, 36]\n",
      "tensor(35.3983)\n",
      "Round 112, Average loss -0.357\n",
      "[24, 66, 30, 47, 85, 62, 61, 88, 53, 38]\n",
      "tensor(28.5867)\n",
      "Round 113, Average loss -0.386\n",
      "[24, 66, 47, 85, 62, 53, 30, 11, 36, 38]\n",
      "tensor(27.4300)\n",
      "Round 114, Average loss -0.391\n",
      "[24, 85, 47, 62, 66, 30, 53, 68, 38, 36]\n",
      "tensor(29.9750)\n",
      "Round 115, Average loss -0.380\n",
      "[24, 85, 62, 66, 47, 30, 53, 11, 36, 61]\n",
      "tensor(26.9533)\n",
      "Round 116, Average loss -0.393\n",
      "[24, 47, 62, 66, 30, 85, 36, 53, 96, 22]\n",
      "tensor(29.2450)\n",
      "Round 117, Average loss -0.383\n",
      "[85, 66, 47, 24, 36, 38, 30, 62, 53, 96]\n",
      "tensor(34.6267)\n",
      "Round 118, Average loss -0.360\n",
      "[24, 47, 85, 66, 36, 62, 53, 30, 61, 71]\n",
      "tensor(26.8500)\n",
      "Round 119, Average loss -0.394\n",
      "[30, 66, 47, 36, 24, 85, 88, 69, 92, 61]\n",
      "tensor(11.2750)\n",
      "Round 120, Average loss -0.456\n",
      "saved\n",
      "[66, 85, 30, 24, 88, 62, 47, 53, 38, 11]\n",
      "tensor(11.0750)\n",
      "Round 121, Average loss -0.456\n",
      "[66, 24, 47, 85, 53, 30, 88, 11, 77, 43]\n",
      "tensor(17.6483)\n",
      "Round 122, Average loss -0.431\n",
      "[24, 85, 30, 66, 47, 62, 53, 43, 36, 61]\n",
      "tensor(21.3967)\n",
      "Round 123, Average loss -0.416\n",
      "[24, 85, 47, 53, 62, 66, 30, 36, 71, 43]\n",
      "tensor(27.9567)\n",
      "Round 124, Average loss -0.389\n",
      "[24, 85, 66, 30, 53, 47, 36, 62, 88, 38]\n",
      "tensor(28.1217)\n",
      "Round 125, Average loss -0.388\n",
      "[85, 24, 30, 66, 47, 62, 61, 36, 11, 68]\n",
      "tensor(30.1600)\n",
      "Round 126, Average loss -0.379\n",
      "[24, 85, 47, 62, 36, 30, 66, 53, 96, 61]\n",
      "tensor(28.9983)\n",
      "Round 127, Average loss -0.384\n",
      "[85, 24, 66, 47, 30, 53, 62, 88, 36, 11]\n",
      "tensor(28.2533)\n",
      "Round 128, Average loss -0.388\n",
      "[85, 24, 30, 47, 66, 62, 53, 11, 68, 69]\n",
      "tensor(29.0500)\n",
      "Round 129, Average loss -0.384\n",
      "[24, 85, 66, 47, 62, 30, 36, 38, 61, 68]\n",
      "tensor(28.8400)\n",
      "Round 130, Average loss -0.385\n",
      "saved\n",
      "[85, 24, 47, 66, 62, 53, 30, 38, 22, 11]\n",
      "tensor(28.7883)\n",
      "Round 131, Average loss -0.385\n",
      "[24, 85, 47, 62, 30, 36, 53, 66, 38, 61]\n",
      "tensor(35.6383)\n",
      "Round 132, Average loss -0.355\n",
      "[24, 85, 47, 36, 66, 53, 62, 61, 11, 30]\n",
      "tensor(39.6783)\n",
      "Round 133, Average loss -0.337\n",
      "[24, 85, 53, 47, 62, 92, 61, 36, 66, 43]\n",
      "tensor(32.1917)\n",
      "Round 134, Average loss -0.371\n",
      "[24, 85, 47, 53, 62, 36, 38, 30, 61, 66]\n",
      "tensor(29.9067)\n",
      "Round 135, Average loss -0.381\n",
      "[24, 85, 47, 53, 66, 30, 36, 62, 38, 61]\n",
      "tensor(28.4433)\n",
      "Round 136, Average loss -0.387\n",
      "[24, 85, 47, 36, 62, 66, 53, 61, 30, 11]\n",
      "tensor(30.2167)\n",
      "Round 137, Average loss -0.379\n",
      "[24, 85, 47, 66, 30, 62, 36, 53, 11, 38]\n",
      "tensor(29.0350)\n",
      "Round 138, Average loss -0.384\n",
      "[85, 24, 38, 47, 30, 62, 53, 66, 88, 96]\n",
      "tensor(28.2683)\n",
      "Round 139, Average loss -0.388\n",
      "[85, 62, 30, 55, 53, 66, 24, 57, 77, 92]\n",
      "tensor(9.3500)\n",
      "Round 140, Average loss -0.463\n",
      "saved\n",
      "[24, 66, 85, 62, 30, 38, 36, 11, 47, 53]\n",
      "tensor(10.4417)\n",
      "Round 141, Average loss -0.459\n",
      "[24, 53, 85, 47, 66, 62, 30, 36, 38, 59]\n",
      "tensor(10.5117)\n",
      "Round 142, Average loss -0.458\n",
      "[24, 85, 53, 62, 66, 47, 36, 30, 43, 38]\n",
      "tensor(23.4067)\n",
      "Round 143, Average loss -0.408\n",
      "[24, 66, 47, 85, 30, 53, 62, 96, 43, 69]\n",
      "tensor(30.2550)\n",
      "Round 144, Average loss -0.379\n",
      "[24, 85, 66, 47, 43, 62, 30, 96, 53, 38]\n",
      "tensor(39.1983)\n",
      "Round 145, Average loss -0.339\n",
      "[24, 85, 47, 66, 53, 62, 30, 96, 61, 36]\n",
      "tensor(39.1717)\n",
      "Round 146, Average loss -0.339\n",
      "[66, 85, 24, 62, 53, 30, 47, 38, 36, 50]\n",
      "tensor(36.6583)\n",
      "Round 147, Average loss -0.351\n",
      "[24, 66, 85, 47, 62, 53, 30, 36, 38, 22]\n",
      "tensor(31.1750)\n",
      "Round 148, Average loss -0.375\n",
      "[24, 62, 85, 66, 53, 11, 30, 47, 36, 48]\n",
      "tensor(30.8250)\n",
      "Round 149, Average loss -0.377\n",
      "[24, 85, 47, 62, 36, 53, 66, 68, 96, 22]\n",
      "tensor(34.4267)\n",
      "Round 150, Average loss -0.361\n",
      "saved\n",
      "[24, 85, 47, 36, 30, 62, 66, 53, 88, 61]\n",
      "tensor(31.3717)\n",
      "Round 151, Average loss -0.374\n",
      "[24, 47, 85, 66, 36, 62, 53, 30, 61, 88]\n",
      "tensor(34.8250)\n",
      "Round 152, Average loss -0.359\n",
      "[24, 85, 47, 36, 30, 66, 11, 53, 62, 22]\n",
      "tensor(31.1533)\n",
      "Round 153, Average loss -0.375\n",
      "[24, 85, 62, 47, 66, 36, 30, 11, 38, 53]\n",
      "tensor(32.5650)\n",
      "Round 154, Average loss -0.369\n",
      "[24, 85, 62, 47, 30, 53, 36, 66, 61, 88]\n",
      "tensor(28.0483)\n",
      "Round 155, Average loss -0.388\n",
      "[24, 85, 47, 53, 66, 30, 62, 96, 36, 48]\n",
      "tensor(27.9683)\n",
      "Round 156, Average loss -0.389\n",
      "[47, 85, 24, 66, 30, 88, 53, 36, 62, 38]\n",
      "tensor(30.6983)\n",
      "Round 157, Average loss -0.377\n",
      "[24, 47, 66, 53, 62, 85, 96, 30, 11, 36]\n",
      "tensor(24.6183)\n",
      "Round 158, Average loss -0.403\n",
      "[24, 85, 62, 53, 36, 66, 30, 47, 11, 38]\n",
      "tensor(29.0183)\n",
      "Round 159, Average loss -0.384\n",
      "[30, 66, 24, 85, 59, 62, 88, 53, 87, 11]\n",
      "tensor(11.2483)\n",
      "Round 160, Average loss -0.456\n",
      "saved\n",
      "[66, 24, 62, 85, 53, 47, 38, 36, 88, 30]\n",
      "tensor(14.7900)\n",
      "Round 161, Average loss -0.442\n",
      "[66, 24, 85, 47, 53, 30, 62, 38, 61, 36]\n",
      "tensor(19.8483)\n",
      "Round 162, Average loss -0.422\n",
      "[24, 85, 66, 30, 47, 88, 53, 38, 62, 11]\n",
      "tensor(23.7783)\n",
      "Round 163, Average loss -0.406\n",
      "[62, 85, 66, 30, 24, 36, 47, 53, 11, 61]\n",
      "tensor(25.3950)\n",
      "Round 164, Average loss -0.400\n",
      "[66, 24, 47, 53, 85, 30, 62, 36, 28, 38]\n",
      "tensor(27.5833)\n",
      "Round 165, Average loss -0.390\n",
      "[24, 66, 62, 85, 47, 36, 88, 30, 53, 61]\n",
      "tensor(26.4533)\n",
      "Round 166, Average loss -0.395\n"
     ]
    }
   ],
   "source": [
    "# training rl\n",
    "\n",
    "#先全部训练一次，然后找topk个客户端，用q来算权值。之后把global发送给所有local，开启第二轮训练。被选中的客户端训练两次，其他的训练一次。\n",
    "\n",
    "\n",
    "loss_train = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "\n",
    "loss_save = []\n",
    "m = max(int(args.frac * args.num_users), 1)\n",
    "constant = 2\n",
    "target_acc = 0.99\n",
    "args.emb = False\n",
    "# dqn = torch.load('doubledqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "# dqn = torch.load('dqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "\n",
    "last_replay_data = []\n",
    "for iter in range(args.epochs):\n",
    "    if iter==0:\n",
    "        random_n = 0\n",
    "        n_weight = []\n",
    "        while random_n<args.num_users*args.frac:\n",
    "            n_weight.append(random.random()) # 随机初始化参数\n",
    "            random_n+=1\n",
    "        action = random.sample(range(0,100),10)\n",
    "\n",
    "    #重置global参数    \n",
    "    if iter % args.reset_flag == 0:\n",
    "        if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "            net_glob = CNNCifar(args=args).to(args.device)\n",
    "        elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "            net_glob = CNNMnist(args=args).to(args.device)\n",
    "        elif args.model == 'mlp':\n",
    "            len_in = 1\n",
    "            for x in img_size:\n",
    "                len_in *= x\n",
    "            net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "        else:\n",
    "            exit('Error: unrecognized model')\n",
    "        \n",
    "        \n",
    "    loss_locals = []\n",
    "    w_locals = []\n",
    "    p_emb_collect = []\n",
    "    for i in layer_name:\n",
    "        if i == 'conv1':\n",
    "            emb_global = layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "        else:\n",
    "            emb_global += layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "    p_emb_collect.append(emb_global)\n",
    "    \n",
    "    for idx in range(100):\n",
    "        if idx in action:\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx], flag=True)\n",
    "        else:\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "#         if loss != 0:\n",
    "#             print('local loss:', loss)\n",
    "        #把parameter转成嵌入\n",
    "        \n",
    "        ##########  求均值方式  #######\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "        \n",
    "        #分母对应local网络的层数\n",
    "        avg_emb_feature = emb_feature/4\n",
    "        \n",
    "        ##########  拼接方式  ##########\n",
    "#         for i in layer_name:\n",
    "#             if i == 'conv1':\n",
    "#                 cat_emb = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "#             else:\n",
    "#                 cat_emb = torch.cat([cat_emb, layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))], 1)\n",
    "                \n",
    "        ############ 储存嵌入 ##############\n",
    "        p_emb_collect.append(avg_emb_feature)\n",
    "        ############ 储存参数 ##############\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "    p_emb_collect = torch.cat(p_emb_collect,1).unsqueeze(0).to(args.device)\n",
    "#     print(p_emb_collect)\n",
    "    choice, action_next = dqn.choose_action_run(p_emb_collect)\n",
    "    \n",
    "    ###########  计算当前轮的reward，然后将当前轮的reward添加到上一个replay_data中    \n",
    "    net_glob.eval()\n",
    "    global_acc, loss_train = test_img(net_glob, dataset_train, args)\n",
    "    print(global_acc)\n",
    "    reward = constant ** (global_acc.numpy()/100 - target_acc) - 1\n",
    "    loss_save.append(reward)\n",
    "    \n",
    "    if len(last_replay_data)==2:\n",
    "        last_replay_data.append(reward)#r\n",
    "        last_replay_data.append(p_emb_collect)#s_next\n",
    "        dqn.replay_buffer.add(last_replay_data[0], last_replay_data[1], last_replay_data[2], last_replay_data[3])\n",
    "    \n",
    "    last_replay_data = [p_emb_collect, choice]#s, a\n",
    "    action = choice\n",
    "    \n",
    "    # update global weights\n",
    "#     w_glob = FedAvg(w_locals)\n",
    "#     w_glob = FedPareto(w_locals, action_next)\n",
    "    \n",
    "    # copy weight to net_glob\n",
    "#     net_glob.train()\n",
    "#     net_glob.load_state_dict(w_locals[action])\n",
    "    \n",
    "    #ours\n",
    "    w_glob = FedPareto(w_locals, action_next, choice)\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    \n",
    "    # print loss\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, reward))\n",
    "#     loss_train.append(-reward)\n",
    "    \n",
    "    if iter > 0:\n",
    "        dqn.optimize()\n",
    "    if iter % 10 == 0:\n",
    "#         torch.save(dqn, 'dqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "        torch.save(dqn, 'doubledqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "        print(\"saved\")\n",
    "#     args.lr = max(args.lr*args.lr_decay, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(dqn, 'doubledqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[42, 64, 74, 54, 82, 45, 25, 11, 55, 98]\n",
      "Round   0, Average loss 0.463\n",
      "[74, 82, 42, 64, 72, 98, 11, 70, 45, 46]\n",
      "Round   1, Average loss 0.457\n",
      "[74, 82, 45, 98, 64, 42, 70, 97, 11, 72]\n",
      "Round   2, Average loss 0.157\n",
      "[74, 42, 82, 98, 46, 72, 45, 11, 70, 33]\n",
      "Round   3, Average loss 0.098\n",
      "[74, 82, 42, 11, 98, 46, 64, 70, 72, 95]\n",
      "Round   4, Average loss 0.077\n",
      "[74, 42, 82, 64, 30, 98, 11, 72, 70, 46]\n",
      "Round   5, Average loss 0.061\n",
      "[74, 64, 70, 11, 45, 30, 72, 42, 82, 46]\n",
      "Round   6, Average loss 0.049\n",
      "[74, 64, 82, 42, 98, 30, 11, 46, 86, 45]\n",
      "Round   7, Average loss 0.042\n",
      "[74, 82, 42, 98, 11, 64, 46, 45, 54, 30]\n",
      "Round   8, Average loss 0.036\n",
      "[74, 64, 42, 98, 82, 11, 30, 46, 45, 70]\n",
      "Round   9, Average loss 0.036\n"
     ]
    }
   ],
   "source": [
    "# fl\n",
    "#initialize\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob = CNNCifar(args=args).to(args.device)\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob = CNNMnist(args=args).to(args.device)\n",
    "elif args.model == 'mlp':\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "net_glob.train()\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "\n",
    "m = max(int(args.frac * args.num_users), 1)\n",
    "# dqn = torch.load('doubledqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "constant = 2\n",
    "target_acc = 0.98\n",
    "for iter in range(args.validation_epochs):\n",
    "    if iter==0:\n",
    "        random_n = 0\n",
    "        n_weight = []\n",
    "        while random_n<args.num_users*args.frac:\n",
    "            n_weight.append(random.random()) # 随机初始化参数\n",
    "            random_n+=1\n",
    "        l = list(range(0,100))\n",
    "        action = random.sample(l, m)\n",
    "      \n",
    "    loss_locals = []\n",
    "    w_locals = []\n",
    "    p_emb_collect = []\n",
    "    choice = []\n",
    "    \n",
    "    for i in layer_name:\n",
    "        if i == 'conv1':\n",
    "            emb_global = layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "        else:\n",
    "            emb_global += layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "    p_emb_collect.append(emb_global)\n",
    "    \n",
    "    for idx in range(100):\n",
    "        if idx in choice:\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx], flag=True)\n",
    "        else:\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        \n",
    "        #把parameter转成嵌入\n",
    "        \n",
    "        ##########  求均值方式  #######\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "        \n",
    "        #分母对应local网络的层数\n",
    "        avg_emb_feature = emb_feature/4\n",
    "        \n",
    "        \n",
    "        ##########  拼接方式  ##########\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                cat_emb = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "            else:\n",
    "                cat_emb = torch.cat([cat_emb, layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))], 1)\n",
    "                \n",
    "        ############ 储存嵌入 ##############\n",
    "        p_emb_collect.append(avg_emb_feature)\n",
    "        \n",
    "        ############ 储存参数 ##############\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        \n",
    "    p_emb_collect = torch.cat(p_emb_collect,1).unsqueeze(0).to(args.device)\n",
    "    choice, action_next = dqn.choose_action_run(p_emb_collect)\n",
    "    \n",
    "    ###########  计算当前轮的reward，然后将当前轮的reward添加到上一个replay_data中    \n",
    "#     net_glob.eval()\n",
    "    global_acc, loss_train = test_img(net_glob, dataset_train, args)\n",
    "    reward = constant ** (global_acc.numpy()/100 - target_acc) - 1\n",
    "\n",
    "    action = action_next\n",
    "    \n",
    "    # update global weights\n",
    "    #fedavg\n",
    "#     chosen_w = []\n",
    "#     for i in choice:\n",
    "#         chosen_w.append(w_locals[i])\n",
    "#     w_glob = FedAvg(chosen_w)\n",
    "    \n",
    "    #ours\n",
    "    w_glob = FedPareto(w_locals, action_next, choice)\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    # print loss\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, -reward))\n",
    "#     loss_train.append(-reward)\n",
    "\n",
    "#     args.lr = max(args.lr*args.lr_decay, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "def test_img(net_g, datatest, args):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=args.bs)\n",
    "    l = len(data_loader)\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        if args.gpu != -1:\n",
    "            data, target = data, target\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if args.verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 93.33\n",
      "Testing accuracy: 93.81\n"
     ]
    }
   ],
   "source": [
    "# plot loss curve\n",
    "# plt.figure()\n",
    "# plt.plot(range(len(loss_train)), loss_train)\n",
    "# plt.ylabel('train_loss')\n",
    "# plt.show()\n",
    "# plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
    "\n",
    "# testing\n",
    "net_glob.eval()\n",
    "acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(len(loss_save)), loss_save)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
