{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # federated arguments\n",
    "    #RL的训练轮次\n",
    "    parser.add_argument('--epochs', type=int, default=500, help=\"rounds of training\")\n",
    "    \n",
    "    #嵌入向量的训练轮次\n",
    "    parser.add_argument('--emb_train_epochs', type=int, default=10, help=\"rounds of training\")\n",
    "    parser.add_argument('--reset_flag', type=int, default=100, help=\"reset flag\")\n",
    "    #验证RL和Fedavg哪个更好的验证轮次\n",
    "    parser.add_argument('--validation_epochs', type=int, default=50, help=\"rounds of training\")\n",
    "    parser.add_argument('--divide', type=str, default='True')\n",
    "    \n",
    "    #将训练集分为几份\n",
    "    parser.add_argument('--divide_num', type=int, default=2, help=\"divide number\")\n",
    "    \n",
    "    #有多少个local client\n",
    "    parser.add_argument('--num_users', type=int, default=100, help=\"number of users: K\")\n",
    "    parser.add_argument('--k', type=int, default=10, help=\"k\")\n",
    "    \n",
    "    #每次选多少个local client参与训练\n",
    "    parser.add_argument('--frac', type=float, default=0.1, help=\"the fraction of clients: C\")\n",
    "    parser.add_argument('--train_frac', type=float, default=1, help=\"the fraction of training: C\")\n",
    "    \n",
    "    parser.add_argument('--collect_ep', type=int, default=1000, help=\"rounds of training\")\n",
    "    \n",
    "    #local client自己本地训练的轮次\n",
    "    parser.add_argument('--local_ep', type=int, default=20, help=\"the number of local epochs: E\")\n",
    "    \n",
    "    #验证环节的local clinet本地训练轮次\n",
    "    parser.add_argument('--local_validation_ep', type=int, default=10, help=\"the number of local epochs: E\")\n",
    "    \n",
    "    #local client本地训练的batchsize\n",
    "    parser.add_argument('--local_bs', type=int, default=10, help=\"local batch size: B\")\n",
    "    parser.add_argument('--bs', type=int, default=128, help=\"test batch size\")\n",
    "    \n",
    "    #RL的学习率和衰减率\n",
    "    parser.add_argument('--lr', type=float, default=0.01, help=\"learning rate (default: 0.01)\")\n",
    "    parser.add_argument('--lr_decay', type=float, default=1, help=\"lr decay\")\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, help=\"SGD momentum (default: 0.5)\")\n",
    "    parser.add_argument('--split', type=str, default='user', help=\"train-test split type, user or sample\")\n",
    "\n",
    "    # model arguments\n",
    "    \n",
    "    #使用的client 模型\n",
    "    parser.add_argument('--model', type=str, default='cnn', help='model name')\n",
    "    parser.add_argument('--kernel_num', type=int, default=9, help='number of each kind of kernel')\n",
    "    parser.add_argument('--kernel_sizes', type=str, default='3,4,5',\n",
    "                        help='comma-separated kernel size to use for convolution')\n",
    "    parser.add_argument('--norm', type=str, default='batch_norm', help=\"batch_norm, layer_norm, or None\")\n",
    "    parser.add_argument('--num_filters', type=int, default=32, help=\"number of filters for conv nets\")\n",
    "    parser.add_argument('--max_pool', type=str, default='True',\n",
    "                        help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    \n",
    "    #使用的数据集\n",
    "    parser.add_argument('--dataset', type=str, default='mnist', help=\"name of dataset\")\n",
    "    \n",
    "    #数据集的划分是否满足独立同分布\n",
    "    parser.add_argument('--iid', action='store_true', help='whether i.i.d or not')\n",
    "    \n",
    "    #输出的分类个数\n",
    "    parser.add_argument('--num_classes', type=int, default=10, help=\"number of classes\")\n",
    "    \n",
    "    #输入的图片的通道数\n",
    "    parser.add_argument('--num_channels', type=int, default=1, help=\"number of channels of imges\")\n",
    "    parser.add_argument('--gpu', type=int, default=-1, help=\"GPU ID, -1 for CPU\")\n",
    "    parser.add_argument('--stopping_rounds', type=int, default=10, help='rounds of early stopping')\n",
    "    parser.add_argument('--verbose', action='store_true', help='verbose print')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='random seed (default: 1)')\n",
    "    parser.add_argument('--all_clients', action='store_true', help='aggregation over all clients')\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from collections import deque\n",
    "  \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid, mnist_iid_drl_local_divide, cifar_iid_drl_local_divide, mnist_noniid_drl_local_divide\n",
    "from models.Update import LocalUpdate\n",
    "from models.Update_divide import LocalUpdate_divide\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, CNNCifarEmb, CNNCifarEmbReverse, CNNMnistEmb, CNNMnistEmbReverse\n",
    "from models.Fed import FedAvg\n",
    "# from models.test import test_img\n",
    "\n",
    "# parse args\n",
    "args = args_parser()\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.repeat(1,1,1))])\n",
    "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=True, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    args.iid = False\n",
    "    args.divide = False\n",
    "    #第一组作为drl训练，第二组作为和fedavg对比\n",
    "    if args.divide:\n",
    "        if args.iid:\n",
    "#             print(int(len(dataset_train)/args.divide_num))\n",
    "#             print(int(len(dataset_train)/args.num_users/args.divide_num))\n",
    "            dict_users = mnist_iid_drl_local_divide(dataset_train, args.num_users, args.divide_num)\n",
    "        else:\n",
    "            dict_users = mnist_noniid_drl_local_divide(dataset_train, args.num_users, args.divide_num)\n",
    "    else:\n",
    "        if args.iid:\n",
    "            dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "        else:\n",
    "            dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    dataset_train = datasets.CIFAR10('./data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('./data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "    args.iid = True\n",
    "    if args.divide:\n",
    "        if args.iid:\n",
    "#             print(int(len(dataset_train)/args.divide_num))\n",
    "#             print(int(len(dataset_train)/args.num_users/args.divide_num))\n",
    "            dict_users = cifar_iid_drl_local_divide(dataset_train, args.num_users, args.divide_num)\n",
    "        else:\n",
    "            exit('Error: only consider IID setting in CIFAR10')\n",
    "    else:\n",
    "        if args.iid:\n",
    "            dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "        else:\n",
    "            exit('Error: only consider IID setting in CIFAR10')\n",
    "else:\n",
    "    exit('Error: unrecognized dataset')\n",
    "img_size = dataset_train[0][0].shape\n",
    "\n",
    "# build model\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob = CNNCifar(args=args).to(args.device)\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob = CNNMnist(args=args).to(args.device)\n",
    "elif args.model == 'mlp':\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "print(net_glob)\n",
    "net_glob.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer: # MemoryBuffer类实现的功能：buffer内采样，往buffer里塞（sars）\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size) #buffer设置为双端队列\n",
    "        self.maxSize = size\n",
    "        self.len = 0\n",
    "        \n",
    "    def state_reco(self, s):\n",
    "        s_1 = [i[0] for i in s]\n",
    "        s_2 = [i[1] for i in s]\n",
    "        s_3 = [i[2] for i in s]\n",
    "        return [torch.cat(s_1),torch.cat(s_2),torch.cat(s_3)]\n",
    "\n",
    "    def sample(self, count):\n",
    "        \"\"\"\n",
    "        samples a random batch from the replay memory buffer\n",
    "        :param count: batch size\n",
    "        :return: batch (numpy array)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "        count = min(count, self.len)\n",
    "        batch = random.sample(self.buffer, count) # 随机取样\n",
    "\n",
    "        s_arr = [arr[0] for arr in batch]\n",
    "        a_arr = torch.cat([arr[1] for arr in batch])\n",
    "        r_arr = torch.tensor([arr[2] for arr in batch]).reshape(-1,1)\n",
    "        s1_arr = [arr[3] for arr in batch]\n",
    "\n",
    "        return self.state_reco(s_arr), a_arr, r_arr, self.state_reco(s1_arr)\n",
    "\n",
    "    def len(self):\n",
    "        return self.len\n",
    "\n",
    "    def add(self, s, a, r, s1):\n",
    "        \"\"\"\n",
    "        adds a particular transaction in the memory buffer\n",
    "        :param s: current state\n",
    "        :param a: action taken\n",
    "        :param r: reward received\n",
    "        :param s1: next state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        transition = (s,a,r,s1)\n",
    "        self.len += 1\n",
    "        if self.len > self.maxSize:\n",
    "            self.len = self.maxSize\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, parameter_dim, loss_dim, action_dim, args):\n",
    "        super(Actor, self).__init__()\n",
    "        self.args = args\n",
    "        self.parameter_dim = parameter_dim\n",
    "        self.loss_dim = loss_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(parameter_dim, action_dim)\n",
    "\n",
    "        self.fc2 = nn.Linear(action_dim*(int(args.num_users*args.frac*args.train_frac+2)),action_dim)\n",
    "\n",
    "    def forward(self, parameters, last_loss, last_weight):\n",
    "        parameter_lst = []\n",
    "        for i in range(self.action_dim):\n",
    "            parameter_lst.append(self.fc1(parameters[:,i,:]))\n",
    "        parameter_layer = torch.cat(parameter_lst,dim=1)\n",
    "        x = torch.cat([parameter_layer,last_loss, last_weight],dim=1)\n",
    "        action = F.softmax(self.fc2(x),dim=1)\n",
    "\n",
    "        return action\n",
    "\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, parameter_dim, loss_dim, action_dim, args):\n",
    "        super(Critic, self).__init__()\n",
    "        self.args = args\n",
    "        self.parameter_dim = parameter_dim\n",
    "        self.loss_dim = loss_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        self.fc1 = nn.Linear(parameter_dim,action_dim)\n",
    "\n",
    "        self.fc2 = nn.Linear(action_dim*(int(args.num_users*args.frac*args.train_frac+3)),1)\n",
    "\n",
    "    def forward(self, parameters, last_loss, last_weight, action):\n",
    "        parameter_lst = []\n",
    "        for i in range(self.action_dim):\n",
    "            parameter_lst.append(self.fc1(parameters[:,i,:]))\n",
    "        parameter_layer = torch.cat(parameter_lst,dim=1)\n",
    "        x = torch.cat([parameter_layer,last_loss, last_weight, action],dim=1)\n",
    "        q = self.fc2(x)\n",
    "\n",
    "        return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "GAMMA=0.8\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self, parameter_dim, loss_dim, action_dim, replay_buffer, args):\n",
    "        self.parameter_dim = parameter_dim\n",
    "        self.loss_dim = loss_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.iter = 0\n",
    "        #self.noise = utils.OrnsteinUhlenbeckActionNoise(self.action_dim)\n",
    "\n",
    "        self.actor = Actor(self.parameter_dim, \n",
    "                                 self.loss_dim, \n",
    "                                 self.action_dim, args).to(args.device)\n",
    "        self.target_actor = Actor(self.parameter_dim, \n",
    "                                 self.loss_dim, \n",
    "                                 self.action_dim, args).to(args.device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(),0.01)\n",
    "\n",
    "        self.critic = Critic(self.parameter_dim, \n",
    "                                 self.loss_dim, \n",
    "                                 self.action_dim, args).to(args.device)\n",
    "        self.target_critic = Critic(self.parameter_dim, \n",
    "                                 self.loss_dim, \n",
    "                                 self.action_dim, args).to(args.device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(),0.01)\n",
    "\n",
    "        self.hard_update(self.target_actor, self.actor)\n",
    "        self.hard_update(self.target_critic, self.critic)\n",
    "\n",
    "    def soft_update(self, target, source, tau):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "            target_param.data.copy_(\n",
    "                target_param.data * (1.0 - tau) + param.data * tau\n",
    "            )\n",
    "    def hard_update(self, target, source):\n",
    "        for target_param, param in zip(target.parameters(), source.parameters()):\n",
    "                target_param.data.copy_(param.data)\n",
    "                \n",
    "    def get_exploitation_action(self, parameters, last_loss, last_weight):\n",
    "        action = self.target_actor.forward(parameters, last_loss, last_weight).detach()\n",
    "        return action.data.numpy()\n",
    "\n",
    "#     def get_exploration_action(self, state):\n",
    "#         \"\"\"\n",
    "#         gets the action from actor added with exploration noise\n",
    "#         :param state: state (Numpy array)\n",
    "#         :return: sampled action (Numpy array)\n",
    "#         \"\"\"\n",
    "#         state = Variable(torch.from_numpy(state))\n",
    "#         action = self.actor.forward(state).detach()\n",
    "#         new_action = action.data.numpy() + (self.noise.sample())\n",
    "#         return new_action\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Samples a random batch from replay memory and performs optimization\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        s1,a1,r1,s2 = self.replay_buffer.sample(8)\n",
    "\n",
    "#         s1 = Variable(torch.from_numpy(s1))\n",
    "#         a1 = Variable(torch.from_numpy(a1))\n",
    "#         r1 = Variable(torch.from_numpy(r1))\n",
    "#         s2 = Variable(torch.from_numpy(s2))\n",
    "\n",
    "        # ---------------------- optimize critic ----------------------\n",
    "        # Use target actor exploitation policy here for loss evaluation\n",
    "        # 这里应该是TD的方法\n",
    "        a2 = self.target_actor.forward(s2[0],s2[1],s2[2]).detach()\n",
    "        next_val = torch.squeeze(self.target_critic.forward(s2[0],s2[1],s2[2], a2).detach())\n",
    "        # y_exp = r + gamma*Q'( s2, pi'(s2))\n",
    "        y_expected = r1 + GAMMA*next_val\n",
    "        # y_pred = Q( s1, a1)\n",
    "        y_predicted = torch.squeeze(self.critic.forward(s1[0],s1[1],s1[2], a1))\n",
    "        # compute critic loss, and update the critic\n",
    "        loss_critic = F.smooth_l1_loss(y_predicted.float(), y_expected.float())\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        loss_critic.backward(retain_graph=True)\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # ---------------------- optimize actor ----------------------\n",
    "        pred_a1 = self.actor.forward(s1[0],s1[1],s1[2])\n",
    "        loss_actor = -1*torch.sum(self.critic.forward(s1[0],s1[1],s1[2], pred_a1))\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        loss_actor.backward(retain_graph=True)\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        self.soft_update(self.target_actor, self.actor, 0.001)\n",
    "        self.soft_update(self.target_critic, self.critic, 0.001)\n",
    "\n",
    "        if self.iter % 1 == 0:\n",
    "            print('Iteration :', self.iter, ' Loss_actor :', loss_actor.data.numpy(),\\\n",
    "                ' Loss_critic :', loss_critic.data.numpy())\n",
    "        self.iter += 1\n",
    "\n",
    "    def save_models(self, episode_count):\n",
    "        torch.save(self.target_actor.state_dict(), './Models/' + str(episode_count) + '_actor.pt')\n",
    "        torch.save(self.target_critic.state_dict(), './Models/' + str(episode_count) + '_critic.pt')\n",
    "        \n",
    "    def load_models(self, episode):\n",
    "        self.actor.load_state_dict(torch.load('./Models/' + str(episode) + '_actor.pt'))\n",
    "        self.critic.load_state_dict(torch.load('./Models/' + str(episode) + '_critic.pt'))\n",
    "        utils.hard_update(self.target_actor, self.actor)\n",
    "        utils.hard_update(self.target_critic, self.critic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramter = []\n",
    "for i in w_glob.keys():\n",
    "    paramter.append(w_glob[i].reshape(1,-1))\n",
    "\n",
    "parameter_dim = torch.cat(paramter,axis=1).shape[1]\n",
    "loss_dim = max(int(args.frac * args.num_users * args.train_frac), 1)\n",
    "action_dim = max(int(args.frac * args.num_users * args.train_frac), 1)\n",
    "# for i in w_glob.keys():\n",
    "#     print(i)\n",
    "\n",
    "# print(len(w_glob['conv1.weight'].reshape(1,-1).numpy()[0]))\n",
    "# print(len(w_glob['conv1.bias'].reshape(1,-1).numpy()[0]))\n",
    "# print(len(w_glob['conv2.weight'].reshape(1,-1).numpy()[0]))\n",
    "# print(len(w_glob['conv2.bias'].reshape(1,-1).numpy()[0]))\n",
    "# print(len(w_glob['fc1.weight'].reshape(1,-1).numpy()[0]))\n",
    "# print(len(w_glob['fc1.bias'].reshape(1,-1).numpy()[0]))\n",
    "# print(len(w_glob['fc2.weight'].reshape(1,-1).numpy()[0]))\n",
    "# print(len(w_glob['fc2.bias'].reshape(1,-1).numpy()[0]))\n",
    "# print(len(w_glob['fc3.weight'].reshape(1,-1).numpy()[0]))\n",
    "# print(len(w_glob['fc3.bias'].reshape(1,-1).numpy()[0]))\n",
    "\n",
    "# testcnn = CNNCifarEmb(len(w_glob['conv1.weight'].reshape(1,-1).numpy()[0]))\n",
    "# testcnn.forward(w_glob['conv1.weight'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss_avg:0.8267110586166382\n",
      "epoch:0, loss_avg:1.0634058713912964\n",
      "epoch:0, loss_avg:0.8738073110580444\n",
      "epoch:0, loss_avg:0.498268723487854\n",
      "epoch:0, loss_avg:0.642773449420929\n",
      "epoch:0, loss_avg:0.49711257219314575\n",
      "epoch:0, loss_avg:0.9371582269668579\n",
      "epoch:0, loss_avg:0.5596201419830322\n",
      "epoch:0, loss_avg:0.44442808628082275\n",
      "epoch:0, loss_avg:0.41640084981918335\n",
      "epoch:0, loss_avg:0.44640591740608215\n",
      "epoch:0, loss_avg:0.5192306041717529\n",
      "epoch:0, loss_avg:0.3565357029438019\n",
      "epoch:0, loss_avg:0.3365165591239929\n",
      "epoch:0, loss_avg:0.1825781911611557\n",
      "epoch:0, loss_avg:0.3818092346191406\n",
      "epoch:0, loss_avg:0.28852957487106323\n",
      "epoch:0, loss_avg:0.46920353174209595\n",
      "epoch:0, loss_avg:0.3629680275917053\n",
      "epoch:0, loss_avg:0.32713016867637634\n",
      "epoch:1, loss_avg:0.4226560592651367\n",
      "epoch:1, loss_avg:0.6488065719604492\n",
      "epoch:1, loss_avg:0.3188665211200714\n",
      "epoch:1, loss_avg:0.3069033622741699\n",
      "epoch:1, loss_avg:0.26423177123069763\n",
      "epoch:1, loss_avg:0.31888166069984436\n",
      "epoch:1, loss_avg:0.37041932344436646\n",
      "epoch:1, loss_avg:0.275429904460907\n",
      "epoch:1, loss_avg:0.3679780066013336\n",
      "epoch:1, loss_avg:0.28973788022994995\n",
      "epoch:1, loss_avg:0.4793894588947296\n",
      "epoch:1, loss_avg:0.24609807133674622\n",
      "epoch:1, loss_avg:0.3990301787853241\n",
      "epoch:1, loss_avg:0.4101957678794861\n",
      "epoch:1, loss_avg:0.8582848906517029\n",
      "epoch:1, loss_avg:0.3980559706687927\n",
      "epoch:1, loss_avg:0.37009891867637634\n",
      "epoch:1, loss_avg:0.3812863826751709\n",
      "epoch:1, loss_avg:0.5195807218551636\n",
      "epoch:1, loss_avg:0.3455241620540619\n",
      "epoch:2, loss_avg:0.2982338070869446\n",
      "epoch:2, loss_avg:0.46490126848220825\n",
      "epoch:2, loss_avg:0.32947492599487305\n",
      "epoch:2, loss_avg:0.46599286794662476\n",
      "epoch:2, loss_avg:0.346975713968277\n",
      "epoch:2, loss_avg:0.2609879970550537\n",
      "epoch:2, loss_avg:0.34537622332572937\n",
      "epoch:2, loss_avg:2.155487537384033\n",
      "epoch:2, loss_avg:0.5459403395652771\n",
      "epoch:2, loss_avg:0.36585789918899536\n",
      "epoch:2, loss_avg:0.3144112527370453\n",
      "epoch:2, loss_avg:0.38746994733810425\n",
      "epoch:2, loss_avg:0.5293249487876892\n",
      "epoch:2, loss_avg:0.4806370139122009\n",
      "epoch:2, loss_avg:0.5610718727111816\n",
      "epoch:2, loss_avg:0.3957531154155731\n",
      "epoch:2, loss_avg:0.36016061902046204\n",
      "epoch:2, loss_avg:0.33668044209480286\n",
      "epoch:2, loss_avg:0.352704256772995\n",
      "epoch:2, loss_avg:0.4413796663284302\n",
      "epoch:3, loss_avg:0.33875539898872375\n",
      "epoch:3, loss_avg:0.2737663686275482\n",
      "epoch:3, loss_avg:0.42499563097953796\n",
      "epoch:3, loss_avg:0.3859533667564392\n",
      "epoch:3, loss_avg:0.5403580665588379\n",
      "epoch:3, loss_avg:0.3041224479675293\n",
      "epoch:3, loss_avg:0.392566978931427\n",
      "epoch:3, loss_avg:0.3193308711051941\n",
      "epoch:3, loss_avg:0.3243492543697357\n",
      "epoch:3, loss_avg:0.28094378113746643\n",
      "epoch:3, loss_avg:0.5265962481498718\n",
      "epoch:3, loss_avg:0.35112351179122925\n",
      "epoch:3, loss_avg:0.3416822552680969\n",
      "epoch:3, loss_avg:0.4157361388206482\n",
      "epoch:3, loss_avg:0.1465609222650528\n",
      "epoch:3, loss_avg:0.13264113664627075\n",
      "epoch:3, loss_avg:0.3571595549583435\n",
      "epoch:3, loss_avg:0.12252869457006454\n",
      "epoch:3, loss_avg:0.41261014342308044\n",
      "epoch:3, loss_avg:0.41616469621658325\n",
      "epoch:4, loss_avg:0.5647373199462891\n",
      "epoch:4, loss_avg:0.4507729411125183\n",
      "epoch:4, loss_avg:0.553217351436615\n",
      "epoch:4, loss_avg:0.09727780520915985\n",
      "epoch:4, loss_avg:0.3045335114002228\n",
      "epoch:4, loss_avg:0.10801241546869278\n",
      "epoch:4, loss_avg:0.24337659776210785\n",
      "epoch:4, loss_avg:0.4080709218978882\n",
      "epoch:4, loss_avg:0.3846416473388672\n",
      "epoch:4, loss_avg:0.27201414108276367\n",
      "epoch:4, loss_avg:0.28743261098861694\n",
      "epoch:4, loss_avg:0.2969512343406677\n",
      "epoch:4, loss_avg:0.41092076897621155\n",
      "epoch:4, loss_avg:0.44750016927719116\n",
      "epoch:4, loss_avg:0.3196128010749817\n",
      "epoch:4, loss_avg:0.5141161680221558\n",
      "epoch:4, loss_avg:0.4251026511192322\n",
      "epoch:4, loss_avg:0.4288574457168579\n",
      "epoch:4, loss_avg:0.3842226266860962\n",
      "epoch:4, loss_avg:0.46319374442100525\n",
      "epoch:5, loss_avg:0.4275609254837036\n",
      "epoch:5, loss_avg:0.2845391631126404\n",
      "epoch:5, loss_avg:0.32391518354415894\n",
      "epoch:5, loss_avg:0.3409685492515564\n",
      "epoch:5, loss_avg:0.3606014847755432\n",
      "epoch:5, loss_avg:0.3685721158981323\n",
      "epoch:5, loss_avg:0.4733099341392517\n",
      "epoch:5, loss_avg:0.3199813961982727\n",
      "epoch:5, loss_avg:0.5106362700462341\n",
      "epoch:5, loss_avg:0.3774295747280121\n",
      "epoch:5, loss_avg:0.30865031480789185\n",
      "epoch:5, loss_avg:0.38118112087249756\n",
      "epoch:5, loss_avg:0.3727934658527374\n",
      "epoch:5, loss_avg:0.30497536063194275\n",
      "epoch:5, loss_avg:0.25030696392059326\n",
      "epoch:5, loss_avg:0.4559808373451233\n",
      "epoch:5, loss_avg:0.36719393730163574\n",
      "epoch:5, loss_avg:0.30834442377090454\n",
      "epoch:5, loss_avg:0.3511304259300232\n",
      "epoch:5, loss_avg:0.4276730418205261\n",
      "epoch:6, loss_avg:0.40759050846099854\n",
      "epoch:6, loss_avg:0.6222410202026367\n",
      "epoch:6, loss_avg:0.37762540578842163\n",
      "epoch:6, loss_avg:0.34050679206848145\n",
      "epoch:6, loss_avg:0.292030930519104\n",
      "epoch:6, loss_avg:0.3168123960494995\n",
      "epoch:6, loss_avg:0.3001275956630707\n",
      "epoch:6, loss_avg:0.35678935050964355\n",
      "epoch:6, loss_avg:0.2880191206932068\n",
      "epoch:6, loss_avg:0.7622615694999695\n",
      "epoch:6, loss_avg:0.2576294243335724\n",
      "epoch:6, loss_avg:0.4271204173564911\n",
      "epoch:6, loss_avg:0.39187073707580566\n",
      "epoch:6, loss_avg:0.33164411783218384\n",
      "epoch:6, loss_avg:0.37980884313583374\n",
      "epoch:6, loss_avg:0.3656495213508606\n",
      "epoch:6, loss_avg:0.3204571306705475\n",
      "epoch:6, loss_avg:0.47599852085113525\n",
      "epoch:6, loss_avg:0.3768811523914337\n",
      "epoch:6, loss_avg:0.2592669129371643\n",
      "epoch:7, loss_avg:0.2634403705596924\n",
      "epoch:7, loss_avg:0.47024059295654297\n",
      "epoch:7, loss_avg:0.34184902906417847\n",
      "epoch:7, loss_avg:0.42720288038253784\n",
      "epoch:7, loss_avg:0.42131659388542175\n",
      "epoch:7, loss_avg:0.3233059048652649\n",
      "epoch:7, loss_avg:0.25030818581581116\n",
      "epoch:7, loss_avg:0.38906311988830566\n",
      "epoch:7, loss_avg:0.4389868378639221\n",
      "epoch:7, loss_avg:0.5075565576553345\n",
      "epoch:7, loss_avg:0.39894673228263855\n",
      "epoch:7, loss_avg:0.26091280579566956\n",
      "epoch:7, loss_avg:0.23511230945587158\n",
      "epoch:7, loss_avg:0.20546138286590576\n",
      "epoch:7, loss_avg:0.35321593284606934\n",
      "epoch:7, loss_avg:0.3254305422306061\n",
      "epoch:7, loss_avg:0.4541220963001251\n",
      "epoch:7, loss_avg:0.2895244061946869\n",
      "epoch:7, loss_avg:0.4717966914176941\n",
      "epoch:7, loss_avg:0.3944416046142578\n",
      "epoch:8, loss_avg:0.42441996932029724\n",
      "epoch:8, loss_avg:0.31049880385398865\n",
      "epoch:8, loss_avg:0.5335521101951599\n",
      "epoch:8, loss_avg:0.2744385898113251\n",
      "epoch:8, loss_avg:0.2693919837474823\n",
      "epoch:8, loss_avg:0.34597069025039673\n",
      "epoch:8, loss_avg:0.27712690830230713\n",
      "epoch:8, loss_avg:0.288752019405365\n",
      "epoch:8, loss_avg:0.4704786539077759\n",
      "epoch:8, loss_avg:0.3720342218875885\n",
      "epoch:8, loss_avg:0.30357033014297485\n",
      "epoch:8, loss_avg:0.262146919965744\n",
      "epoch:8, loss_avg:0.28507205843925476\n",
      "epoch:8, loss_avg:0.2694738507270813\n",
      "epoch:8, loss_avg:0.5378971099853516\n",
      "epoch:8, loss_avg:0.38221627473831177\n",
      "epoch:8, loss_avg:0.2802009582519531\n",
      "epoch:8, loss_avg:0.42487841844558716\n",
      "epoch:8, loss_avg:0.2568424344062805\n",
      "epoch:8, loss_avg:0.5184215307235718\n",
      "epoch:9, loss_avg:0.4048953652381897\n",
      "epoch:9, loss_avg:0.38650816679000854\n",
      "epoch:9, loss_avg:0.40749865770339966\n",
      "epoch:9, loss_avg:0.35760992765426636\n",
      "epoch:9, loss_avg:0.3957129120826721\n",
      "epoch:9, loss_avg:0.3133313059806824\n",
      "epoch:9, loss_avg:0.23097655177116394\n",
      "epoch:9, loss_avg:0.510149359703064\n",
      "epoch:9, loss_avg:0.27559971809387207\n",
      "epoch:9, loss_avg:0.22738507390022278\n",
      "epoch:9, loss_avg:0.2260701060295105\n",
      "epoch:9, loss_avg:0.3555259108543396\n",
      "epoch:9, loss_avg:0.34401118755340576\n",
      "epoch:9, loss_avg:0.38864514231681824\n",
      "epoch:9, loss_avg:0.44638919830322266\n",
      "epoch:9, loss_avg:0.49250322580337524\n",
      "epoch:9, loss_avg:0.2811267077922821\n",
      "epoch:9, loss_avg:0.1884164959192276\n",
      "epoch:9, loss_avg:0.30956366658210754\n",
      "epoch:9, loss_avg:0.3166992664337158\n"
     ]
    }
   ],
   "source": [
    "layer_dict = {}\n",
    "layer_name = []\n",
    "count = 0\n",
    "for name in w_glob.keys():\n",
    "    if count % 2 == 0:\n",
    "        layer_name.append(name.split('.',1)[0])\n",
    "    count += 1\n",
    "    \n",
    "for i in layer_name:\n",
    "#     layer_dict[i] = CNNCifarEmb(torch.cat([w_glob[i+'.weight'].reshape(1,-1), w_glob[i+'.bias'].reshape(1,-1)], 1).numel())\n",
    "    layer_dict[i] = CNNMnistEmb(torch.cat([w_glob[i+'.weight'].reshape(1,-1), w_glob[i+'.bias'].reshape(1,-1)], 1).numel())\n",
    "# emb_reverse = CNNCifarEmbReverse(args)\n",
    "emb_reverse = CNNMnistEmbReverse(args)\n",
    "# print(w_glob[i+'.weight'].reshape(1,-1).numpy()[0])\n",
    "\n",
    "# print(w_glob[i+'.bias'].numpy())\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':layer_dict[layer_name[0]].parameters()},\n",
    "    {'params':layer_dict[layer_name[1]].parameters()},\n",
    "    {'params':layer_dict[layer_name[2]].parameters()},\n",
    "    {'params':layer_dict[layer_name[3]].parameters()},\n",
    "#     {'params':layer_dict[layer_name[4]].parameters()},\n",
    "    {'params':emb_reverse.parameters()}\n",
    "] ,0.01)\n",
    "\n",
    "for iter in range(args.emb_train_epochs):\n",
    "    idxs_users = np.random.choice(range(args.num_users), 20, replace=False)\n",
    "    \n",
    "    for idx in idxs_users:\n",
    "#         local = LocalUpdate_divide(args=args, dataset=dataset_train, idxs=dict_users[0][idx])\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "#         w, loss, loss_list = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1).to(args.device))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1).to(args.device))\n",
    "        avg_emb_feature = emb_feature/5\n",
    "        transform_w = emb_reverse.forward(avg_emb_feature)\n",
    "        loss_w = [sum((w[i].reshape(1,-1) - transform_w[i].reshape(1,-1)) ** 2) for i in w_glob.keys()]\n",
    "        loss_avg = 0\n",
    "        loss_check_dict = {}\n",
    "        for i in range(len(loss_w)):\n",
    "            loss_avg += sum(loss_w[i])/len(loss_w[i])\n",
    "            loss_check_dict[len(loss_w[i])] = loss_w[i]\n",
    "            if loss_avg.item() > 3.0:\n",
    "                print('len:{},w:{}'.format(len(loss_w[i]), loss_check_dict[len(loss_w[i])]))\n",
    "                print('loss_sum:', loss_avg)\n",
    "                print('\\n***************')\n",
    "        optimizer.zero_grad()\n",
    "        loss_avg.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        print('epoch:{}, loss_avg:{}'.format(iter, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = MemoryBuffer(500)\n",
    "trainer = Trainer(100, loss_dim, action_dim, replay_buffer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedPareto(w,action):\n",
    "    w_avg = copy.deepcopy(w[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(0, len(w)):\n",
    "            if i==0:\n",
    "                w_avg[k] = action[i] * w[i][k]\n",
    "            else:\n",
    "                w_avg[k] += action[i] * w[i][k]\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:21: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round   0, reward -0.100\n",
      "Round   0, rl loss 0.999\n",
      "Round   0, fedavg loss 1.051\n",
      "saved\n",
      "Round   1, reward -0.448\n",
      "Round   1, rl loss 4.476\n",
      "Round   1, fedavg loss 0.957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: Using a target size (torch.Size([1, 1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 0  Loss_actor : -30.310423  Loss_critic : 0.12013513\n",
      "Round   2, reward -1.003\n",
      "Round   2, rl loss 10.029\n",
      "Round   2, fedavg loss 1.684\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: Using a target size (torch.Size([2, 2])) that is different to the input size (torch.Size([2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 1  Loss_actor : -15.811584  Loss_critic : 38.102657\n",
      "Round   3, reward -1.247\n",
      "Round   3, rl loss 12.472\n",
      "Round   3, fedavg loss 8.701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: Using a target size (torch.Size([3, 3])) that is different to the input size (torch.Size([3])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 2  Loss_actor : 104.21046  Loss_critic : 8.056464\n",
      "Round   4, reward -2.142\n",
      "Round   4, rl loss 21.421\n",
      "Round   4, fedavg loss 13.094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: Using a target size (torch.Size([4, 4])) that is different to the input size (torch.Size([4])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 3  Loss_actor : 88.879166  Loss_critic : 38.463127\n",
      "Round   5, reward -3.729\n",
      "Round   5, rl loss 37.290\n",
      "Round   5, fedavg loss 20.300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: Using a target size (torch.Size([5, 5])) that is different to the input size (torch.Size([5])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 4  Loss_actor : -119.70024  Loss_critic : 26.322271\n",
      "Round   6, reward -3.360\n",
      "Round   6, rl loss 33.599\n",
      "Round   6, fedavg loss 17.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: Using a target size (torch.Size([6, 6])) that is different to the input size (torch.Size([6])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 5  Loss_actor : -163.57962  Loss_critic : 26.813955\n",
      "Round   7, reward -4.110\n",
      "Round   7, rl loss 41.096\n",
      "Round   7, fedavg loss 32.249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: Using a target size (torch.Size([7, 7])) that is different to the input size (torch.Size([7])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 6  Loss_actor : -34.884315  Loss_critic : 27.492817\n",
      "Round   8, reward -4.842\n",
      "Round   8, rl loss 48.423\n",
      "Round   8, fedavg loss 26.308\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:77: UserWarning: Using a target size (torch.Size([8, 8])) that is different to the input size (torch.Size([8])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 7  Loss_actor : 151.97385  Loss_critic : 6.357923\n",
      "Round   9, reward -2.994\n",
      "Round   9, rl loss 29.941\n",
      "Round   9, fedavg loss 23.824\n",
      "Iteration : 8  Loss_actor : 176.90237  Loss_critic : 18.947931\n",
      "Round  10, reward -2.740\n",
      "Round  10, rl loss 27.401\n",
      "Round  10, fedavg loss 26.727\n",
      "Iteration : 9  Loss_actor : 111.48115  Loss_critic : 25.633385\n",
      "saved\n",
      "Round  11, reward -2.323\n",
      "Round  11, rl loss 23.227\n",
      "Round  11, fedavg loss 19.659\n",
      "Iteration : 10  Loss_actor : -64.15719  Loss_critic : 11.1888\n",
      "Round  12, reward -1.549\n",
      "Round  12, rl loss 15.489\n",
      "Round  12, fedavg loss 13.671\n",
      "Iteration : 11  Loss_actor : -118.9982  Loss_critic : 9.370986\n",
      "Round  13, reward -2.736\n",
      "Round  13, rl loss 27.355\n",
      "Round  13, fedavg loss 26.854\n",
      "Iteration : 12  Loss_actor : -62.48469  Loss_critic : 14.7296915\n",
      "Round  14, reward -2.457\n",
      "Round  14, rl loss 24.573\n",
      "Round  14, fedavg loss 15.652\n",
      "Iteration : 13  Loss_actor : 58.059364  Loss_critic : 8.937932\n",
      "Round  15, reward -1.711\n",
      "Round  15, rl loss 17.111\n",
      "Round  15, fedavg loss 11.522\n",
      "Iteration : 14  Loss_actor : 84.52759  Loss_critic : 5.599441\n",
      "Round  16, reward -2.052\n",
      "Round  16, rl loss 20.520\n",
      "Round  16, fedavg loss 14.777\n",
      "Iteration : 15  Loss_actor : 30.338266  Loss_critic : 6.1408772\n",
      "Round  17, reward -3.689\n",
      "Round  17, rl loss 36.895\n",
      "Round  17, fedavg loss 24.536\n",
      "Iteration : 16  Loss_actor : -36.304806  Loss_critic : 2.363091\n",
      "Round  18, reward -3.236\n",
      "Round  18, rl loss 32.359\n",
      "Round  18, fedavg loss 24.105\n",
      "Iteration : 17  Loss_actor : -39.69767  Loss_critic : 4.943547\n",
      "Round  19, reward -2.090\n",
      "Round  19, rl loss 20.903\n",
      "Round  19, fedavg loss 12.423\n",
      "Iteration : 18  Loss_actor : 10.025163  Loss_critic : 4.1143603\n",
      "Round  20, reward -6.516\n",
      "Round  20, rl loss 65.156\n",
      "Round  20, fedavg loss 42.322\n",
      "Iteration : 19  Loss_actor : 67.691605  Loss_critic : 3.0472467\n",
      "saved\n",
      "Round  21, reward -4.668\n",
      "Round  21, rl loss 46.676\n",
      "Round  21, fedavg loss 35.697\n",
      "Iteration : 20  Loss_actor : 57.92071  Loss_critic : 7.6451073\n",
      "Round  22, reward -4.686\n",
      "Round  22, rl loss 46.863\n",
      "Round  22, fedavg loss 23.430\n",
      "Iteration : 21  Loss_actor : -11.09708  Loss_critic : 5.2412424\n",
      "Round  23, reward -2.948\n",
      "Round  23, rl loss 29.482\n",
      "Round  23, fedavg loss 24.601\n",
      "Iteration : 22  Loss_actor : -39.007225  Loss_critic : 3.7742016\n",
      "Round  24, reward -5.569\n",
      "Round  24, rl loss 55.694\n",
      "Round  24, fedavg loss 32.508\n",
      "Iteration : 23  Loss_actor : 2.640501  Loss_critic : 5.2277513\n",
      "Round  25, reward -4.561\n",
      "Round  25, rl loss 45.614\n",
      "Round  25, fedavg loss 35.146\n",
      "Iteration : 24  Loss_actor : 31.753098  Loss_critic : 2.1999264\n",
      "Round  26, reward -2.437\n",
      "Round  26, rl loss 24.369\n",
      "Round  26, fedavg loss 15.492\n",
      "Iteration : 25  Loss_actor : 9.589514  Loss_critic : 7.9620275\n",
      "Round  27, reward -5.038\n",
      "Round  27, rl loss 50.382\n",
      "Round  27, fedavg loss 29.126\n",
      "Iteration : 26  Loss_actor : -36.5018  Loss_critic : 3.9220445\n",
      "Round  28, reward -3.894\n",
      "Round  28, rl loss 38.944\n",
      "Round  28, fedavg loss 21.323\n",
      "Iteration : 27  Loss_actor : -31.93066  Loss_critic : 4.032616\n",
      "Round  29, reward -4.030\n",
      "Round  29, rl loss 40.299\n",
      "Round  29, fedavg loss 32.981\n",
      "Iteration : 28  Loss_actor : -5.1378384  Loss_critic : 5.043233\n",
      "Round  30, reward -2.598\n",
      "Round  30, rl loss 25.982\n",
      "Round  30, fedavg loss 26.281\n",
      "Iteration : 29  Loss_actor : 22.873928  Loss_critic : 3.3841655\n",
      "saved\n",
      "Round  31, reward -4.005\n",
      "Round  31, rl loss 40.053\n",
      "Round  31, fedavg loss 22.127\n",
      "Iteration : 30  Loss_actor : 16.800703  Loss_critic : 2.687717\n",
      "Round  32, reward -5.325\n",
      "Round  32, rl loss 53.249\n",
      "Round  32, fedavg loss 30.573\n",
      "Iteration : 31  Loss_actor : 16.255747  Loss_critic : 2.7979043\n",
      "Round  33, reward -2.920\n",
      "Round  33, rl loss 29.202\n",
      "Round  33, fedavg loss 27.131\n",
      "Iteration : 32  Loss_actor : 12.711587  Loss_critic : 5.962586\n",
      "Round  34, reward -4.349\n",
      "Round  34, rl loss 43.492\n",
      "Round  34, fedavg loss 25.815\n",
      "Iteration : 33  Loss_actor : -53.90628  Loss_critic : 4.9218445\n",
      "Round  35, reward -2.723\n",
      "Round  35, rl loss 27.226\n",
      "Round  35, fedavg loss 25.228\n",
      "Iteration : 34  Loss_actor : -30.364155  Loss_critic : 6.510288\n",
      "Round  36, reward -3.223\n",
      "Round  36, rl loss 32.228\n",
      "Round  36, fedavg loss 29.342\n",
      "Iteration : 35  Loss_actor : 12.1555805  Loss_critic : 3.6301703\n",
      "Round  37, reward -3.947\n",
      "Round  37, rl loss 39.473\n",
      "Round  37, fedavg loss 17.834\n",
      "Iteration : 36  Loss_actor : -20.079914  Loss_critic : 2.8811493\n",
      "Round  38, reward -2.676\n",
      "Round  38, rl loss 26.758\n",
      "Round  38, fedavg loss 19.197\n",
      "Iteration : 37  Loss_actor : -22.357082  Loss_critic : 3.8772118\n",
      "Round  39, reward -1.740\n",
      "Round  39, rl loss 17.400\n",
      "Round  39, fedavg loss 13.301\n",
      "Iteration : 38  Loss_actor : -9.304191  Loss_critic : 6.7787504\n",
      "Round  40, reward -6.236\n",
      "Round  40, rl loss 62.357\n",
      "Round  40, fedavg loss 43.069\n",
      "Iteration : 39  Loss_actor : 33.149204  Loss_critic : 1.88095\n",
      "saved\n",
      "Round  41, reward -5.172\n",
      "Round  41, rl loss 51.718\n",
      "Round  41, fedavg loss 22.379\n",
      "Iteration : 40  Loss_actor : 3.6347828  Loss_critic : 5.6895676\n",
      "Round  42, reward -3.268\n",
      "Round  42, rl loss 32.676\n",
      "Round  42, fedavg loss 21.027\n",
      "Iteration : 41  Loss_actor : -18.296875  Loss_critic : 6.605853\n",
      "Round  43, reward -3.945\n",
      "Round  43, rl loss 39.453\n",
      "Round  43, fedavg loss 24.514\n",
      "Iteration : 42  Loss_actor : -53.54438  Loss_critic : 2.0984366\n",
      "Round  44, reward -4.671\n",
      "Round  44, rl loss 46.710\n",
      "Round  44, fedavg loss 27.854\n",
      "Iteration : 43  Loss_actor : -24.703386  Loss_critic : 4.913465\n",
      "Round  45, reward -4.627\n",
      "Round  45, rl loss 46.271\n",
      "Round  45, fedavg loss 21.586\n",
      "Iteration : 44  Loss_actor : -6.2214336  Loss_critic : 1.6342528\n",
      "Round  46, reward -3.836\n",
      "Round  46, rl loss 38.364\n",
      "Round  46, fedavg loss 22.020\n",
      "Iteration : 45  Loss_actor : 16.147532  Loss_critic : 5.2760587\n",
      "Round  47, reward -2.198\n",
      "Round  47, rl loss 21.978\n",
      "Round  47, fedavg loss 24.040\n",
      "Iteration : 46  Loss_actor : 34.345978  Loss_critic : 4.00082\n",
      "Round  48, reward -3.893\n",
      "Round  48, rl loss 38.929\n",
      "Round  48, fedavg loss 29.198\n",
      "Iteration : 47  Loss_actor : 11.760038  Loss_critic : 4.0138826\n",
      "Round  49, reward -3.413\n",
      "Round  49, rl loss 34.129\n",
      "Round  49, fedavg loss 17.992\n",
      "Iteration : 48  Loss_actor : -10.20363  Loss_critic : 3.0183332\n",
      "Round  50, reward -3.100\n",
      "Round  50, rl loss 30.996\n",
      "Round  50, fedavg loss 24.219\n",
      "Iteration : 49  Loss_actor : -34.735073  Loss_critic : 1.7571888\n",
      "saved\n",
      "Round  51, reward -1.435\n",
      "Round  51, rl loss 14.353\n",
      "Round  51, fedavg loss 15.846\n",
      "Iteration : 50  Loss_actor : -1.769764  Loss_critic : 2.4195185\n",
      "Round  52, reward -2.271\n",
      "Round  52, rl loss 22.714\n",
      "Round  52, fedavg loss 18.189\n",
      "Iteration : 51  Loss_actor : 14.685701  Loss_critic : 1.9281094\n",
      "Round  53, reward -2.483\n",
      "Round  53, rl loss 24.829\n",
      "Round  53, fedavg loss 24.900\n",
      "Iteration : 52  Loss_actor : 10.221134  Loss_critic : 2.2418203\n",
      "Round  54, reward -2.342\n",
      "Round  54, rl loss 23.425\n",
      "Round  54, fedavg loss 13.282\n",
      "Iteration : 53  Loss_actor : 5.892461  Loss_critic : 4.800975\n",
      "Round  55, reward -5.147\n",
      "Round  55, rl loss 51.471\n",
      "Round  55, fedavg loss 31.097\n",
      "Iteration : 54  Loss_actor : -60.94277  Loss_critic : 13.355102\n",
      "Round  56, reward -3.761\n",
      "Round  56, rl loss 37.613\n",
      "Round  56, fedavg loss 19.560\n",
      "Iteration : 55  Loss_actor : -63.54267  Loss_critic : 10.957436\n",
      "Round  57, reward -3.701\n",
      "Round  57, rl loss 37.009\n",
      "Round  57, fedavg loss 20.256\n",
      "Iteration : 56  Loss_actor : 16.378855  Loss_critic : 4.687334\n",
      "Round  58, reward -4.879\n",
      "Round  58, rl loss 48.787\n",
      "Round  58, fedavg loss 34.404\n",
      "Iteration : 57  Loss_actor : 65.13241  Loss_critic : 8.097872\n",
      "Round  59, reward -5.431\n",
      "Round  59, rl loss 54.308\n",
      "Round  59, fedavg loss 17.329\n",
      "Iteration : 58  Loss_actor : -1.0778776  Loss_critic : 7.596198\n",
      "Round  60, reward -3.245\n",
      "Round  60, rl loss 32.454\n",
      "Round  60, fedavg loss 22.301\n",
      "Iteration : 59  Loss_actor : -41.017914  Loss_critic : 6.5819135\n",
      "saved\n",
      "Round  61, reward -4.824\n",
      "Round  61, rl loss 48.241\n",
      "Round  61, fedavg loss 27.757\n",
      "Iteration : 60  Loss_actor : -138.6102  Loss_critic : 4.4455423\n",
      "Round  62, reward -3.998\n",
      "Round  62, rl loss 39.975\n",
      "Round  62, fedavg loss 36.025\n",
      "Iteration : 61  Loss_actor : -1.2503314  Loss_critic : 10.038269\n",
      "Round  63, reward -2.339\n",
      "Round  63, rl loss 23.395\n",
      "Round  63, fedavg loss 16.390\n",
      "Iteration : 62  Loss_actor : 52.148956  Loss_critic : 4.426764\n",
      "Round  64, reward -3.822\n",
      "Round  64, rl loss 38.218\n",
      "Round  64, fedavg loss 20.341\n",
      "Iteration : 63  Loss_actor : 10.51765  Loss_critic : 8.935669\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round  65, reward -1.714\n",
      "Round  65, rl loss 17.141\n",
      "Round  65, fedavg loss 15.980\n",
      "Iteration : 64  Loss_actor : -26.382797  Loss_critic : 7.0202246\n",
      "Round  66, reward -4.240\n",
      "Round  66, rl loss 42.401\n",
      "Round  66, fedavg loss 25.003\n",
      "Iteration : 65  Loss_actor : -33.363148  Loss_critic : 4.9147277\n",
      "Round  67, reward -2.473\n",
      "Round  67, rl loss 24.732\n",
      "Round  67, fedavg loss 15.602\n",
      "Iteration : 66  Loss_actor : -5.3878126  Loss_critic : 7.750224\n",
      "Round  68, reward -4.513\n",
      "Round  68, rl loss 45.132\n",
      "Round  68, fedavg loss 27.873\n",
      "Iteration : 67  Loss_actor : 31.913572  Loss_critic : 5.2619486\n",
      "Round  69, reward -2.721\n",
      "Round  69, rl loss 27.210\n",
      "Round  69, fedavg loss 19.909\n",
      "Iteration : 68  Loss_actor : -20.227135  Loss_critic : 13.907407\n",
      "Round  70, reward -4.404\n",
      "Round  70, rl loss 44.037\n",
      "Round  70, fedavg loss 27.738\n",
      "Iteration : 69  Loss_actor : -50.8968  Loss_critic : 11.52011\n",
      "saved\n",
      "Round  71, reward -7.090\n",
      "Round  71, rl loss 70.899\n",
      "Round  71, fedavg loss 30.825\n",
      "Iteration : 70  Loss_actor : 86.229904  Loss_critic : 4.614481\n",
      "Round  72, reward -3.847\n",
      "Round  72, rl loss 38.469\n",
      "Round  72, fedavg loss 33.227\n",
      "Iteration : 71  Loss_actor : -15.19139  Loss_critic : 26.370657\n",
      "Round  73, reward -5.558\n",
      "Round  73, rl loss 55.579\n",
      "Round  73, fedavg loss 30.440\n",
      "Iteration : 72  Loss_actor : -33.212402  Loss_critic : 4.976915\n",
      "Round  74, reward -4.463\n",
      "Round  74, rl loss 44.630\n",
      "Round  74, fedavg loss 33.867\n",
      "Iteration : 73  Loss_actor : -26.862556  Loss_critic : 5.4702578\n",
      "Round  75, reward -6.799\n",
      "Round  75, rl loss 67.989\n",
      "Round  75, fedavg loss 35.595\n",
      "Iteration : 74  Loss_actor : -77.96857  Loss_critic : 6.57239\n",
      "Round  76, reward -5.092\n",
      "Round  76, rl loss 50.925\n",
      "Round  76, fedavg loss 31.246\n",
      "Iteration : 75  Loss_actor : -45.730663  Loss_critic : 4.6802707\n",
      "Round  77, reward -2.204\n",
      "Round  77, rl loss 22.036\n",
      "Round  77, fedavg loss 19.978\n",
      "Iteration : 76  Loss_actor : 141.69571  Loss_critic : 10.380068\n",
      "Round  78, reward -3.503\n",
      "Round  78, rl loss 35.034\n",
      "Round  78, fedavg loss 23.387\n",
      "Iteration : 77  Loss_actor : -5.610956  Loss_critic : 23.186403\n",
      "Round  79, reward -3.149\n",
      "Round  79, rl loss 31.490\n",
      "Round  79, fedavg loss 27.705\n",
      "Iteration : 78  Loss_actor : -167.79065  Loss_critic : 4.3773384\n",
      "Round  80, reward -3.140\n",
      "Round  80, rl loss 31.404\n",
      "Round  80, fedavg loss 19.333\n",
      "Iteration : 79  Loss_actor : -102.56627  Loss_critic : 26.466196\n",
      "saved\n",
      "Round  81, reward -2.149\n",
      "Round  81, rl loss 21.487\n",
      "Round  81, fedavg loss 22.791\n",
      "Iteration : 80  Loss_actor : 34.918304  Loss_critic : 9.343918\n",
      "Round  82, reward -2.425\n",
      "Round  82, rl loss 24.245\n",
      "Round  82, fedavg loss 18.094\n",
      "Iteration : 81  Loss_actor : 32.12016  Loss_critic : 13.5654745\n",
      "Round  83, reward -2.264\n",
      "Round  83, rl loss 22.644\n",
      "Round  83, fedavg loss 25.217\n",
      "Iteration : 82  Loss_actor : -104.933914  Loss_critic : 3.37041\n",
      "Round  84, reward -2.397\n",
      "Round  84, rl loss 23.974\n",
      "Round  84, fedavg loss 13.869\n",
      "Iteration : 83  Loss_actor : -61.22622  Loss_critic : 7.3267345\n",
      "Round  85, reward -3.845\n",
      "Round  85, rl loss 38.446\n",
      "Round  85, fedavg loss 29.915\n",
      "Iteration : 84  Loss_actor : 343.81607  Loss_critic : 15.557779\n",
      "Round  86, reward -2.087\n",
      "Round  86, rl loss 20.875\n",
      "Round  86, fedavg loss 13.194\n",
      "Iteration : 85  Loss_actor : 150.82292  Loss_critic : 46.608345\n",
      "Round  87, reward -3.402\n",
      "Round  87, rl loss 34.016\n",
      "Round  87, fedavg loss 22.748\n",
      "Iteration : 86  Loss_actor : -167.9533  Loss_critic : 16.507576\n",
      "Round  88, reward -4.392\n",
      "Round  88, rl loss 43.918\n",
      "Round  88, fedavg loss 27.121\n",
      "Iteration : 87  Loss_actor : -100.45673  Loss_critic : 10.665448\n",
      "Round  89, reward -3.476\n",
      "Round  89, rl loss 34.756\n",
      "Round  89, fedavg loss 19.142\n",
      "Iteration : 88  Loss_actor : 181.28583  Loss_critic : 18.096285\n",
      "Round  90, reward -2.420\n",
      "Round  90, rl loss 24.205\n",
      "Round  90, fedavg loss 17.815\n",
      "Iteration : 89  Loss_actor : 249.20451  Loss_critic : 27.661877\n",
      "saved\n",
      "Round  91, reward -4.109\n",
      "Round  91, rl loss 41.085\n",
      "Round  91, fedavg loss 22.981\n",
      "Iteration : 90  Loss_actor : 0.5179448  Loss_critic : 37.376938\n",
      "Round  92, reward -2.735\n",
      "Round  92, rl loss 27.354\n",
      "Round  92, fedavg loss 16.263\n",
      "Iteration : 91  Loss_actor : -69.55436  Loss_critic : 7.706567\n",
      "Round  93, reward -2.729\n",
      "Round  93, rl loss 27.293\n",
      "Round  93, fedavg loss 18.805\n",
      "Iteration : 92  Loss_actor : -22.043272  Loss_critic : 9.65961\n",
      "Round  94, reward -3.524\n",
      "Round  94, rl loss 35.244\n",
      "Round  94, fedavg loss 18.602\n",
      "Iteration : 93  Loss_actor : -5.2203903  Loss_critic : 18.432121\n",
      "Round  95, reward -4.552\n",
      "Round  95, rl loss 45.518\n",
      "Round  95, fedavg loss 22.500\n",
      "Iteration : 94  Loss_actor : -24.632507  Loss_critic : 9.265081\n",
      "Round  96, reward -4.543\n",
      "Round  96, rl loss 45.430\n",
      "Round  96, fedavg loss 44.939\n",
      "Iteration : 95  Loss_actor : -132.42915  Loss_critic : 12.016863\n",
      "Round  97, reward -1.938\n",
      "Round  97, rl loss 19.382\n",
      "Round  97, fedavg loss 21.073\n",
      "Iteration : 96  Loss_actor : 40.156536  Loss_critic : 14.387791\n",
      "Round  98, reward -2.661\n",
      "Round  98, rl loss 26.615\n",
      "Round  98, fedavg loss 32.489\n",
      "Iteration : 97  Loss_actor : 59.194992  Loss_critic : 11.290444\n",
      "Round  99, reward -2.786\n",
      "Round  99, rl loss 27.859\n",
      "Round  99, fedavg loss 26.911\n",
      "Iteration : 98  Loss_actor : -49.164536  Loss_critic : 15.515607\n",
      "Round 100, reward -0.085\n",
      "Round 100, rl loss 0.849\n",
      "Round 100, fedavg loss 0.877\n",
      "Iteration : 99  Loss_actor : -103.536606  Loss_critic : 11.494189\n",
      "saved\n",
      "Round 101, reward -0.101\n",
      "Round 101, rl loss 1.015\n",
      "Round 101, fedavg loss 0.851\n",
      "Iteration : 100  Loss_actor : 8.101334  Loss_critic : 19.587057\n",
      "Round 102, reward -0.608\n",
      "Round 102, rl loss 6.084\n",
      "Round 102, fedavg loss 1.086\n",
      "Iteration : 101  Loss_actor : 27.54186  Loss_critic : 8.112489\n",
      "Round 103, reward -0.402\n",
      "Round 103, rl loss 4.019\n",
      "Round 103, fedavg loss 2.373\n",
      "Iteration : 102  Loss_actor : 37.596924  Loss_critic : 14.510616\n",
      "Round 104, reward -0.712\n",
      "Round 104, rl loss 7.123\n",
      "Round 104, fedavg loss 2.748\n",
      "Iteration : 103  Loss_actor : -47.229996  Loss_critic : 13.468498\n",
      "Round 105, reward -1.256\n",
      "Round 105, rl loss 12.557\n",
      "Round 105, fedavg loss 5.624\n",
      "Iteration : 104  Loss_actor : -107.46472  Loss_critic : 8.469368\n",
      "Round 106, reward -2.035\n",
      "Round 106, rl loss 20.353\n",
      "Round 106, fedavg loss 9.374\n",
      "Iteration : 105  Loss_actor : -35.27731  Loss_critic : 8.146605\n",
      "Round 107, reward -2.092\n",
      "Round 107, rl loss 20.915\n",
      "Round 107, fedavg loss 13.745\n",
      "Iteration : 106  Loss_actor : -23.017647  Loss_critic : 7.343343\n",
      "Round 108, reward -2.766\n",
      "Round 108, rl loss 27.664\n",
      "Round 108, fedavg loss 18.068\n",
      "Iteration : 107  Loss_actor : -92.973526  Loss_critic : 11.372311\n",
      "Round 109, reward -2.557\n",
      "Round 109, rl loss 25.567\n",
      "Round 109, fedavg loss 19.477\n",
      "Iteration : 108  Loss_actor : 90.43047  Loss_critic : 8.975395\n",
      "Round 110, reward -2.433\n",
      "Round 110, rl loss 24.332\n",
      "Round 110, fedavg loss 17.626\n",
      "Iteration : 109  Loss_actor : -33.99052  Loss_critic : 13.288204\n",
      "saved\n",
      "Round 111, reward -2.988\n",
      "Round 111, rl loss 29.876\n",
      "Round 111, fedavg loss 17.292\n",
      "Iteration : 110  Loss_actor : -214.7963  Loss_critic : 11.761863\n",
      "Round 112, reward -4.200\n",
      "Round 112, rl loss 42.002\n",
      "Round 112, fedavg loss 17.251\n",
      "Iteration : 111  Loss_actor : 36.569775  Loss_critic : 14.892859\n",
      "Round 113, reward -4.741\n",
      "Round 113, rl loss 47.406\n",
      "Round 113, fedavg loss 32.055\n",
      "Iteration : 112  Loss_actor : 40.62047  Loss_critic : 8.12\n",
      "Round 114, reward -3.703\n",
      "Round 114, rl loss 37.032\n",
      "Round 114, fedavg loss 27.876\n",
      "Iteration : 113  Loss_actor : -188.00685  Loss_critic : 11.512723\n",
      "Round 115, reward -3.306\n",
      "Round 115, rl loss 33.063\n",
      "Round 115, fedavg loss 21.296\n",
      "Iteration : 114  Loss_actor : -89.86294  Loss_critic : 25.122938\n",
      "Round 116, reward -3.295\n",
      "Round 116, rl loss 32.946\n",
      "Round 116, fedavg loss 18.640\n",
      "Iteration : 115  Loss_actor : 130.70204  Loss_critic : 7.926115\n",
      "Round 117, reward -2.104\n",
      "Round 117, rl loss 21.041\n",
      "Round 117, fedavg loss 19.608\n",
      "Iteration : 116  Loss_actor : 26.862709  Loss_critic : 10.564743\n",
      "Round 118, reward -3.367\n",
      "Round 118, rl loss 33.670\n",
      "Round 118, fedavg loss 20.563\n",
      "Iteration : 117  Loss_actor : -167.60072  Loss_critic : 14.369619\n",
      "Round 119, reward -1.956\n",
      "Round 119, rl loss 19.557\n",
      "Round 119, fedavg loss 12.070\n",
      "Iteration : 118  Loss_actor : -216.77647  Loss_critic : 9.255961\n",
      "Round 120, reward -5.058\n",
      "Round 120, rl loss 50.579\n",
      "Round 120, fedavg loss 35.663\n",
      "Iteration : 119  Loss_actor : -77.87602  Loss_critic : 15.864388\n",
      "saved\n",
      "Round 121, reward -3.345\n",
      "Round 121, rl loss 33.454\n",
      "Round 121, fedavg loss 20.712\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 120  Loss_actor : 87.036606  Loss_critic : 19.236483\n",
      "Round 122, reward -5.483\n",
      "Round 122, rl loss 54.835\n",
      "Round 122, fedavg loss 30.152\n",
      "Iteration : 121  Loss_actor : 130.38676  Loss_critic : 10.580189\n",
      "Round 123, reward -2.590\n",
      "Round 123, rl loss 25.895\n",
      "Round 123, fedavg loss 21.093\n",
      "Iteration : 122  Loss_actor : -68.61432  Loss_critic : 20.933552\n",
      "Round 124, reward -5.404\n",
      "Round 124, rl loss 54.043\n",
      "Round 124, fedavg loss 30.366\n",
      "Iteration : 123  Loss_actor : -372.48642  Loss_critic : 9.056093\n",
      "Round 125, reward -2.775\n",
      "Round 125, rl loss 27.748\n",
      "Round 125, fedavg loss 28.833\n",
      "Iteration : 124  Loss_actor : -114.838554  Loss_critic : 34.389618\n",
      "Round 126, reward -2.643\n",
      "Round 126, rl loss 26.428\n",
      "Round 126, fedavg loss 24.318\n",
      "Iteration : 125  Loss_actor : 35.047848  Loss_critic : 16.730377\n",
      "Round 127, reward -2.593\n",
      "Round 127, rl loss 25.933\n",
      "Round 127, fedavg loss 14.751\n",
      "Iteration : 126  Loss_actor : 33.206863  Loss_critic : 5.7730927\n",
      "Round 128, reward -1.377\n",
      "Round 128, rl loss 13.775\n",
      "Round 128, fedavg loss 16.920\n",
      "Iteration : 127  Loss_actor : 16.078085  Loss_critic : 17.952904\n",
      "Round 129, reward -2.344\n",
      "Round 129, rl loss 23.441\n",
      "Round 129, fedavg loss 16.630\n",
      "Iteration : 128  Loss_actor : -97.800644  Loss_critic : 16.911495\n",
      "Round 130, reward -2.271\n",
      "Round 130, rl loss 22.709\n",
      "Round 130, fedavg loss 18.383\n",
      "Iteration : 129  Loss_actor : -132.93674  Loss_critic : 5.389142\n",
      "saved\n",
      "Round 131, reward -2.177\n",
      "Round 131, rl loss 21.770\n",
      "Round 131, fedavg loss 13.602\n",
      "Iteration : 130  Loss_actor : -231.04724  Loss_critic : 11.879362\n",
      "Round 132, reward -3.727\n",
      "Round 132, rl loss 37.270\n",
      "Round 132, fedavg loss 21.562\n",
      "Iteration : 131  Loss_actor : -126.075714  Loss_critic : 23.99226\n",
      "Round 133, reward -3.057\n",
      "Round 133, rl loss 30.566\n",
      "Round 133, fedavg loss 24.713\n",
      "Iteration : 132  Loss_actor : 175.21056  Loss_critic : 8.962393\n",
      "Round 134, reward -3.607\n",
      "Round 134, rl loss 36.069\n",
      "Round 134, fedavg loss 23.442\n",
      "Iteration : 133  Loss_actor : 217.65112  Loss_critic : 26.086445\n",
      "Round 135, reward -2.880\n",
      "Round 135, rl loss 28.802\n",
      "Round 135, fedavg loss 20.871\n",
      "Iteration : 134  Loss_actor : 6.598167  Loss_critic : 32.53579\n",
      "Round 136, reward -2.533\n",
      "Round 136, rl loss 25.331\n",
      "Round 136, fedavg loss 26.231\n",
      "Iteration : 135  Loss_actor : -176.0199  Loss_critic : 9.4174\n",
      "Round 137, reward -2.401\n",
      "Round 137, rl loss 24.006\n",
      "Round 137, fedavg loss 21.201\n",
      "Iteration : 136  Loss_actor : -45.210648  Loss_critic : 7.1258926\n",
      "Round 138, reward -4.038\n",
      "Round 138, rl loss 40.381\n",
      "Round 138, fedavg loss 24.264\n",
      "Iteration : 137  Loss_actor : 96.296585  Loss_critic : 6.516722\n",
      "Round 139, reward -4.891\n",
      "Round 139, rl loss 48.911\n",
      "Round 139, fedavg loss 30.516\n",
      "Iteration : 138  Loss_actor : 101.81883  Loss_critic : 22.13943\n",
      "Round 140, reward -4.817\n",
      "Round 140, rl loss 48.172\n",
      "Round 140, fedavg loss 26.427\n",
      "Iteration : 139  Loss_actor : -251.08195  Loss_critic : 18.4651\n",
      "saved\n",
      "Round 141, reward -2.284\n",
      "Round 141, rl loss 22.839\n",
      "Round 141, fedavg loss 19.082\n",
      "Iteration : 140  Loss_actor : -303.1201  Loss_critic : 17.54925\n",
      "Round 142, reward -5.667\n",
      "Round 142, rl loss 56.670\n",
      "Round 142, fedavg loss 36.233\n",
      "Iteration : 141  Loss_actor : -38.96305  Loss_critic : 18.900347\n",
      "Round 143, reward -4.090\n",
      "Round 143, rl loss 40.901\n",
      "Round 143, fedavg loss 29.938\n",
      "Iteration : 142  Loss_actor : 68.92723  Loss_critic : 7.7420554\n",
      "Round 144, reward -1.786\n",
      "Round 144, rl loss 17.860\n",
      "Round 144, fedavg loss 14.068\n",
      "Iteration : 143  Loss_actor : -121.26637  Loss_critic : 14.073256\n",
      "Round 145, reward -2.641\n",
      "Round 145, rl loss 26.411\n",
      "Round 145, fedavg loss 13.198\n",
      "Iteration : 144  Loss_actor : -89.44091  Loss_critic : 6.358344\n",
      "Round 146, reward -4.411\n",
      "Round 146, rl loss 44.114\n",
      "Round 146, fedavg loss 26.019\n",
      "Iteration : 145  Loss_actor : -177.70927  Loss_critic : 5.441282\n",
      "Round 147, reward -4.594\n",
      "Round 147, rl loss 45.942\n",
      "Round 147, fedavg loss 28.905\n",
      "Iteration : 146  Loss_actor : -118.68538  Loss_critic : 25.818848\n",
      "Round 148, reward -3.373\n",
      "Round 148, rl loss 33.729\n",
      "Round 148, fedavg loss 20.106\n",
      "Iteration : 147  Loss_actor : 23.495083  Loss_critic : 8.944387\n",
      "Round 149, reward -2.088\n",
      "Round 149, rl loss 20.877\n",
      "Round 149, fedavg loss 16.655\n",
      "Iteration : 148  Loss_actor : 63.686058  Loss_critic : 16.017498\n",
      "Round 150, reward -3.147\n",
      "Round 150, rl loss 31.475\n",
      "Round 150, fedavg loss 21.846\n",
      "Iteration : 149  Loss_actor : 40.566933  Loss_critic : 8.074235\n",
      "saved\n",
      "Round 151, reward -2.320\n",
      "Round 151, rl loss 23.198\n",
      "Round 151, fedavg loss 16.579\n",
      "Iteration : 150  Loss_actor : -32.98338  Loss_critic : 9.30331\n",
      "Round 152, reward -2.974\n",
      "Round 152, rl loss 29.739\n",
      "Round 152, fedavg loss 17.031\n",
      "Iteration : 151  Loss_actor : -113.94864  Loss_critic : 9.962204\n",
      "Round 153, reward -4.403\n",
      "Round 153, rl loss 44.032\n",
      "Round 153, fedavg loss 21.152\n",
      "Iteration : 152  Loss_actor : -101.955574  Loss_critic : 17.225416\n",
      "Round 154, reward -4.450\n",
      "Round 154, rl loss 44.504\n",
      "Round 154, fedavg loss 19.199\n",
      "Iteration : 153  Loss_actor : 47.663967  Loss_critic : 14.225482\n",
      "Round 155, reward -4.768\n",
      "Round 155, rl loss 47.683\n",
      "Round 155, fedavg loss 25.759\n",
      "Iteration : 154  Loss_actor : 66.082954  Loss_critic : 17.382284\n",
      "Round 156, reward -7.637\n",
      "Round 156, rl loss 76.367\n",
      "Round 156, fedavg loss 41.476\n",
      "Iteration : 155  Loss_actor : -136.94133  Loss_critic : 18.584421\n",
      "Round 157, reward -3.231\n",
      "Round 157, rl loss 32.314\n",
      "Round 157, fedavg loss 21.483\n",
      "Iteration : 156  Loss_actor : -154.6271  Loss_critic : 9.08648\n",
      "Round 158, reward -2.264\n",
      "Round 158, rl loss 22.638\n",
      "Round 158, fedavg loss 30.794\n",
      "Iteration : 157  Loss_actor : -58.31652  Loss_critic : 12.972941\n",
      "Round 159, reward -4.719\n",
      "Round 159, rl loss 47.193\n",
      "Round 159, fedavg loss 33.473\n",
      "Iteration : 158  Loss_actor : 57.851288  Loss_critic : 12.128609\n",
      "Round 160, reward -2.700\n",
      "Round 160, rl loss 26.996\n",
      "Round 160, fedavg loss 23.444\n",
      "Iteration : 159  Loss_actor : 162.95047  Loss_critic : 14.264822\n",
      "saved\n",
      "Round 161, reward -4.009\n",
      "Round 161, rl loss 40.094\n",
      "Round 161, fedavg loss 27.969\n",
      "Iteration : 160  Loss_actor : -12.799514  Loss_critic : 25.743496\n",
      "Round 162, reward -2.212\n",
      "Round 162, rl loss 22.121\n",
      "Round 162, fedavg loss 16.935\n",
      "Iteration : 161  Loss_actor : -201.23654  Loss_critic : 12.265069\n",
      "Round 163, reward -1.978\n",
      "Round 163, rl loss 19.781\n",
      "Round 163, fedavg loss 18.874\n",
      "Iteration : 162  Loss_actor : -105.966705  Loss_critic : 17.554272\n",
      "Round 164, reward -2.159\n",
      "Round 164, rl loss 21.591\n",
      "Round 164, fedavg loss 13.871\n",
      "Iteration : 163  Loss_actor : 155.92914  Loss_critic : 10.275414\n",
      "Round 165, reward -3.592\n",
      "Round 165, rl loss 35.922\n",
      "Round 165, fedavg loss 20.977\n",
      "Iteration : 164  Loss_actor : 78.07158  Loss_critic : 19.685474\n",
      "Round 166, reward -4.910\n",
      "Round 166, rl loss 49.100\n",
      "Round 166, fedavg loss 26.880\n",
      "Iteration : 165  Loss_actor : -52.246353  Loss_critic : 22.374668\n",
      "Round 167, reward -4.544\n",
      "Round 167, rl loss 45.440\n",
      "Round 167, fedavg loss 26.287\n",
      "Iteration : 166  Loss_actor : -290.44568  Loss_critic : 14.651245\n",
      "Round 168, reward -6.223\n",
      "Round 168, rl loss 62.231\n",
      "Round 168, fedavg loss 37.051\n",
      "Iteration : 167  Loss_actor : -168.24586  Loss_critic : 37.117554\n",
      "Round 169, reward -4.325\n",
      "Round 169, rl loss 43.247\n",
      "Round 169, fedavg loss 30.762\n",
      "Iteration : 168  Loss_actor : 185.75919  Loss_critic : 9.882752\n",
      "Round 170, reward -4.060\n",
      "Round 170, rl loss 40.602\n",
      "Round 170, fedavg loss 25.526\n",
      "Iteration : 169  Loss_actor : 59.851692  Loss_critic : 29.246147\n",
      "saved\n",
      "Round 171, reward -2.615\n",
      "Round 171, rl loss 26.149\n",
      "Round 171, fedavg loss 23.738\n",
      "Iteration : 170  Loss_actor : -117.619225  Loss_critic : 17.547\n",
      "Round 172, reward -4.745\n",
      "Round 172, rl loss 47.449\n",
      "Round 172, fedavg loss 25.503\n",
      "Iteration : 171  Loss_actor : -198.56248  Loss_critic : 18.253784\n",
      "Round 173, reward -3.507\n",
      "Round 173, rl loss 35.073\n",
      "Round 173, fedavg loss 26.797\n",
      "Iteration : 172  Loss_actor : -82.63269  Loss_critic : 13.955019\n",
      "Round 174, reward -2.137\n",
      "Round 174, rl loss 21.366\n",
      "Round 174, fedavg loss 14.830\n",
      "Iteration : 173  Loss_actor : 87.57649  Loss_critic : 14.310396\n",
      "Round 175, reward -1.817\n",
      "Round 175, rl loss 18.166\n",
      "Round 175, fedavg loss 12.480\n",
      "Iteration : 174  Loss_actor : 87.246796  Loss_critic : 6.1699696\n",
      "Round 176, reward -2.549\n",
      "Round 176, rl loss 25.486\n",
      "Round 176, fedavg loss 18.815\n",
      "Iteration : 175  Loss_actor : 26.103062  Loss_critic : 23.220875\n",
      "Round 177, reward -5.236\n",
      "Round 177, rl loss 52.363\n",
      "Round 177, fedavg loss 37.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 176  Loss_actor : -112.711075  Loss_critic : 5.4124665\n",
      "Round 178, reward -5.440\n",
      "Round 178, rl loss 54.403\n",
      "Round 178, fedavg loss 27.956\n",
      "Iteration : 177  Loss_actor : -94.13104  Loss_critic : 11.587206\n",
      "Round 179, reward -4.498\n",
      "Round 179, rl loss 44.984\n",
      "Round 179, fedavg loss 31.483\n",
      "Iteration : 178  Loss_actor : -33.70404  Loss_critic : 12.432005\n",
      "Round 180, reward -2.025\n",
      "Round 180, rl loss 20.255\n",
      "Round 180, fedavg loss 15.427\n",
      "Iteration : 179  Loss_actor : -27.63062  Loss_critic : 11.691986\n",
      "saved\n",
      "Round 181, reward -3.906\n",
      "Round 181, rl loss 39.059\n",
      "Round 181, fedavg loss 28.472\n",
      "Iteration : 180  Loss_actor : -75.62018  Loss_critic : 7.788928\n",
      "Round 182, reward -3.183\n",
      "Round 182, rl loss 31.831\n",
      "Round 182, fedavg loss 21.546\n",
      "Iteration : 181  Loss_actor : 160.44057  Loss_critic : 13.851082\n",
      "Round 183, reward -6.346\n",
      "Round 183, rl loss 63.458\n",
      "Round 183, fedavg loss 41.868\n",
      "Iteration : 182  Loss_actor : -37.469376  Loss_critic : 27.082241\n",
      "Round 184, reward -4.496\n",
      "Round 184, rl loss 44.960\n",
      "Round 184, fedavg loss 27.174\n",
      "Iteration : 183  Loss_actor : -293.3725  Loss_critic : 6.277379\n",
      "Round 185, reward -3.349\n",
      "Round 185, rl loss 33.489\n",
      "Round 185, fedavg loss 24.733\n",
      "Iteration : 184  Loss_actor : -198.48529  Loss_critic : 24.901941\n",
      "Round 186, reward -2.416\n",
      "Round 186, rl loss 24.165\n",
      "Round 186, fedavg loss 13.414\n",
      "Iteration : 185  Loss_actor : -1.5385014  Loss_critic : 21.214863\n",
      "Round 187, reward -4.381\n",
      "Round 187, rl loss 43.815\n",
      "Round 187, fedavg loss 28.375\n",
      "Iteration : 186  Loss_actor : 170.07039  Loss_critic : 7.490783\n",
      "Round 188, reward -4.268\n",
      "Round 188, rl loss 42.681\n",
      "Round 188, fedavg loss 25.234\n",
      "Iteration : 187  Loss_actor : 22.385641  Loss_critic : 18.930588\n",
      "Round 189, reward -5.175\n",
      "Round 189, rl loss 51.754\n",
      "Round 189, fedavg loss 44.398\n",
      "Iteration : 188  Loss_actor : -240.47943  Loss_critic : 12.266525\n",
      "Round 190, reward -3.847\n",
      "Round 190, rl loss 38.472\n",
      "Round 190, fedavg loss 27.346\n",
      "Iteration : 189  Loss_actor : -267.3765  Loss_critic : 23.205723\n",
      "saved\n",
      "Round 191, reward -2.122\n",
      "Round 191, rl loss 21.216\n",
      "Round 191, fedavg loss 18.284\n",
      "Iteration : 190  Loss_actor : -88.17592  Loss_critic : 35.558445\n",
      "Round 192, reward -3.009\n",
      "Round 192, rl loss 30.089\n",
      "Round 192, fedavg loss 32.964\n",
      "Iteration : 191  Loss_actor : 124.24071  Loss_critic : 6.591345\n",
      "Round 193, reward -2.889\n",
      "Round 193, rl loss 28.886\n",
      "Round 193, fedavg loss 19.279\n",
      "Iteration : 192  Loss_actor : 22.805706  Loss_critic : 14.74609\n",
      "Round 194, reward -3.568\n",
      "Round 194, rl loss 35.677\n",
      "Round 194, fedavg loss 18.459\n",
      "Iteration : 193  Loss_actor : -259.58878  Loss_critic : 12.120538\n",
      "Round 195, reward -5.405\n",
      "Round 195, rl loss 54.054\n",
      "Round 195, fedavg loss 34.963\n",
      "Iteration : 194  Loss_actor : -258.85574  Loss_critic : 23.52657\n",
      "Round 196, reward -2.589\n",
      "Round 196, rl loss 25.892\n",
      "Round 196, fedavg loss 26.812\n",
      "Iteration : 195  Loss_actor : -44.06511  Loss_critic : 30.894375\n",
      "Round 197, reward -1.780\n",
      "Round 197, rl loss 17.799\n",
      "Round 197, fedavg loss 18.581\n",
      "Iteration : 196  Loss_actor : 62.99091  Loss_critic : 6.829906\n",
      "Round 198, reward -2.909\n",
      "Round 198, rl loss 29.088\n",
      "Round 198, fedavg loss 18.762\n",
      "Iteration : 197  Loss_actor : 20.897156  Loss_critic : 9.704025\n",
      "Round 199, reward -5.145\n",
      "Round 199, rl loss 51.453\n",
      "Round 199, fedavg loss 25.641\n",
      "Iteration : 198  Loss_actor : -80.1948  Loss_critic : 7.433177\n",
      "Round 200, reward -0.070\n",
      "Round 200, rl loss 0.695\n",
      "Round 200, fedavg loss 0.794\n",
      "Iteration : 199  Loss_actor : -47.778294  Loss_critic : 10.929654\n",
      "saved\n",
      "Round 201, reward -0.708\n",
      "Round 201, rl loss 7.079\n",
      "Round 201, fedavg loss 1.092\n",
      "Iteration : 200  Loss_actor : -92.830765  Loss_critic : 6.9825697\n",
      "Round 202, reward -0.835\n",
      "Round 202, rl loss 8.346\n",
      "Round 202, fedavg loss 1.730\n",
      "Iteration : 201  Loss_actor : -120.413376  Loss_critic : 5.753578\n",
      "Round 203, reward -0.754\n",
      "Round 203, rl loss 7.540\n",
      "Round 203, fedavg loss 2.537\n",
      "Iteration : 202  Loss_actor : -88.60927  Loss_critic : 8.656979\n",
      "Round 204, reward -1.175\n",
      "Round 204, rl loss 11.747\n",
      "Round 204, fedavg loss 2.627\n",
      "Iteration : 203  Loss_actor : -75.099915  Loss_critic : 5.430955\n",
      "Round 205, reward -1.061\n",
      "Round 205, rl loss 10.607\n",
      "Round 205, fedavg loss 2.937\n",
      "Iteration : 204  Loss_actor : 107.75473  Loss_critic : 8.478821\n",
      "Round 206, reward -0.992\n",
      "Round 206, rl loss 9.922\n",
      "Round 206, fedavg loss 6.188\n",
      "Iteration : 205  Loss_actor : 76.16868  Loss_critic : 31.253609\n",
      "Round 207, reward -1.404\n",
      "Round 207, rl loss 14.041\n",
      "Round 207, fedavg loss 7.618\n",
      "Iteration : 206  Loss_actor : -131.8753  Loss_critic : 16.642046\n",
      "Round 208, reward -1.759\n",
      "Round 208, rl loss 17.590\n",
      "Round 208, fedavg loss 14.445\n",
      "Iteration : 207  Loss_actor : -98.98889  Loss_critic : 18.878017\n",
      "Round 209, reward -2.727\n",
      "Round 209, rl loss 27.272\n",
      "Round 209, fedavg loss 16.131\n",
      "Iteration : 208  Loss_actor : -19.270124  Loss_critic : 9.632295\n",
      "Round 210, reward -2.596\n",
      "Round 210, rl loss 25.960\n",
      "Round 210, fedavg loss 14.560\n",
      "Iteration : 209  Loss_actor : -10.355916  Loss_critic : 9.473482\n",
      "saved\n",
      "Round 211, reward -3.577\n",
      "Round 211, rl loss 35.769\n",
      "Round 211, fedavg loss 26.041\n",
      "Iteration : 210  Loss_actor : -15.290617  Loss_critic : 9.689092\n",
      "Round 212, reward -2.270\n",
      "Round 212, rl loss 22.699\n",
      "Round 212, fedavg loss 7.744\n",
      "Iteration : 211  Loss_actor : -17.410395  Loss_critic : 7.7752237\n",
      "Round 213, reward -3.366\n",
      "Round 213, rl loss 33.659\n",
      "Round 213, fedavg loss 16.190\n",
      "Iteration : 212  Loss_actor : -80.029175  Loss_critic : 3.6698847\n",
      "Round 214, reward -4.698\n",
      "Round 214, rl loss 46.977\n",
      "Round 214, fedavg loss 28.881\n",
      "Iteration : 213  Loss_actor : -0.7495246  Loss_critic : 4.2160463\n",
      "Round 215, reward -3.992\n",
      "Round 215, rl loss 39.918\n",
      "Round 215, fedavg loss 31.315\n",
      "Iteration : 214  Loss_actor : -13.993205  Loss_critic : 7.199974\n",
      "Round 216, reward -3.939\n",
      "Round 216, rl loss 39.395\n",
      "Round 216, fedavg loss 31.451\n",
      "Iteration : 215  Loss_actor : -104.93332  Loss_critic : 9.816627\n",
      "Round 217, reward -3.825\n",
      "Round 217, rl loss 38.245\n",
      "Round 217, fedavg loss 23.190\n",
      "Iteration : 216  Loss_actor : -31.001736  Loss_critic : 9.57595\n",
      "Round 218, reward -3.121\n",
      "Round 218, rl loss 31.214\n",
      "Round 218, fedavg loss 19.536\n",
      "Iteration : 217  Loss_actor : -96.889694  Loss_critic : 3.5399203\n",
      "Round 219, reward -2.453\n",
      "Round 219, rl loss 24.531\n",
      "Round 219, fedavg loss 25.579\n",
      "Iteration : 218  Loss_actor : -30.6511  Loss_critic : 4.691455\n",
      "Round 220, reward -1.443\n",
      "Round 220, rl loss 14.428\n",
      "Round 220, fedavg loss 16.605\n",
      "Iteration : 219  Loss_actor : 32.579002  Loss_critic : 6.4320836\n",
      "saved\n",
      "Round 221, reward -2.541\n",
      "Round 221, rl loss 25.407\n",
      "Round 221, fedavg loss 13.117\n",
      "Iteration : 220  Loss_actor : -21.25264  Loss_critic : 13.602342\n",
      "Round 222, reward -2.976\n",
      "Round 222, rl loss 29.759\n",
      "Round 222, fedavg loss 19.422\n",
      "Iteration : 221  Loss_actor : -35.37499  Loss_critic : 6.41717\n",
      "Round 223, reward -3.820\n",
      "Round 223, rl loss 38.202\n",
      "Round 223, fedavg loss 26.539\n",
      "Iteration : 222  Loss_actor : -91.888245  Loss_critic : 3.4736707\n",
      "Round 224, reward -3.058\n",
      "Round 224, rl loss 30.578\n",
      "Round 224, fedavg loss 22.880\n",
      "Iteration : 223  Loss_actor : -63.33113  Loss_critic : 9.533317\n",
      "Round 225, reward -2.392\n",
      "Round 225, rl loss 23.918\n",
      "Round 225, fedavg loss 26.563\n",
      "Iteration : 224  Loss_actor : 0.21966457  Loss_critic : 7.1028633\n",
      "Round 226, reward -4.890\n",
      "Round 226, rl loss 48.901\n",
      "Round 226, fedavg loss 22.593\n",
      "Iteration : 225  Loss_actor : 2.2282329  Loss_critic : 1.884046\n",
      "Round 227, reward -4.476\n",
      "Round 227, rl loss 44.765\n",
      "Round 227, fedavg loss 21.953\n",
      "Iteration : 226  Loss_actor : 75.41433  Loss_critic : 7.510758\n",
      "Round 228, reward -5.237\n",
      "Round 228, rl loss 52.372\n",
      "Round 228, fedavg loss 29.866\n",
      "Iteration : 227  Loss_actor : 60.366253  Loss_critic : 11.704829\n",
      "Round 229, reward -3.294\n",
      "Round 229, rl loss 32.935\n",
      "Round 229, fedavg loss 28.839\n",
      "Iteration : 228  Loss_actor : -103.63427  Loss_critic : 12.346549\n",
      "Round 230, reward -3.807\n",
      "Round 230, rl loss 38.074\n",
      "Round 230, fedavg loss 21.842\n",
      "Iteration : 229  Loss_actor : -104.85173  Loss_critic : 8.023074\n",
      "saved\n",
      "Round 231, reward -3.531\n",
      "Round 231, rl loss 35.306\n",
      "Round 231, fedavg loss 27.066\n",
      "Iteration : 230  Loss_actor : 48.236244  Loss_critic : 8.204889\n",
      "Round 232, reward -5.067\n",
      "Round 232, rl loss 50.672\n",
      "Round 232, fedavg loss 27.911\n",
      "Iteration : 231  Loss_actor : 183.05911  Loss_critic : 6.280578\n",
      "Round 233, reward -1.781\n",
      "Round 233, rl loss 17.810\n",
      "Round 233, fedavg loss 26.847\n",
      "Iteration : 232  Loss_actor : 98.86143  Loss_critic : 18.312454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 234, reward -3.863\n",
      "Round 234, rl loss 38.629\n",
      "Round 234, fedavg loss 25.224\n",
      "Iteration : 233  Loss_actor : -227.6299  Loss_critic : 25.395185\n",
      "Round 235, reward -4.752\n",
      "Round 235, rl loss 47.518\n",
      "Round 235, fedavg loss 30.418\n",
      "Iteration : 234  Loss_actor : -158.2676  Loss_critic : 17.251102\n",
      "Round 236, reward -4.142\n",
      "Round 236, rl loss 41.424\n",
      "Round 236, fedavg loss 21.267\n",
      "Iteration : 235  Loss_actor : -38.503754  Loss_critic : 19.588158\n",
      "Round 237, reward -6.791\n",
      "Round 237, rl loss 67.913\n",
      "Round 237, fedavg loss 33.967\n",
      "Iteration : 236  Loss_actor : 95.53559  Loss_critic : 7.399094\n",
      "Round 238, reward -3.447\n",
      "Round 238, rl loss 34.475\n",
      "Round 238, fedavg loss 26.643\n",
      "Iteration : 237  Loss_actor : 80.62784  Loss_critic : 22.418781\n",
      "Round 239, reward -2.720\n",
      "Round 239, rl loss 27.202\n",
      "Round 239, fedavg loss 21.353\n",
      "Iteration : 238  Loss_actor : 3.7896943  Loss_critic : 8.844753\n",
      "Round 240, reward -3.683\n",
      "Round 240, rl loss 36.829\n",
      "Round 240, fedavg loss 19.431\n",
      "Iteration : 239  Loss_actor : -53.076447  Loss_critic : 5.745063\n",
      "saved\n",
      "Round 241, reward -3.846\n",
      "Round 241, rl loss 38.459\n",
      "Round 241, fedavg loss 24.896\n",
      "Iteration : 240  Loss_actor : -65.33128  Loss_critic : 10.598871\n",
      "Round 242, reward -3.690\n",
      "Round 242, rl loss 36.897\n",
      "Round 242, fedavg loss 19.578\n",
      "Iteration : 241  Loss_actor : 24.715935  Loss_critic : 5.286231\n",
      "Round 243, reward -4.252\n",
      "Round 243, rl loss 42.522\n",
      "Round 243, fedavg loss 29.981\n",
      "Iteration : 242  Loss_actor : -9.950158  Loss_critic : 6.1507564\n",
      "Round 244, reward -5.336\n",
      "Round 244, rl loss 53.355\n",
      "Round 244, fedavg loss 28.564\n",
      "Iteration : 243  Loss_actor : -76.99642  Loss_critic : 9.413912\n",
      "Round 245, reward -2.931\n",
      "Round 245, rl loss 29.306\n",
      "Round 245, fedavg loss 24.876\n",
      "Iteration : 244  Loss_actor : -79.34543  Loss_critic : 6.083521\n",
      "Round 246, reward -3.882\n",
      "Round 246, rl loss 38.823\n",
      "Round 246, fedavg loss 29.433\n",
      "Iteration : 245  Loss_actor : -82.25924  Loss_critic : 10.382978\n",
      "Round 247, reward -2.509\n",
      "Round 247, rl loss 25.089\n",
      "Round 247, fedavg loss 17.648\n",
      "Iteration : 246  Loss_actor : -3.2182827  Loss_critic : 6.317053\n",
      "Round 248, reward -4.409\n",
      "Round 248, rl loss 44.094\n",
      "Round 248, fedavg loss 22.823\n",
      "Iteration : 247  Loss_actor : 6.7334585  Loss_critic : 5.890969\n",
      "Round 249, reward -5.183\n",
      "Round 249, rl loss 51.825\n",
      "Round 249, fedavg loss 26.904\n",
      "Iteration : 248  Loss_actor : -133.72728  Loss_critic : 6.203218\n",
      "Round 250, reward -2.844\n",
      "Round 250, rl loss 28.444\n",
      "Round 250, fedavg loss 23.439\n",
      "Iteration : 249  Loss_actor : -122.941025  Loss_critic : 5.0596576\n",
      "saved\n",
      "Round 251, reward -4.039\n",
      "Round 251, rl loss 40.392\n",
      "Round 251, fedavg loss 25.151\n",
      "Iteration : 250  Loss_actor : -144.93774  Loss_critic : 10.132111\n",
      "Round 252, reward -3.999\n",
      "Round 252, rl loss 39.989\n",
      "Round 252, fedavg loss 23.109\n",
      "Iteration : 251  Loss_actor : 46.583286  Loss_critic : 19.001295\n",
      "Round 253, reward -2.795\n",
      "Round 253, rl loss 27.946\n",
      "Round 253, fedavg loss 33.252\n",
      "Iteration : 252  Loss_actor : 34.953  Loss_critic : 5.4869285\n",
      "Round 254, reward -6.050\n",
      "Round 254, rl loss 60.497\n",
      "Round 254, fedavg loss 39.436\n",
      "Iteration : 253  Loss_actor : -85.229454  Loss_critic : 10.807228\n",
      "Round 255, reward -4.417\n",
      "Round 255, rl loss 44.166\n",
      "Round 255, fedavg loss 27.033\n",
      "Iteration : 254  Loss_actor : -230.39862  Loss_critic : 7.0396223\n",
      "Round 256, reward -4.439\n",
      "Round 256, rl loss 44.388\n",
      "Round 256, fedavg loss 29.561\n",
      "Iteration : 255  Loss_actor : -165.48706  Loss_critic : 29.884968\n",
      "Round 257, reward -3.574\n",
      "Round 257, rl loss 35.743\n",
      "Round 257, fedavg loss 17.295\n",
      "Iteration : 256  Loss_actor : 6.28051  Loss_critic : 15.945734\n",
      "Round 258, reward -4.302\n",
      "Round 258, rl loss 43.024\n",
      "Round 258, fedavg loss 21.301\n",
      "Iteration : 257  Loss_actor : 22.191776  Loss_critic : 12.131545\n",
      "Round 259, reward -2.489\n",
      "Round 259, rl loss 24.888\n",
      "Round 259, fedavg loss 26.978\n",
      "Iteration : 258  Loss_actor : 3.765571  Loss_critic : 11.174563\n",
      "Round 260, reward -1.956\n",
      "Round 260, rl loss 19.562\n",
      "Round 260, fedavg loss 20.603\n",
      "Iteration : 259  Loss_actor : -131.47841  Loss_critic : 10.249475\n",
      "saved\n",
      "Round 261, reward -2.620\n",
      "Round 261, rl loss 26.204\n",
      "Round 261, fedavg loss 11.953\n",
      "Iteration : 260  Loss_actor : -115.30279  Loss_critic : 17.841278\n",
      "Round 262, reward -2.979\n",
      "Round 262, rl loss 29.785\n",
      "Round 262, fedavg loss 25.961\n",
      "Iteration : 261  Loss_actor : -0.18041301  Loss_critic : 15.658508\n",
      "Round 263, reward -4.316\n",
      "Round 263, rl loss 43.163\n",
      "Round 263, fedavg loss 21.030\n",
      "Iteration : 262  Loss_actor : 25.553638  Loss_critic : 8.177781\n",
      "Round 264, reward -3.649\n",
      "Round 264, rl loss 36.489\n",
      "Round 264, fedavg loss 28.427\n",
      "Iteration : 263  Loss_actor : -45.829876  Loss_critic : 6.9719987\n",
      "Round 265, reward -5.901\n",
      "Round 265, rl loss 59.007\n",
      "Round 265, fedavg loss 30.960\n",
      "Iteration : 264  Loss_actor : -138.35344  Loss_critic : 8.868772\n",
      "Round 266, reward -5.009\n",
      "Round 266, rl loss 50.092\n",
      "Round 266, fedavg loss 33.275\n",
      "Iteration : 265  Loss_actor : -123.724365  Loss_critic : 7.657562\n",
      "Round 267, reward -5.031\n",
      "Round 267, rl loss 50.314\n",
      "Round 267, fedavg loss 29.172\n",
      "Iteration : 266  Loss_actor : -2.752469  Loss_critic : 9.710968\n",
      "Round 268, reward -2.989\n",
      "Round 268, rl loss 29.890\n",
      "Round 268, fedavg loss 22.396\n",
      "Iteration : 267  Loss_actor : 52.135933  Loss_critic : 4.8089585\n",
      "Round 269, reward -4.952\n",
      "Round 269, rl loss 49.516\n",
      "Round 269, fedavg loss 29.202\n",
      "Iteration : 268  Loss_actor : -24.36124  Loss_critic : 8.898084\n",
      "Round 270, reward -4.009\n",
      "Round 270, rl loss 40.087\n",
      "Round 270, fedavg loss 29.897\n",
      "Iteration : 269  Loss_actor : -41.378628  Loss_critic : 8.551586\n",
      "saved\n",
      "Round 271, reward -3.118\n",
      "Round 271, rl loss 31.181\n",
      "Round 271, fedavg loss 26.564\n",
      "Iteration : 270  Loss_actor : -56.89871  Loss_critic : 4.91337\n",
      "Round 272, reward -2.758\n",
      "Round 272, rl loss 27.578\n",
      "Round 272, fedavg loss 21.779\n",
      "Iteration : 271  Loss_actor : -181.09148  Loss_critic : 10.850413\n",
      "Round 273, reward -3.993\n",
      "Round 273, rl loss 39.933\n",
      "Round 273, fedavg loss 20.443\n",
      "Iteration : 272  Loss_actor : -42.63597  Loss_critic : 9.418311\n",
      "Round 274, reward -2.886\n",
      "Round 274, rl loss 28.863\n",
      "Round 274, fedavg loss 17.883\n",
      "Iteration : 273  Loss_actor : -6.316151  Loss_critic : 3.754751\n",
      "Round 275, reward -3.469\n",
      "Round 275, rl loss 34.694\n",
      "Round 275, fedavg loss 19.979\n",
      "Iteration : 274  Loss_actor : 6.034445  Loss_critic : 5.6421003\n",
      "Round 276, reward -2.209\n",
      "Round 276, rl loss 22.090\n",
      "Round 276, fedavg loss 24.212\n",
      "Iteration : 275  Loss_actor : -21.421614  Loss_critic : 4.2975364\n",
      "Round 277, reward -3.786\n",
      "Round 277, rl loss 37.859\n",
      "Round 277, fedavg loss 21.702\n",
      "Iteration : 276  Loss_actor : -50.241665  Loss_critic : 11.992366\n",
      "Round 278, reward -4.675\n",
      "Round 278, rl loss 46.750\n",
      "Round 278, fedavg loss 22.202\n",
      "Iteration : 277  Loss_actor : -18.434689  Loss_critic : 11.254624\n",
      "Round 279, reward -3.863\n",
      "Round 279, rl loss 38.632\n",
      "Round 279, fedavg loss 36.057\n",
      "Iteration : 278  Loss_actor : -7.7981462  Loss_critic : 4.5277987\n",
      "Round 280, reward -2.769\n",
      "Round 280, rl loss 27.689\n",
      "Round 280, fedavg loss 27.693\n",
      "Iteration : 279  Loss_actor : -0.021805763  Loss_critic : 8.534559\n",
      "saved\n",
      "Round 281, reward -3.479\n",
      "Round 281, rl loss 34.790\n",
      "Round 281, fedavg loss 17.710\n",
      "Iteration : 280  Loss_actor : -23.294197  Loss_critic : 16.67737\n",
      "Round 282, reward -2.777\n",
      "Round 282, rl loss 27.767\n",
      "Round 282, fedavg loss 24.207\n",
      "Iteration : 281  Loss_actor : -11.809928  Loss_critic : 14.587352\n",
      "Round 283, reward -3.920\n",
      "Round 283, rl loss 39.200\n",
      "Round 283, fedavg loss 22.233\n",
      "Iteration : 282  Loss_actor : 18.234993  Loss_critic : 7.1061087\n",
      "Round 284, reward -5.495\n",
      "Round 284, rl loss 54.948\n",
      "Round 284, fedavg loss 31.436\n",
      "Iteration : 283  Loss_actor : -8.565936  Loss_critic : 5.7003794\n",
      "Round 285, reward -4.117\n",
      "Round 285, rl loss 41.168\n",
      "Round 285, fedavg loss 19.068\n",
      "Iteration : 284  Loss_actor : 5.5939813  Loss_critic : 5.3575106\n",
      "Round 286, reward -4.208\n",
      "Round 286, rl loss 42.075\n",
      "Round 286, fedavg loss 39.808\n",
      "Iteration : 285  Loss_actor : -11.246904  Loss_critic : 4.516934\n",
      "Round 287, reward -5.229\n",
      "Round 287, rl loss 52.291\n",
      "Round 287, fedavg loss 26.896\n",
      "Iteration : 286  Loss_actor : -53.739388  Loss_critic : 3.914621\n",
      "Round 288, reward -4.963\n",
      "Round 288, rl loss 49.628\n",
      "Round 288, fedavg loss 25.888\n",
      "Iteration : 287  Loss_actor : 5.443557  Loss_critic : 4.528875\n",
      "Round 289, reward -4.737\n",
      "Round 289, rl loss 47.366\n",
      "Round 289, fedavg loss 27.743\n",
      "Iteration : 288  Loss_actor : -38.361794  Loss_critic : 9.313072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 290, reward -5.205\n",
      "Round 290, rl loss 52.046\n",
      "Round 290, fedavg loss 29.557\n",
      "Iteration : 289  Loss_actor : -54.093155  Loss_critic : 2.9810576\n",
      "saved\n",
      "Round 291, reward -6.851\n",
      "Round 291, rl loss 68.512\n",
      "Round 291, fedavg loss 39.708\n",
      "Iteration : 290  Loss_actor : -5.196393  Loss_critic : 5.6602707\n",
      "Round 292, reward -3.802\n",
      "Round 292, rl loss 38.015\n",
      "Round 292, fedavg loss 34.373\n",
      "Iteration : 291  Loss_actor : -31.47489  Loss_critic : 9.558101\n",
      "Round 293, reward -4.240\n",
      "Round 293, rl loss 42.396\n",
      "Round 293, fedavg loss 34.021\n",
      "Iteration : 292  Loss_actor : -45.56451  Loss_critic : 5.662317\n",
      "Round 294, reward -3.472\n",
      "Round 294, rl loss 34.724\n",
      "Round 294, fedavg loss 29.880\n",
      "Iteration : 293  Loss_actor : -74.44887  Loss_critic : 8.804998\n",
      "Round 295, reward -3.281\n",
      "Round 295, rl loss 32.806\n",
      "Round 295, fedavg loss 27.773\n",
      "Iteration : 294  Loss_actor : 0.747385  Loss_critic : 8.13645\n",
      "Round 296, reward -1.777\n",
      "Round 296, rl loss 17.766\n",
      "Round 296, fedavg loss 26.145\n",
      "Iteration : 295  Loss_actor : 14.683656  Loss_critic : 7.5848985\n",
      "Round 297, reward -1.317\n",
      "Round 297, rl loss 13.169\n",
      "Round 297, fedavg loss 14.702\n",
      "Iteration : 296  Loss_actor : -15.809776  Loss_critic : 4.0017424\n",
      "Round 298, reward -1.881\n",
      "Round 298, rl loss 18.805\n",
      "Round 298, fedavg loss 12.558\n",
      "Iteration : 297  Loss_actor : -79.175316  Loss_critic : 11.13697\n",
      "Round 299, reward -2.812\n",
      "Round 299, rl loss 28.123\n",
      "Round 299, fedavg loss 20.547\n",
      "Iteration : 298  Loss_actor : -47.62727  Loss_critic : 14.033975\n",
      "Round 300, reward -0.104\n",
      "Round 300, rl loss 1.036\n",
      "Round 300, fedavg loss 1.079\n",
      "Iteration : 299  Loss_actor : 48.03387  Loss_critic : 5.836381\n",
      "saved\n",
      "Round 301, reward -0.369\n",
      "Round 301, rl loss 3.687\n",
      "Round 301, fedavg loss 0.869\n",
      "Iteration : 300  Loss_actor : 32.784668  Loss_critic : 10.354907\n",
      "Round 302, reward -0.598\n",
      "Round 302, rl loss 5.976\n",
      "Round 302, fedavg loss 1.464\n",
      "Iteration : 301  Loss_actor : -30.779575  Loss_critic : 15.619648\n",
      "Round 303, reward -0.696\n",
      "Round 303, rl loss 6.956\n",
      "Round 303, fedavg loss 1.843\n",
      "Iteration : 302  Loss_actor : -111.34466  Loss_critic : 10.010669\n",
      "Round 304, reward -1.140\n",
      "Round 304, rl loss 11.405\n",
      "Round 304, fedavg loss 2.329\n",
      "Iteration : 303  Loss_actor : -174.65945  Loss_critic : 11.071423\n",
      "Round 305, reward -0.979\n",
      "Round 305, rl loss 9.791\n",
      "Round 305, fedavg loss 4.331\n",
      "Iteration : 304  Loss_actor : 27.90866  Loss_critic : 15.580382\n",
      "Round 306, reward -1.886\n",
      "Round 306, rl loss 18.864\n",
      "Round 306, fedavg loss 4.892\n",
      "Iteration : 305  Loss_actor : 119.082214  Loss_critic : 13.385868\n",
      "Round 307, reward -2.481\n",
      "Round 307, rl loss 24.814\n",
      "Round 307, fedavg loss 7.822\n",
      "Iteration : 306  Loss_actor : -43.711098  Loss_critic : 14.020224\n",
      "Round 308, reward -3.484\n",
      "Round 308, rl loss 34.838\n",
      "Round 308, fedavg loss 20.217\n",
      "Iteration : 307  Loss_actor : -118.31223  Loss_critic : 6.4939294\n",
      "Round 309, reward -3.337\n",
      "Round 309, rl loss 33.373\n",
      "Round 309, fedavg loss 22.280\n",
      "Iteration : 308  Loss_actor : -72.84283  Loss_critic : 23.886202\n",
      "Round 310, reward -3.970\n",
      "Round 310, rl loss 39.698\n",
      "Round 310, fedavg loss 18.511\n",
      "Iteration : 309  Loss_actor : 10.342893  Loss_critic : 10.348911\n",
      "saved\n",
      "Round 311, reward -3.841\n",
      "Round 311, rl loss 38.408\n",
      "Round 311, fedavg loss 22.055\n",
      "Iteration : 310  Loss_actor : 34.3356  Loss_critic : 12.792454\n",
      "Round 312, reward -3.488\n",
      "Round 312, rl loss 34.879\n",
      "Round 312, fedavg loss 20.423\n",
      "Iteration : 311  Loss_actor : -11.043321  Loss_critic : 6.4060183\n",
      "Round 313, reward -3.409\n",
      "Round 313, rl loss 34.094\n",
      "Round 313, fedavg loss 15.649\n",
      "Iteration : 312  Loss_actor : -46.79686  Loss_critic : 8.118067\n",
      "Round 314, reward -3.885\n",
      "Round 314, rl loss 38.852\n",
      "Round 314, fedavg loss 29.013\n",
      "Iteration : 313  Loss_actor : -81.539665  Loss_critic : 8.766077\n",
      "Round 315, reward -5.887\n",
      "Round 315, rl loss 58.869\n",
      "Round 315, fedavg loss 30.396\n",
      "Iteration : 314  Loss_actor : -26.452744  Loss_critic : 9.291183\n",
      "Round 316, reward -3.780\n",
      "Round 316, rl loss 37.797\n",
      "Round 316, fedavg loss 29.833\n",
      "Iteration : 315  Loss_actor : 30.60636  Loss_critic : 6.2981515\n",
      "Round 317, reward -3.334\n",
      "Round 317, rl loss 33.343\n",
      "Round 317, fedavg loss 25.040\n",
      "Iteration : 316  Loss_actor : 14.8520355  Loss_critic : 5.4078474\n",
      "Round 318, reward -1.635\n",
      "Round 318, rl loss 16.351\n",
      "Round 318, fedavg loss 13.498\n",
      "Iteration : 317  Loss_actor : -94.895164  Loss_critic : 4.4900303\n",
      "Round 319, reward -1.896\n",
      "Round 319, rl loss 18.958\n",
      "Round 319, fedavg loss 11.004\n",
      "Iteration : 318  Loss_actor : -108.60753  Loss_critic : 6.612626\n",
      "Round 320, reward -2.584\n",
      "Round 320, rl loss 25.844\n",
      "Round 320, fedavg loss 18.456\n",
      "Iteration : 319  Loss_actor : -59.6157  Loss_critic : 12.386361\n",
      "saved\n",
      "Round 321, reward -1.826\n",
      "Round 321, rl loss 18.256\n",
      "Round 321, fedavg loss 22.865\n",
      "Iteration : 320  Loss_actor : 5.5418344  Loss_critic : 7.626074\n",
      "Round 322, reward -2.638\n",
      "Round 322, rl loss 26.384\n",
      "Round 322, fedavg loss 15.914\n",
      "Iteration : 321  Loss_actor : -28.205381  Loss_critic : 3.5604153\n",
      "Round 323, reward -3.439\n",
      "Round 323, rl loss 34.391\n",
      "Round 323, fedavg loss 20.651\n",
      "Iteration : 322  Loss_actor : -29.880165  Loss_critic : 11.914088\n",
      "Round 324, reward -3.052\n",
      "Round 324, rl loss 30.518\n",
      "Round 324, fedavg loss 25.758\n",
      "Iteration : 323  Loss_actor : -155.7706  Loss_critic : 5.616247\n",
      "Round 325, reward -5.767\n",
      "Round 325, rl loss 57.669\n",
      "Round 325, fedavg loss 37.904\n",
      "Iteration : 324  Loss_actor : -93.72531  Loss_critic : 15.3994465\n",
      "Round 326, reward -3.880\n",
      "Round 326, rl loss 38.799\n",
      "Round 326, fedavg loss 31.093\n",
      "Iteration : 325  Loss_actor : 37.137417  Loss_critic : 11.325059\n",
      "Round 327, reward -2.759\n",
      "Round 327, rl loss 27.589\n",
      "Round 327, fedavg loss 28.180\n",
      "Iteration : 326  Loss_actor : 40.001274  Loss_critic : 11.951811\n",
      "Round 328, reward -3.100\n",
      "Round 328, rl loss 30.996\n",
      "Round 328, fedavg loss 20.754\n",
      "Iteration : 327  Loss_actor : 20.772373  Loss_critic : 9.4600525\n",
      "Round 329, reward -2.129\n",
      "Round 329, rl loss 21.292\n",
      "Round 329, fedavg loss 11.607\n",
      "Iteration : 328  Loss_actor : -60.15779  Loss_critic : 11.775588\n",
      "Round 330, reward -3.256\n",
      "Round 330, rl loss 32.563\n",
      "Round 330, fedavg loss 19.996\n",
      "Iteration : 329  Loss_actor : -88.672104  Loss_critic : 8.219757\n",
      "saved\n",
      "Round 331, reward -3.839\n",
      "Round 331, rl loss 38.386\n",
      "Round 331, fedavg loss 29.504\n",
      "Iteration : 330  Loss_actor : 8.559623  Loss_critic : 17.187473\n",
      "Round 332, reward -2.467\n",
      "Round 332, rl loss 24.667\n",
      "Round 332, fedavg loss 27.640\n",
      "Iteration : 331  Loss_actor : 77.16452  Loss_critic : 6.058188\n",
      "Round 333, reward -4.426\n",
      "Round 333, rl loss 44.262\n",
      "Round 333, fedavg loss 26.066\n",
      "Iteration : 332  Loss_actor : -43.17949  Loss_critic : 20.572964\n",
      "Round 334, reward -4.970\n",
      "Round 334, rl loss 49.695\n",
      "Round 334, fedavg loss 26.136\n",
      "Iteration : 333  Loss_actor : -55.52948  Loss_critic : 8.212192\n",
      "Round 335, reward -3.994\n",
      "Round 335, rl loss 39.938\n",
      "Round 335, fedavg loss 33.119\n",
      "Iteration : 334  Loss_actor : -38.5913  Loss_critic : 2.6288855\n",
      "Round 336, reward -2.849\n",
      "Round 336, rl loss 28.485\n",
      "Round 336, fedavg loss 25.946\n",
      "Iteration : 335  Loss_actor : 22.86352  Loss_critic : 7.6177864\n",
      "Round 337, reward -3.961\n",
      "Round 337, rl loss 39.611\n",
      "Round 337, fedavg loss 22.660\n",
      "Iteration : 336  Loss_actor : 35.085705  Loss_critic : 8.638471\n",
      "Round 338, reward -2.597\n",
      "Round 338, rl loss 25.973\n",
      "Round 338, fedavg loss 23.942\n",
      "Iteration : 337  Loss_actor : -33.562588  Loss_critic : 10.931313\n",
      "Round 339, reward -2.318\n",
      "Round 339, rl loss 23.178\n",
      "Round 339, fedavg loss 15.450\n",
      "Iteration : 338  Loss_actor : -82.34347  Loss_critic : 8.242404\n",
      "Round 340, reward -5.692\n",
      "Round 340, rl loss 56.921\n",
      "Round 340, fedavg loss 37.443\n",
      "Iteration : 339  Loss_actor : -73.98389  Loss_critic : 12.733079\n",
      "saved\n",
      "Round 341, reward -2.867\n",
      "Round 341, rl loss 28.668\n",
      "Round 341, fedavg loss 22.960\n",
      "Iteration : 340  Loss_actor : 54.584763  Loss_critic : 13.114737\n",
      "Round 342, reward -4.749\n",
      "Round 342, rl loss 47.490\n",
      "Round 342, fedavg loss 30.678\n",
      "Iteration : 341  Loss_actor : 118.580376  Loss_critic : 12.266059\n",
      "Round 343, reward -3.700\n",
      "Round 343, rl loss 36.999\n",
      "Round 343, fedavg loss 30.738\n",
      "Iteration : 342  Loss_actor : 35.111534  Loss_critic : 13.132069\n",
      "Round 344, reward -3.007\n",
      "Round 344, rl loss 30.072\n",
      "Round 344, fedavg loss 17.925\n",
      "Iteration : 343  Loss_actor : -99.79967  Loss_critic : 12.487882\n",
      "Round 345, reward -2.726\n",
      "Round 345, rl loss 27.257\n",
      "Round 345, fedavg loss 24.993\n",
      "Iteration : 344  Loss_actor : -3.0205076  Loss_critic : 19.129736\n",
      "Round 346, reward -5.113\n",
      "Round 346, rl loss 51.128\n",
      "Round 346, fedavg loss 30.271\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 345  Loss_actor : 75.51717  Loss_critic : 3.5320642\n",
      "Round 347, reward -3.619\n",
      "Round 347, rl loss 36.189\n",
      "Round 347, fedavg loss 22.873\n",
      "Iteration : 346  Loss_actor : 0.2639537  Loss_critic : 25.317642\n",
      "Round 348, reward -4.955\n",
      "Round 348, rl loss 49.552\n",
      "Round 348, fedavg loss 24.236\n",
      "Iteration : 347  Loss_actor : -99.709236  Loss_critic : 4.8468494\n",
      "Round 349, reward -4.627\n",
      "Round 349, rl loss 46.267\n",
      "Round 349, fedavg loss 21.487\n",
      "Iteration : 348  Loss_actor : -61.443043  Loss_critic : 14.635476\n",
      "Round 350, reward -2.533\n",
      "Round 350, rl loss 25.334\n",
      "Round 350, fedavg loss 17.954\n",
      "Iteration : 349  Loss_actor : 126.76157  Loss_critic : 8.448574\n",
      "saved\n",
      "Round 351, reward -4.279\n",
      "Round 351, rl loss 42.793\n",
      "Round 351, fedavg loss 30.023\n",
      "Iteration : 350  Loss_actor : 58.196743  Loss_critic : 7.123741\n",
      "Round 352, reward -5.514\n",
      "Round 352, rl loss 55.143\n",
      "Round 352, fedavg loss 29.765\n",
      "Iteration : 351  Loss_actor : 7.3383784  Loss_critic : 8.275998\n",
      "Round 353, reward -5.732\n",
      "Round 353, rl loss 57.315\n",
      "Round 353, fedavg loss 31.361\n",
      "Iteration : 352  Loss_actor : -132.13171  Loss_critic : 12.974357\n",
      "Round 354, reward -3.404\n",
      "Round 354, rl loss 34.041\n",
      "Round 354, fedavg loss 22.516\n",
      "Iteration : 353  Loss_actor : -106.752235  Loss_critic : 12.278168\n",
      "Round 355, reward -3.027\n",
      "Round 355, rl loss 30.271\n",
      "Round 355, fedavg loss 31.927\n",
      "Iteration : 354  Loss_actor : 0.5541498  Loss_critic : 12.190594\n",
      "Round 356, reward -2.936\n",
      "Round 356, rl loss 29.365\n",
      "Round 356, fedavg loss 19.261\n",
      "Iteration : 355  Loss_actor : 38.414608  Loss_critic : 5.7606487\n",
      "Round 357, reward -3.143\n",
      "Round 357, rl loss 31.430\n",
      "Round 357, fedavg loss 32.955\n",
      "Iteration : 356  Loss_actor : -11.951733  Loss_critic : 4.803329\n",
      "Round 358, reward -1.626\n",
      "Round 358, rl loss 16.264\n",
      "Round 358, fedavg loss 14.592\n",
      "Iteration : 357  Loss_actor : -340.54694  Loss_critic : 12.116246\n",
      "Round 359, reward -2.473\n",
      "Round 359, rl loss 24.735\n",
      "Round 359, fedavg loss 12.792\n",
      "Iteration : 358  Loss_actor : -224.42204  Loss_critic : 30.238651\n",
      "Round 360, reward -1.937\n",
      "Round 360, rl loss 19.370\n",
      "Round 360, fedavg loss 19.697\n",
      "Iteration : 359  Loss_actor : -23.469574  Loss_critic : 19.536526\n",
      "saved\n",
      "Round 361, reward -4.384\n",
      "Round 361, rl loss 43.839\n",
      "Round 361, fedavg loss 29.839\n",
      "Iteration : 360  Loss_actor : 48.34836  Loss_critic : 7.3525395\n",
      "Round 362, reward -3.413\n",
      "Round 362, rl loss 34.127\n",
      "Round 362, fedavg loss 26.481\n",
      "Iteration : 361  Loss_actor : -87.593155  Loss_critic : 7.264824\n",
      "Round 363, reward -2.921\n",
      "Round 363, rl loss 29.213\n",
      "Round 363, fedavg loss 22.620\n",
      "Iteration : 362  Loss_actor : -257.83478  Loss_critic : 6.2448378\n",
      "Round 364, reward -2.590\n",
      "Round 364, rl loss 25.901\n",
      "Round 364, fedavg loss 20.319\n",
      "Iteration : 363  Loss_actor : -270.04236  Loss_critic : 37.981773\n",
      "Round 365, reward -2.748\n",
      "Round 365, rl loss 27.481\n",
      "Round 365, fedavg loss 25.230\n",
      "Iteration : 364  Loss_actor : -12.949362  Loss_critic : 29.670229\n",
      "Round 366, reward -2.210\n",
      "Round 366, rl loss 22.104\n",
      "Round 366, fedavg loss 15.001\n",
      "Iteration : 365  Loss_actor : 49.851604  Loss_critic : 6.186377\n",
      "Round 367, reward -1.782\n",
      "Round 367, rl loss 17.821\n",
      "Round 367, fedavg loss 13.604\n",
      "Iteration : 366  Loss_actor : -70.86725  Loss_critic : 4.542636\n",
      "Round 368, reward -2.562\n",
      "Round 368, rl loss 25.615\n",
      "Round 368, fedavg loss 20.788\n",
      "Iteration : 367  Loss_actor : -94.07814  Loss_critic : 10.304447\n",
      "Round 369, reward -4.336\n",
      "Round 369, rl loss 43.361\n",
      "Round 369, fedavg loss 27.110\n",
      "Iteration : 368  Loss_actor : 156.96109  Loss_critic : 6.712665\n",
      "Round 370, reward -4.423\n",
      "Round 370, rl loss 44.229\n",
      "Round 370, fedavg loss 26.984\n",
      "Iteration : 369  Loss_actor : -21.155802  Loss_critic : 18.332304\n",
      "saved\n",
      "Round 371, reward -2.500\n",
      "Round 371, rl loss 25.000\n",
      "Round 371, fedavg loss 21.323\n",
      "Iteration : 370  Loss_actor : -16.638458  Loss_critic : 8.330859\n",
      "Round 372, reward -2.985\n",
      "Round 372, rl loss 29.854\n",
      "Round 372, fedavg loss 19.063\n",
      "Iteration : 371  Loss_actor : 45.829998  Loss_critic : 4.690904\n",
      "Round 373, reward -2.363\n",
      "Round 373, rl loss 23.633\n",
      "Round 373, fedavg loss 16.288\n",
      "Iteration : 372  Loss_actor : -347.9494  Loss_critic : 13.71003\n",
      "Round 374, reward -3.643\n",
      "Round 374, rl loss 36.432\n",
      "Round 374, fedavg loss 25.812\n",
      "Iteration : 373  Loss_actor : -167.5333  Loss_critic : 36.359978\n",
      "Round 375, reward -2.213\n",
      "Round 375, rl loss 22.127\n",
      "Round 375, fedavg loss 13.329\n",
      "Iteration : 374  Loss_actor : 24.688183  Loss_critic : 12.872649\n",
      "Round 376, reward -3.807\n",
      "Round 376, rl loss 38.075\n",
      "Round 376, fedavg loss 29.815\n",
      "Iteration : 375  Loss_actor : -5.582781  Loss_critic : 10.392881\n",
      "Round 377, reward -4.699\n",
      "Round 377, rl loss 46.987\n",
      "Round 377, fedavg loss 28.475\n",
      "Iteration : 376  Loss_actor : -306.54645  Loss_critic : 9.440061\n",
      "Round 378, reward -4.232\n",
      "Round 378, rl loss 42.324\n",
      "Round 378, fedavg loss 31.437\n",
      "Iteration : 377  Loss_actor : -219.15121  Loss_critic : 19.02666\n",
      "Round 379, reward -6.792\n",
      "Round 379, rl loss 67.918\n",
      "Round 379, fedavg loss 32.437\n",
      "Iteration : 378  Loss_actor : -118.096954  Loss_critic : 30.26102\n",
      "Round 380, reward -3.030\n",
      "Round 380, rl loss 30.300\n",
      "Round 380, fedavg loss 27.588\n",
      "Iteration : 379  Loss_actor : 27.118578  Loss_critic : 13.869583\n",
      "saved\n",
      "Round 381, reward -5.341\n",
      "Round 381, rl loss 53.408\n",
      "Round 381, fedavg loss 25.494\n",
      "Iteration : 380  Loss_actor : -39.171803  Loss_critic : 4.5669937\n",
      "Round 382, reward -2.880\n",
      "Round 382, rl loss 28.796\n",
      "Round 382, fedavg loss 23.246\n",
      "Iteration : 381  Loss_actor : -246.78069  Loss_critic : 8.298723\n",
      "Round 383, reward -7.010\n",
      "Round 383, rl loss 70.102\n",
      "Round 383, fedavg loss 41.260\n",
      "Iteration : 382  Loss_actor : -137.44193  Loss_critic : 24.373787\n",
      "Round 384, reward -5.232\n",
      "Round 384, rl loss 52.324\n",
      "Round 384, fedavg loss 33.974\n",
      "Iteration : 383  Loss_actor : 385.34692  Loss_critic : 23.4042\n",
      "Round 385, reward -4.274\n",
      "Round 385, rl loss 42.744\n",
      "Round 385, fedavg loss 29.017\n",
      "Iteration : 384  Loss_actor : 283.08984  Loss_critic : 49.553574\n",
      "Round 386, reward -2.188\n",
      "Round 386, rl loss 21.885\n",
      "Round 386, fedavg loss 20.872\n",
      "Iteration : 385  Loss_actor : -132.53477  Loss_critic : 29.61951\n",
      "Round 387, reward -1.587\n",
      "Round 387, rl loss 15.867\n",
      "Round 387, fedavg loss 24.537\n",
      "Iteration : 386  Loss_actor : -164.19998  Loss_critic : 11.367508\n",
      "Round 388, reward -2.019\n",
      "Round 388, rl loss 20.193\n",
      "Round 388, fedavg loss 13.085\n",
      "Iteration : 387  Loss_actor : 18.215652  Loss_critic : 21.846354\n",
      "Round 389, reward -3.217\n",
      "Round 389, rl loss 32.172\n",
      "Round 389, fedavg loss 22.928\n",
      "Iteration : 388  Loss_actor : 141.61415  Loss_critic : 3.72738\n",
      "Round 390, reward -3.489\n",
      "Round 390, rl loss 34.892\n",
      "Round 390, fedavg loss 19.296\n",
      "Iteration : 389  Loss_actor : 6.6094522  Loss_critic : 28.727047\n",
      "saved\n",
      "Round 391, reward -4.289\n",
      "Round 391, rl loss 42.889\n",
      "Round 391, fedavg loss 20.370\n",
      "Iteration : 390  Loss_actor : -252.6106  Loss_critic : 8.7581415\n",
      "Round 392, reward -4.939\n",
      "Round 392, rl loss 49.393\n",
      "Round 392, fedavg loss 29.219\n",
      "Iteration : 391  Loss_actor : -237.68448  Loss_critic : 31.691273\n",
      "Round 393, reward -3.265\n",
      "Round 393, rl loss 32.646\n",
      "Round 393, fedavg loss 19.522\n",
      "Iteration : 392  Loss_actor : 2.0455356  Loss_critic : 27.159863\n",
      "Round 394, reward -3.279\n",
      "Round 394, rl loss 32.790\n",
      "Round 394, fedavg loss 31.993\n",
      "Iteration : 393  Loss_actor : 201.02072  Loss_critic : 7.4733725\n",
      "Round 395, reward -4.880\n",
      "Round 395, rl loss 48.803\n",
      "Round 395, fedavg loss 24.686\n",
      "Iteration : 394  Loss_actor : -25.808884  Loss_critic : 26.330124\n",
      "Round 396, reward -5.176\n",
      "Round 396, rl loss 51.761\n",
      "Round 396, fedavg loss 26.264\n",
      "Iteration : 395  Loss_actor : -457.40622  Loss_critic : 6.7139397\n",
      "Round 397, reward -4.868\n",
      "Round 397, rl loss 48.684\n",
      "Round 397, fedavg loss 31.721\n",
      "Iteration : 396  Loss_actor : -437.5059  Loss_critic : 46.437244\n",
      "Round 398, reward -1.527\n",
      "Round 398, rl loss 15.268\n",
      "Round 398, fedavg loss 19.099\n",
      "Iteration : 397  Loss_actor : -76.690384  Loss_critic : 81.79844\n",
      "Round 399, reward -3.051\n",
      "Round 399, rl loss 30.508\n",
      "Round 399, fedavg loss 19.964\n",
      "Iteration : 398  Loss_actor : 203.19882  Loss_critic : 10.686955\n",
      "Round 400, reward -0.126\n",
      "Round 400, rl loss 1.261\n",
      "Round 400, fedavg loss 1.275\n",
      "Iteration : 399  Loss_actor : -833.5462  Loss_critic : 48.631706\n",
      "saved\n",
      "Round 401, reward -0.381\n",
      "Round 401, rl loss 3.812\n",
      "Round 401, fedavg loss 1.157\n",
      "Iteration : 400  Loss_actor : -712.5875  Loss_critic : 58.766575\n",
      "Round 402, reward -0.437\n",
      "Round 402, rl loss 4.368\n",
      "Round 402, fedavg loss 1.276\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration : 401  Loss_actor : -197.8333  Loss_critic : 63.06843\n",
      "Round 403, reward -1.044\n",
      "Round 403, rl loss 10.439\n",
      "Round 403, fedavg loss 3.105\n",
      "Iteration : 402  Loss_actor : 611.3882  Loss_critic : 33.42472\n",
      "Round 404, reward -0.561\n",
      "Round 404, rl loss 5.610\n",
      "Round 404, fedavg loss 3.880\n",
      "Iteration : 403  Loss_actor : 652.99585  Loss_critic : 94.03175\n",
      "Round 405, reward -1.151\n",
      "Round 405, rl loss 11.509\n",
      "Round 405, fedavg loss 3.454\n",
      "Iteration : 404  Loss_actor : -62.83849  Loss_critic : 95.788795\n",
      "Round 406, reward -2.042\n",
      "Round 406, rl loss 20.421\n",
      "Round 406, fedavg loss 7.616\n",
      "Iteration : 405  Loss_actor : -267.78705  Loss_critic : 6.146339\n",
      "Round 407, reward -1.799\n",
      "Round 407, rl loss 17.990\n",
      "Round 407, fedavg loss 8.127\n",
      "Iteration : 406  Loss_actor : 68.87697  Loss_critic : 40.499405\n",
      "Round 408, reward -1.968\n",
      "Round 408, rl loss 19.681\n",
      "Round 408, fedavg loss 9.775\n",
      "Iteration : 407  Loss_actor : 236.07399  Loss_critic : 19.697556\n",
      "Round 409, reward -2.397\n",
      "Round 409, rl loss 23.972\n",
      "Round 409, fedavg loss 11.733\n",
      "Iteration : 408  Loss_actor : -341.57434  Loss_critic : 28.995956\n",
      "Round 410, reward -3.720\n",
      "Round 410, rl loss 37.201\n",
      "Round 410, fedavg loss 21.358\n",
      "Iteration : 409  Loss_actor : 336.62604  Loss_critic : 51.12898\n",
      "saved\n",
      "Round 411, reward -3.809\n",
      "Round 411, rl loss 38.092\n",
      "Round 411, fedavg loss 29.646\n",
      "Iteration : 410  Loss_actor : -28.111097  Loss_critic : 24.12848\n",
      "Round 412, reward -2.351\n",
      "Round 412, rl loss 23.510\n",
      "Round 412, fedavg loss 15.428\n",
      "Iteration : 411  Loss_actor : -252.1359  Loss_critic : 7.3575115\n",
      "Round 413, reward -4.424\n",
      "Round 413, rl loss 44.238\n",
      "Round 413, fedavg loss 22.967\n",
      "Iteration : 412  Loss_actor : 223.59074  Loss_critic : 27.093115\n",
      "Round 414, reward -4.004\n",
      "Round 414, rl loss 40.038\n",
      "Round 414, fedavg loss 34.822\n",
      "Iteration : 413  Loss_actor : 93.54272  Loss_critic : 24.18102\n",
      "Round 415, reward -4.195\n",
      "Round 415, rl loss 41.951\n",
      "Round 415, fedavg loss 28.428\n",
      "Iteration : 414  Loss_actor : -471.58548  Loss_critic : 24.162176\n",
      "Round 416, reward -2.281\n",
      "Round 416, rl loss 22.808\n",
      "Round 416, fedavg loss 21.257\n",
      "Iteration : 415  Loss_actor : -266.5823  Loss_critic : 50.610462\n",
      "Round 417, reward -3.415\n",
      "Round 417, rl loss 34.152\n",
      "Round 417, fedavg loss 21.307\n",
      "Iteration : 416  Loss_actor : 114.20707  Loss_critic : 24.349375\n",
      "Round 418, reward -4.792\n",
      "Round 418, rl loss 47.923\n",
      "Round 418, fedavg loss 25.905\n",
      "Iteration : 417  Loss_actor : 221.55576  Loss_critic : 21.099104\n",
      "Round 419, reward -5.028\n",
      "Round 419, rl loss 50.277\n",
      "Round 419, fedavg loss 30.750\n",
      "Iteration : 418  Loss_actor : 43.68472  Loss_critic : 20.011023\n",
      "Round 420, reward -2.937\n",
      "Round 420, rl loss 29.373\n",
      "Round 420, fedavg loss 26.458\n",
      "Iteration : 419  Loss_actor : -253.34149  Loss_critic : 14.280993\n",
      "saved\n",
      "Round 421, reward -3.767\n",
      "Round 421, rl loss 37.672\n",
      "Round 421, fedavg loss 27.776\n",
      "Iteration : 420  Loss_actor : -199.79205  Loss_critic : 34.75068\n",
      "Round 422, reward -2.641\n",
      "Round 422, rl loss 26.411\n",
      "Round 422, fedavg loss 20.851\n",
      "Iteration : 421  Loss_actor : 417.51398  Loss_critic : 16.982046\n",
      "Round 423, reward -4.082\n",
      "Round 423, rl loss 40.823\n",
      "Round 423, fedavg loss 22.390\n",
      "Iteration : 422  Loss_actor : 446.19357  Loss_critic : 51.93788\n",
      "Round 424, reward -5.700\n",
      "Round 424, rl loss 57.002\n",
      "Round 424, fedavg loss 26.980\n",
      "Iteration : 423  Loss_actor : 183.5637  Loss_critic : 60.502594\n",
      "Round 425, reward -3.089\n",
      "Round 425, rl loss 30.890\n",
      "Round 425, fedavg loss 23.427\n",
      "Iteration : 424  Loss_actor : -128.25586  Loss_critic : 18.472517\n",
      "Round 426, reward -3.274\n",
      "Round 426, rl loss 32.738\n",
      "Round 426, fedavg loss 24.774\n",
      "Iteration : 425  Loss_actor : -55.615936  Loss_critic : 23.381186\n",
      "Round 427, reward -3.675\n",
      "Round 427, rl loss 36.747\n",
      "Round 427, fedavg loss 24.584\n",
      "Iteration : 426  Loss_actor : 274.33533  Loss_critic : 8.638401\n",
      "Round 428, reward -5.156\n",
      "Round 428, rl loss 51.557\n",
      "Round 428, fedavg loss 30.516\n",
      "Iteration : 427  Loss_actor : 178.71675  Loss_critic : 30.17837\n",
      "Round 429, reward -4.909\n",
      "Round 429, rl loss 49.090\n",
      "Round 429, fedavg loss 30.215\n",
      "Iteration : 428  Loss_actor : -245.76872  Loss_critic : 24.95101\n",
      "Round 430, reward -6.457\n",
      "Round 430, rl loss 64.575\n",
      "Round 430, fedavg loss 34.438\n",
      "Iteration : 429  Loss_actor : -158.47505  Loss_critic : 25.986593\n",
      "saved\n",
      "Round 431, reward -4.040\n",
      "Round 431, rl loss 40.399\n",
      "Round 431, fedavg loss 25.614\n",
      "Iteration : 430  Loss_actor : 189.64201  Loss_critic : 14.149843\n",
      "Round 432, reward -3.997\n",
      "Round 432, rl loss 39.969\n",
      "Round 432, fedavg loss 29.696\n",
      "Iteration : 431  Loss_actor : 195.2211  Loss_critic : 37.94227\n",
      "Round 433, reward -3.269\n",
      "Round 433, rl loss 32.689\n",
      "Round 433, fedavg loss 22.985\n",
      "Iteration : 432  Loss_actor : -137.09117  Loss_critic : 21.775156\n",
      "Round 434, reward -1.568\n",
      "Round 434, rl loss 15.681\n",
      "Round 434, fedavg loss 11.970\n",
      "Iteration : 433  Loss_actor : -87.530304  Loss_critic : 5.7370596\n",
      "Round 435, reward -2.970\n",
      "Round 435, rl loss 29.698\n",
      "Round 435, fedavg loss 20.088\n",
      "Iteration : 434  Loss_actor : 324.52078  Loss_critic : 16.794615\n",
      "Round 436, reward -4.250\n",
      "Round 436, rl loss 42.497\n",
      "Round 436, fedavg loss 26.677\n",
      "Iteration : 435  Loss_actor : 281.36166  Loss_critic : 30.352982\n",
      "Round 437, reward -4.236\n",
      "Round 437, rl loss 42.361\n",
      "Round 437, fedavg loss 24.228\n",
      "Iteration : 436  Loss_actor : 7.196846  Loss_critic : 44.138664\n",
      "Round 438, reward -3.197\n",
      "Round 438, rl loss 31.966\n",
      "Round 438, fedavg loss 23.780\n",
      "Iteration : 437  Loss_actor : -390.41098  Loss_critic : 5.6445985\n",
      "Round 439, reward -3.145\n",
      "Round 439, rl loss 31.454\n",
      "Round 439, fedavg loss 21.735\n",
      "Iteration : 438  Loss_actor : -212.09387  Loss_critic : 47.24118\n",
      "Round 440, reward -2.826\n",
      "Round 440, rl loss 28.255\n",
      "Round 440, fedavg loss 17.056\n",
      "Iteration : 439  Loss_actor : 296.9238  Loss_critic : 15.609777\n",
      "saved\n",
      "Round 441, reward -3.708\n",
      "Round 441, rl loss 37.080\n",
      "Round 441, fedavg loss 23.233\n",
      "Iteration : 440  Loss_actor : 337.00653  Loss_critic : 31.662794\n",
      "Round 442, reward -3.748\n",
      "Round 442, rl loss 37.481\n",
      "Round 442, fedavg loss 25.499\n",
      "Iteration : 441  Loss_actor : 85.53051  Loss_critic : 56.358814\n",
      "Round 443, reward -2.756\n",
      "Round 443, rl loss 27.559\n",
      "Round 443, fedavg loss 17.880\n",
      "Iteration : 442  Loss_actor : -260.58496  Loss_critic : 23.781979\n",
      "Round 444, reward -4.337\n",
      "Round 444, rl loss 43.366\n",
      "Round 444, fedavg loss 28.754\n",
      "Iteration : 443  Loss_actor : -181.51614  Loss_critic : 26.82059\n",
      "Round 445, reward -1.599\n",
      "Round 445, rl loss 15.985\n",
      "Round 445, fedavg loss 15.354\n",
      "Iteration : 444  Loss_actor : 411.51523  Loss_critic : 17.857464\n",
      "Round 446, reward -1.918\n",
      "Round 446, rl loss 19.175\n",
      "Round 446, fedavg loss 13.949\n",
      "Iteration : 445  Loss_actor : 399.30463  Loss_critic : 52.774338\n",
      "Round 447, reward -2.495\n",
      "Round 447, rl loss 24.949\n",
      "Round 447, fedavg loss 13.976\n",
      "Iteration : 446  Loss_actor : -13.91986  Loss_critic : 43.686813\n",
      "Round 448, reward -3.161\n",
      "Round 448, rl loss 31.613\n",
      "Round 448, fedavg loss 18.354\n",
      "Iteration : 447  Loss_actor : -209.98544  Loss_critic : 6.242141\n",
      "Round 449, reward -4.907\n",
      "Round 449, rl loss 49.068\n",
      "Round 449, fedavg loss 29.083\n",
      "Iteration : 448  Loss_actor : -19.546864  Loss_critic : 11.422458\n",
      "Round 450, reward -3.509\n",
      "Round 450, rl loss 35.089\n",
      "Round 450, fedavg loss 22.393\n",
      "Iteration : 449  Loss_actor : 48.369305  Loss_critic : 14.482179\n",
      "saved\n",
      "Round 451, reward -3.037\n",
      "Round 451, rl loss 30.370\n",
      "Round 451, fedavg loss 18.360\n",
      "Iteration : 450  Loss_actor : 86.40646  Loss_critic : 9.727867\n",
      "Round 452, reward -2.451\n",
      "Round 452, rl loss 24.512\n",
      "Round 452, fedavg loss 20.440\n",
      "Iteration : 451  Loss_actor : 89.18112  Loss_critic : 28.41206\n",
      "Round 453, reward -3.083\n",
      "Round 453, rl loss 30.833\n",
      "Round 453, fedavg loss 18.523\n",
      "Iteration : 452  Loss_actor : -97.294655  Loss_critic : 7.6867924\n",
      "Round 454, reward -2.830\n",
      "Round 454, rl loss 28.299\n",
      "Round 454, fedavg loss 28.939\n",
      "Iteration : 453  Loss_actor : -94.152405  Loss_critic : 15.773649\n",
      "Round 455, reward -2.309\n",
      "Round 455, rl loss 23.086\n",
      "Round 455, fedavg loss 18.635\n",
      "Iteration : 454  Loss_actor : -121.22008  Loss_critic : 8.399403\n",
      "Round 456, reward -3.047\n",
      "Round 456, rl loss 30.472\n",
      "Round 456, fedavg loss 19.918\n",
      "Iteration : 455  Loss_actor : -39.80748  Loss_critic : 17.969381\n",
      "Round 457, reward -5.834\n",
      "Round 457, rl loss 58.340\n",
      "Round 457, fedavg loss 34.686\n",
      "Iteration : 456  Loss_actor : 153.7247  Loss_critic : 9.736524\n",
      "Round 458, reward -2.314\n",
      "Round 458, rl loss 23.140\n",
      "Round 458, fedavg loss 24.651\n",
      "Iteration : 457  Loss_actor : 213.9696  Loss_critic : 27.702461\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round 459, reward -1.992\n",
      "Round 459, rl loss 19.917\n",
      "Round 459, fedavg loss 22.010\n",
      "Iteration : 458  Loss_actor : 1.3889399  Loss_critic : 24.600258\n",
      "Round 460, reward -3.577\n",
      "Round 460, rl loss 35.768\n",
      "Round 460, fedavg loss 19.968\n",
      "Iteration : 459  Loss_actor : -40.10305  Loss_critic : 13.069638\n",
      "saved\n",
      "Round 461, reward -5.015\n",
      "Round 461, rl loss 50.155\n",
      "Round 461, fedavg loss 30.662\n",
      "Iteration : 460  Loss_actor : -2.2089803  Loss_critic : 9.350174\n",
      "Round 462, reward -5.526\n",
      "Round 462, rl loss 55.261\n",
      "Round 462, fedavg loss 30.117\n",
      "Iteration : 461  Loss_actor : 95.15771  Loss_critic : 6.582243\n",
      "Round 463, reward -2.411\n",
      "Round 463, rl loss 24.107\n",
      "Round 463, fedavg loss 34.396\n",
      "Iteration : 462  Loss_actor : -70.67366  Loss_critic : 18.02227\n",
      "Round 464, reward -2.315\n",
      "Round 464, rl loss 23.145\n",
      "Round 464, fedavg loss 12.437\n",
      "Iteration : 463  Loss_actor : 181.74455  Loss_critic : 6.1465845\n",
      "Round 465, reward -1.415\n",
      "Round 465, rl loss 14.145\n",
      "Round 465, fedavg loss 12.216\n",
      "Iteration : 464  Loss_actor : -223.5416  Loss_critic : 21.674967\n",
      "Round 466, reward -3.449\n",
      "Round 466, rl loss 34.491\n",
      "Round 466, fedavg loss 16.676\n",
      "Iteration : 465  Loss_actor : -0.9187536  Loss_critic : 16.387848\n",
      "Round 467, reward -2.771\n",
      "Round 467, rl loss 27.709\n",
      "Round 467, fedavg loss 18.960\n",
      "Iteration : 466  Loss_actor : 322.64478  Loss_critic : 10.029856\n",
      "Round 468, reward -3.862\n",
      "Round 468, rl loss 38.617\n",
      "Round 468, fedavg loss 19.036\n",
      "Iteration : 467  Loss_actor : -59.79082  Loss_critic : 41.065536\n",
      "Round 469, reward -4.315\n",
      "Round 469, rl loss 43.150\n",
      "Round 469, fedavg loss 25.606\n",
      "Iteration : 468  Loss_actor : -154.72704  Loss_critic : 6.2916493\n",
      "Round 470, reward -4.409\n",
      "Round 470, rl loss 44.091\n",
      "Round 470, fedavg loss 29.990\n",
      "Iteration : 469  Loss_actor : 234.24384  Loss_critic : 24.868816\n",
      "saved\n",
      "Round 471, reward -3.074\n",
      "Round 471, rl loss 30.745\n",
      "Round 471, fedavg loss 25.424\n",
      "Iteration : 470  Loss_actor : 141.26987  Loss_critic : 38.546112\n",
      "Round 472, reward -3.501\n",
      "Round 472, rl loss 35.010\n",
      "Round 472, fedavg loss 26.826\n",
      "Iteration : 471  Loss_actor : -155.32298  Loss_critic : 19.472862\n",
      "Round 473, reward -3.477\n",
      "Round 473, rl loss 34.765\n",
      "Round 473, fedavg loss 20.970\n",
      "Iteration : 472  Loss_actor : -19.69358  Loss_critic : 9.948555\n",
      "Round 474, reward -3.984\n",
      "Round 474, rl loss 39.840\n",
      "Round 474, fedavg loss 20.102\n",
      "Iteration : 473  Loss_actor : 119.84469  Loss_critic : 18.58917\n",
      "Round 475, reward -4.302\n",
      "Round 475, rl loss 43.015\n",
      "Round 475, fedavg loss 28.860\n",
      "Iteration : 474  Loss_actor : 176.55931  Loss_critic : 11.398194\n",
      "Round 476, reward -3.331\n",
      "Round 476, rl loss 33.313\n",
      "Round 476, fedavg loss 37.761\n",
      "Iteration : 475  Loss_actor : 103.62043  Loss_critic : 16.857641\n",
      "Round 477, reward -3.568\n",
      "Round 477, rl loss 35.681\n",
      "Round 477, fedavg loss 20.565\n",
      "Iteration : 476  Loss_actor : -25.176464  Loss_critic : 8.482155\n",
      "Round 478, reward -2.685\n",
      "Round 478, rl loss 26.845\n",
      "Round 478, fedavg loss 26.466\n",
      "Iteration : 477  Loss_actor : -170.35406  Loss_critic : 11.211022\n",
      "Round 479, reward -4.761\n",
      "Round 479, rl loss 47.612\n",
      "Round 479, fedavg loss 28.471\n",
      "Iteration : 478  Loss_actor : -71.77152  Loss_critic : 13.609947\n",
      "Round 480, reward -3.082\n",
      "Round 480, rl loss 30.822\n",
      "Round 480, fedavg loss 27.221\n",
      "Iteration : 479  Loss_actor : -13.8907175  Loss_critic : 12.437527\n",
      "saved\n",
      "Round 481, reward -3.959\n",
      "Round 481, rl loss 39.588\n",
      "Round 481, fedavg loss 18.990\n",
      "Iteration : 480  Loss_actor : -8.075759  Loss_critic : 8.057869\n",
      "Round 482, reward -2.771\n",
      "Round 482, rl loss 27.715\n",
      "Round 482, fedavg loss 24.297\n",
      "Iteration : 481  Loss_actor : 1.4279411  Loss_critic : 7.536422\n",
      "Round 483, reward -2.814\n",
      "Round 483, rl loss 28.143\n",
      "Round 483, fedavg loss 21.116\n",
      "Iteration : 482  Loss_actor : 44.028214  Loss_critic : 11.025042\n",
      "Round 484, reward -4.208\n",
      "Round 484, rl loss 42.081\n",
      "Round 484, fedavg loss 24.846\n",
      "Iteration : 483  Loss_actor : 68.66348  Loss_critic : 8.405014\n",
      "Round 485, reward -4.226\n",
      "Round 485, rl loss 42.264\n",
      "Round 485, fedavg loss 34.424\n",
      "Iteration : 484  Loss_actor : 29.582962  Loss_critic : 8.603171\n",
      "Round 486, reward -2.437\n",
      "Round 486, rl loss 24.372\n",
      "Round 486, fedavg loss 23.606\n",
      "Iteration : 485  Loss_actor : -70.43527  Loss_critic : 12.890573\n",
      "Round 487, reward -4.233\n",
      "Round 487, rl loss 42.327\n",
      "Round 487, fedavg loss 24.823\n",
      "Iteration : 486  Loss_actor : -49.252396  Loss_critic : 11.942355\n",
      "Round 488, reward -4.905\n",
      "Round 488, rl loss 49.050\n",
      "Round 488, fedavg loss 27.117\n",
      "Iteration : 487  Loss_actor : -20.233273  Loss_critic : 10.911288\n",
      "Round 489, reward -3.612\n",
      "Round 489, rl loss 36.118\n",
      "Round 489, fedavg loss 15.727\n",
      "Iteration : 488  Loss_actor : -6.4932766  Loss_critic : 5.1215367\n",
      "Round 490, reward -5.884\n",
      "Round 490, rl loss 58.844\n",
      "Round 490, fedavg loss 39.868\n",
      "Iteration : 489  Loss_actor : -6.887606  Loss_critic : 9.0370865\n",
      "saved\n",
      "Round 491, reward -3.798\n",
      "Round 491, rl loss 37.975\n",
      "Round 491, fedavg loss 32.193\n",
      "Iteration : 490  Loss_actor : -23.600075  Loss_critic : 6.3650675\n",
      "Round 492, reward -3.495\n",
      "Round 492, rl loss 34.953\n",
      "Round 492, fedavg loss 30.335\n",
      "Iteration : 491  Loss_actor : -38.002693  Loss_critic : 6.236743\n",
      "Round 493, reward -4.291\n",
      "Round 493, rl loss 42.909\n",
      "Round 493, fedavg loss 26.222\n",
      "Iteration : 492  Loss_actor : -33.746613  Loss_critic : 6.411723\n",
      "Round 494, reward -2.883\n",
      "Round 494, rl loss 28.827\n",
      "Round 494, fedavg loss 20.391\n",
      "Iteration : 493  Loss_actor : -47.878708  Loss_critic : 10.484725\n",
      "Round 495, reward -4.006\n",
      "Round 495, rl loss 40.061\n",
      "Round 495, fedavg loss 21.982\n",
      "Iteration : 494  Loss_actor : 1.3167255  Loss_critic : 7.6456213\n",
      "Round 496, reward -5.335\n",
      "Round 496, rl loss 53.355\n",
      "Round 496, fedavg loss 30.590\n",
      "Iteration : 495  Loss_actor : 153.17699  Loss_critic : 9.988564\n",
      "Round 497, reward -3.458\n",
      "Round 497, rl loss 34.584\n",
      "Round 497, fedavg loss 36.106\n",
      "Iteration : 496  Loss_actor : 39.5205  Loss_critic : 13.299239\n",
      "Round 498, reward -2.184\n",
      "Round 498, rl loss 21.842\n",
      "Round 498, fedavg loss 23.125\n",
      "Iteration : 497  Loss_actor : -44.851753  Loss_critic : 9.127881\n",
      "Round 499, reward -1.990\n",
      "Round 499, rl loss 19.902\n",
      "Round 499, fedavg loss 17.630\n",
      "Iteration : 498  Loss_actor : -70.56155  Loss_critic : 7.215477\n"
     ]
    }
   ],
   "source": [
    "# training\n",
    "loss_train = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "m = max(int(args.frac * args.num_users * args.train_frac), 1)\n",
    "loss_save = []\n",
    "# trainer = torch.load('rl_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "# trainer = torch.load('rl_mnist.pt')\n",
    "\n",
    "last_replay_data = []\n",
    "for iter in range(args.epochs):\n",
    "    if iter==0:\n",
    "        random_n = 0\n",
    "        n_weight = []\n",
    "        while random_n<args.num_users*args.frac*args.train_frac:\n",
    "            n_weight.append(random.random()) # 随机初始化参数\n",
    "            random_n+=1\n",
    "        action = F.softmax(torch.tensor(n_weight))\n",
    "        net_fedavg = copy.deepcopy(net_glob).to(args.device)\n",
    "    \n",
    "    if iter % args.reset_flag == 0:\n",
    "        if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "            net_glob = CNNCifar(args=args).to(args.device)\n",
    "        elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "            net_glob = CNNMnist(args=args).to(args.device)\n",
    "        elif args.model == 'mlp':\n",
    "            len_in = 1\n",
    "            for x in img_size:\n",
    "                len_in *= x\n",
    "            net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "        else:\n",
    "            exit('Error: unrecognized model')\n",
    "        net_fedavg = copy.deepcopy(net_glob).to(args.device)\n",
    "    \n",
    "    loss_locals = []\n",
    "    w_locals = []\n",
    "#     m = max(int(args.frac * args.num_users), 1)\n",
    "    m = max(int(args.frac * args.num_users * args.train_frac), 1)\n",
    "#     m = max(int(args.frac * args.num_users * args.train_frac * args.k * iter/args.epochs), 1)\n",
    "#     m = min(max(int(args.frac * args.num_users * args.train_frac * iter), 1), 10)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    p_emb_collect = []\n",
    "    w_fedavg_list = []\n",
    "    loss_fedavg_list = []\n",
    "    loss_total_rl_list = []\n",
    "    loss_total_fedavg_list = []\n",
    "    for idx in idxs_users:\n",
    "#         local = LocalUpdate_divide(args=args, dataset=dataset_train, idxs=dict_users[0][idx])\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "#         w, loss, loss_epoch_rl_list = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "#         w_fedavg, loss_fedavg, loss_epoch_fedavg_list = local.train(net=copy.deepcopy(net_fedavg).to(args.device))\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w_fedavg, loss_fedavg = local.train(net=copy.deepcopy(net_fedavg).to(args.device))\n",
    "#         loss_total_rl_list += loss_epoch_rl_list\n",
    "#         loss_total_fedavg_list += loss_epoch_fedavg_list\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "        avg_emb_feature = emb_feature/4\n",
    "        ############ 储存参数 ##############\n",
    "        p_emb_collect.append(avg_emb_feature)\n",
    "        \n",
    "        ############ 储存loss ##############\n",
    "#         w_fedavg_list.append(copy.deepcopy(w_fedavg))\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_fedavg_list.append(copy.deepcopy(loss_fedavg))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    p_emb_collect = torch.cat(p_emb_collect).unsqueeze(0).to(args.device)\n",
    "    action_next = trainer.target_actor.forward(p_emb_collect,\n",
    "                                          torch.tensor(loss_locals).reshape(1,-1).to(args.device), \n",
    "                                          action.reshape(1,-1).to(args.device))[0]\n",
    "#     print(action)\n",
    "#     print(action_next)\n",
    "\n",
    "\n",
    "    ###########计算当前轮的reward，然后将当前轮的reward添加到上一个replay_data中\n",
    "    reward1 = -sum(loss_locals) / len(loss_locals)\n",
    "    reward2 = -sum(loss_locals) / len(loss_locals) + sum(loss_fedavg_list) / len(loss_fedavg_list)\n",
    "    reward3 = -sum(loss_locals) + sum(loss_fedavg_list)\n",
    "    reward4 = 0\n",
    "    for i in range(len(loss_total_rl_list)):\n",
    "        if loss_total_rl_list[i] <= loss_total_fedavg_list[i]:\n",
    "            reward4 += 1\n",
    "        else:\n",
    "            reward4 -= 1\n",
    "\n",
    "#     reward = 0.5 * reward3 + 0.05 * reward4 / (args.local_ep*m)\n",
    "    reward = reward1\n",
    "    \n",
    "    loss_save.append(reward)\n",
    "\n",
    "    \n",
    "#     if iter > 0:\n",
    "    if len(last_replay_data)==2:\n",
    "        last_replay_data.append(reward)#r\n",
    "        last_replay_data.append([p_emb_collect, \n",
    "                                 torch.tensor(loss_locals).reshape(1,-1).to(args.device), \n",
    "                                 action.reshape(1,-1).to(args.device)])#s_next\n",
    "        trainer.replay_buffer.add(last_replay_data[0],\n",
    "                                  last_replay_data[1],\n",
    "                                  last_replay_data[2],\n",
    "                                  last_replay_data[3])\n",
    "    last_replay_data = [[p_emb_collect, \n",
    "                         torch.tensor(loss_locals).reshape(1,-1).to(args.device), \n",
    "                         action.reshape(1,-1).to(args.device)], \n",
    "                        action_next.reshape(1,-1).to(args.device)]#s, a\n",
    "    action = action_next\n",
    "    # update global weights\n",
    "#     w_glob = FedAvg(w_locals)\n",
    "    w_glob = FedPareto(w_locals, action_next)\n",
    "    \n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    net_fedavg.load_state_dict(FedAvg(w_locals))\n",
    "    \n",
    "    \n",
    "    # print loss\n",
    "    print('Round {:3d}, reward {:.3f}'.format(iter, reward))\n",
    "    print('Round {:3d}, rl loss {:.3f}'.format(iter, sum(loss_locals)))\n",
    "    print('Round {:3d}, fedavg loss {:.3f}'.format(iter, sum(loss_fedavg_list)))\n",
    "    loss_train.append(-reward)\n",
    "    \n",
    "    if iter > 0:\n",
    "        trainer.optimize()\n",
    "    args.lr = max(args.lr*args.lr_decay, 0.001)\n",
    "    if iter % 10 == 0:\n",
    "        torch.save(trainer, 'rl_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "        print(\"saved\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:26: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round   0, Average loss 0.096\n",
      "Round   1, Average loss 0.541\n",
      "Round   2, Average loss 0.734\n",
      "Round   3, Average loss 0.976\n",
      "Round   4, Average loss 1.325\n",
      "Round   5, Average loss 2.287\n",
      "Round   6, Average loss 3.354\n",
      "Round   7, Average loss 3.918\n",
      "Round   8, Average loss 4.206\n",
      "Round   9, Average loss 3.034\n",
      "Round  10, Average loss 2.733\n",
      "Round  11, Average loss 3.218\n",
      "Round  12, Average loss 2.628\n",
      "Round  13, Average loss 2.345\n",
      "Round  14, Average loss 4.351\n",
      "Round  15, Average loss 4.422\n",
      "Round  16, Average loss 3.645\n",
      "Round  17, Average loss 2.405\n",
      "Round  18, Average loss 3.512\n",
      "Round  19, Average loss 2.625\n",
      "Round  20, Average loss 4.125\n",
      "Round  21, Average loss 4.826\n",
      "Round  22, Average loss 3.100\n",
      "Round  23, Average loss 4.682\n",
      "Round  24, Average loss 3.740\n",
      "Round  25, Average loss 4.818\n",
      "Round  26, Average loss 5.068\n",
      "Round  27, Average loss 4.507\n",
      "Round  28, Average loss 4.896\n",
      "Round  29, Average loss 4.174\n",
      "Round  30, Average loss 3.695\n",
      "Round  31, Average loss 3.816\n",
      "Round  32, Average loss 1.799\n",
      "Round  33, Average loss 1.777\n",
      "Round  34, Average loss 4.059\n",
      "Round  35, Average loss 2.936\n",
      "Round  36, Average loss 4.966\n",
      "Round  37, Average loss 4.513\n",
      "Round  38, Average loss 3.987\n",
      "Round  39, Average loss 2.921\n",
      "Round  40, Average loss 2.274\n",
      "Round  41, Average loss 2.477\n",
      "Round  42, Average loss 3.049\n",
      "Round  43, Average loss 4.098\n",
      "Round  44, Average loss 6.822\n",
      "Round  45, Average loss 5.432\n",
      "Round  46, Average loss 3.703\n",
      "Round  47, Average loss 4.311\n",
      "Round  48, Average loss 6.332\n",
      "Round  49, Average loss 6.202\n"
     ]
    }
   ],
   "source": [
    "trainer = torch.load('rl_{}_{}_{}_clientnumber{}.pt'.format(args.dataset, args.epochs, args.model, m))\n",
    "\n",
    "#initialize\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob = CNNCifar(args=args).to(args.device)\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob = CNNMnist(args=args).to(args.device)\n",
    "elif args.model == 'mlp':\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "net_glob.train()\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "\n",
    "for iter in range(args.validation_epochs):\n",
    "    if iter==0:\n",
    "        random_n = 0\n",
    "        n_weight = []\n",
    "        while random_n<args.num_users*args.frac*args.train_frac:\n",
    "            n_weight.append(random.random()) # 随机初始化参数\n",
    "            random_n+=1\n",
    "        action = F.softmax(torch.tensor(n_weight))\n",
    "    loss_locals = []\n",
    "    w_locals = []\n",
    "#     m = max(int(args.frac * args.num_users), 1)\n",
    "    m = max(int(args.frac * args.num_users * args.train_frac), 1)\n",
    "#     m = max(int(args.frac * args.num_users * args.train_frac * args.k * iter/args.epochs), 1)\n",
    "#     m = min(max(int(args.frac * args.num_users * args.train_frac * iter), 1), 10)\n",
    "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
    "    p_emb_collect = []\n",
    "    for idx in idxs_users:\n",
    "#         local = LocalUpdate_divide(args=args, dataset=dataset_train, idxs=dict_users[0][idx])\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "        avg_emb_feature = emb_feature/5\n",
    "        ############ 储存参数 ##############\n",
    "        p_emb_collect.append(avg_emb_feature)\n",
    "\n",
    "        ############ 储存loss ##############\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        loss_locals.append(copy.deepcopy(loss))\n",
    "    p_emb_collect = torch.cat(p_emb_collect).unsqueeze(0).to(args.device)\n",
    "    action_next = trainer.target_actor.forward(p_emb_collect,\n",
    "                                          torch.tensor(loss_locals).reshape(1,-1).to(args.device), \n",
    "                                          action.reshape(1,-1).to(args.device))[0]\n",
    "#     print(action)\n",
    "#     print(action_next)\n",
    "    ###########计算当前轮的reward，然后将当前轮的reward添加到上一个replay_data中\n",
    "    reward = -sum(loss_locals) / len(loss_locals)\n",
    "    action = action_next\n",
    "    # update global weights\n",
    "#     w_glob = FedAvg(w_locals)\n",
    "    w_glob = FedPareto(w_locals, action_next)\n",
    "\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "\n",
    "    # print loss\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, -reward))\n",
    "#     loss_train.append(-reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "def test_img(net_g, datatest, args):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=args.bs)\n",
    "    l = len(data_loader)\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        if args.gpu != -1:\n",
    "            data, target = data, target\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if args.verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 9.86\n",
      "Testing accuracy: 9.58\n"
     ]
    }
   ],
   "source": [
    "# plot loss curve\n",
    "# plt.figure()\n",
    "# plt.plot(range(len(loss_train)), loss_train)\n",
    "# plt.ylabel('train_loss')\n",
    "# plt.show()\n",
    "# plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
    "\n",
    "# testing\n",
    "net_glob.eval()\n",
    "acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(len(loss_save)), loss_save)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
