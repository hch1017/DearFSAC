{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "def args_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    # federated arguments\n",
    "    #RL的训练轮次\n",
    "    parser.add_argument('--epochs', type=int, default=1000, help=\"rounds of training\")\n",
    "    \n",
    "    #嵌入向量的训练轮次\n",
    "    parser.add_argument('--emb_train_epochs', type=int, default=10, help=\"rounds of training\")\n",
    "    parser.add_argument('--emb', default=True)\n",
    "    \n",
    "    #验证RL和Fedavg哪个更好的验证轮次\n",
    "    parser.add_argument('--validation_epochs', type=int, default=10, help=\"rounds of training\")\n",
    "    parser.add_argument('--divide', default=True)\n",
    "    parser.add_argument('--reset_flag', type=int, default=40, help=\"reset flag\")\n",
    "    \n",
    "    #将训练集分为几份\n",
    "    parser.add_argument('--divide_num', type=int, default=2, help=\"divide number\")\n",
    "    \n",
    "    #有多少个local client\n",
    "    parser.add_argument('--num_users', type=int, default=100, help=\"number of users: K\")\n",
    "    parser.add_argument('--k', type=int, default=10, help=\"k\")\n",
    "    \n",
    "    #每次选多少个local client参与训练\n",
    "    parser.add_argument('--frac', type=float, default=0.1, help=\"the fraction of clients: C\")\n",
    "    parser.add_argument('--train_frac', type=float, default=1, help=\"the fraction of training: C\")\n",
    "    \n",
    "    parser.add_argument('--collect_ep', type=int, default=1000, help=\"rounds of training\")\n",
    "    \n",
    "    #local client自己本地训练的轮次\n",
    "    parser.add_argument('--local_emb_ep', type=int, default=1, help=\"the number of local epochs: E\")\n",
    "    parser.add_argument('--local_ep', type=int, default=1, help=\"the number of local epochs: E\")\n",
    "    parser.add_argument('--local_chosen_ep', type=int, default=2, help=\"the number of local epochs: E\")\n",
    "    \n",
    "    #验证环节的local clinet本地训练轮次\n",
    "    parser.add_argument('--local_validation_ep', type=int, default=1, help=\"the number of local epochs: E\")\n",
    "    \n",
    "    #local client本地训练的batchsize\n",
    "    parser.add_argument('--local_bs', type=int, default=10, help=\"local batch size: B\")\n",
    "    parser.add_argument('--bs', type=int, default=128, help=\"test batch size\")\n",
    "    \n",
    "    #RL的学习率和衰减率\n",
    "    parser.add_argument('--lr', type=float, default=0.01, help=\"learning rate (default: 0.01)\")\n",
    "    parser.add_argument('--lr_decay', type=float, default=1, help=\"lr decay\")\n",
    "    parser.add_argument('--momentum', type=float, default=0.5, help=\"SGD momentum (default: 0.5)\")\n",
    "    parser.add_argument('--split', type=str, default='user', help=\"train-test split type, user or sample\")\n",
    "\n",
    "    # model arguments\n",
    "    \n",
    "    #使用的client 模型\n",
    "    parser.add_argument('--model', type=str, default='cnn', help='model name')\n",
    "    parser.add_argument('--kernel_num', type=int, default=9, help='number of each kind of kernel')\n",
    "    parser.add_argument('--kernel_sizes', type=str, default='3,4,5',\n",
    "                        help='comma-separated kernel size to use for convolution')\n",
    "    parser.add_argument('--norm', type=str, default='batch_norm', help=\"batch_norm, layer_norm, or None\")\n",
    "    parser.add_argument('--num_filters', type=int, default=32, help=\"number of filters for conv nets\")\n",
    "    parser.add_argument('--max_pool', type=str, default='True',\n",
    "                        help=\"Whether use max pooling rather than strided convolutions\")\n",
    "\n",
    "    # other arguments\n",
    "    \n",
    "    #使用的数据集\n",
    "    parser.add_argument('--dataset', type=str, default='mnist', help=\"name of dataset\")\n",
    "    \n",
    "    #数据集的划分是否满足独立同分布\n",
    "    parser.add_argument('--iid', action='store_true', help='whether i.i.d or not')\n",
    "    \n",
    "    #输出的分类个数\n",
    "    parser.add_argument('--num_classes', type=int, default=10, help=\"number of classes\")\n",
    "    \n",
    "    #输入的图片的通道数\n",
    "    parser.add_argument('--num_channels', type=int, default=1, help=\"number of channels of imges\")\n",
    "    parser.add_argument('--gpu', type=int, default=-1, help=\"GPU ID, -1 for CPU\")\n",
    "    parser.add_argument('--stopping_rounds', type=int, default=10, help='rounds of early stopping')\n",
    "    parser.add_argument('--verbose', action='store_true', help='verbose print')\n",
    "    parser.add_argument('--seed', type=int, default=1, help='random seed (default: 1)')\n",
    "    parser.add_argument('--all_clients', action='store_true', help='aggregation over all clients')\n",
    "    args = parser.parse_args(args=[])\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid, mnist_iid_drl_local_divide, cifar_iid_drl_local_divide, mnist_noniid_drl_local_divide\n",
    "from models.Update import LocalUpdate\n",
    "from models.Update_divide import LocalUpdate_divide\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, CNNCifarEmb, CNNCifarEmbReverse, CNNMnistEmb, CNNMnistEmbReverse\n",
    "from models.Fed import FedAvg\n",
    "# from models.test import test_img\n",
    "\n",
    "# parse args\n",
    "args = args_parser()\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.repeat(1,1,1))])\n",
    "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=False, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    args.iid = True\n",
    "    if args.iid:\n",
    "        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    dataset_train = datasets.CIFAR10('./data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('./data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "    args.iid = True\n",
    "    if args.iid:\n",
    "        dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        exit('Error: only consider IID setting in CIFAR10')\n",
    "else:\n",
    "    exit('Error: unrecognized dataset')\n",
    "img_size = dataset_train[0][0].shape\n",
    "\n",
    "# build model\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob = CNNCifar(args=args).to(args.device)\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob = CNNMnist(args=args).to(args.device)\n",
    "elif args.model == 'mlp':\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "print(net_glob)\n",
    "net_glob.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import heapq\n",
    "\n",
    "# Hyper Parameters\n",
    "BATCH_SIZE = 32\n",
    "LR = 0.01                   # learning rate\n",
    "EPSILON = 0.95             # greedy policy\n",
    "GAMMA = 0.9                 # reward discount\n",
    "TARGET_REPLACE_ITER = 100   # target update frequency\n",
    "\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, parameter_dim, action_dim, args):\n",
    "        super(Net, self).__init__()\n",
    "        self.args = args\n",
    "        self.parameter_dim = parameter_dim\n",
    "        self.action_dim = action_dim\n",
    "\n",
    "        # N+1 即global的p拼接上所有local的p\n",
    "        self.fc1 = nn.Linear(parameter_dim, 1000)\n",
    "        self.fc2 = nn.Linear(1000, action_dim)\n",
    "\n",
    "    def forward(self, parameters):\n",
    "        #拼接的p\n",
    "        x = self.fc1(parameters)\n",
    "        q = self.fc2(x)\n",
    "        \n",
    "        #100维\n",
    "        return q\n",
    "\n",
    "class DQN(object):\n",
    "    def __init__(self, parameter_dim, action_dim, replay_buffer, args):\n",
    "        self.parameter_dim = parameter_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.replay_buffer = replay_buffer\n",
    "        self.args = args\n",
    "        self.iter = 0\n",
    "        #self.noise = utils.OrnsteinUhlenbeckActionNoise(self.action_dim)\n",
    "\n",
    "        self.eval_net = Net(self.parameter_dim,  \n",
    "                                 self.action_dim, args).to(args.device)\n",
    "        self.target_net = Net(self.parameter_dim, \n",
    "                                 self.action_dim, args).to(args.device)\n",
    "\n",
    "        self.learn_step_counter = 0                                     # for target updating\n",
    "        self.optimizer = torch.optim.Adam(self.eval_net.parameters(), 0.01)\n",
    "#         self.loss_func = F.smooth_l1_loss\n",
    "         \n",
    "    #选q最大的local，直接赋值给global，只用一个local参与训练的目的是加快训练\n",
    "    def choose_action_train(self, parameters):\n",
    "        q = self.eval_net.forward(parameters)\n",
    "        action = q.detach().numpy()[0][0].tolist().index(max(q.detach().numpy()[0][0]))\n",
    "        #这是索引\n",
    "        print(action)\n",
    "        return action\n",
    "    \n",
    "    #选q最大的10个local，再将它们的q用softmax变成权值\n",
    "    def choose_action_run(self, parameters):\n",
    "        q = self.eval_net.forward(parameters)\n",
    "        #l是q的列表形式\n",
    "        l = q.detach().numpy()[0][0].tolist()\n",
    "        \n",
    "        #最大的k个q值\n",
    "        topk = heapq.nlargest(int(self.args.frac * self.args.num_users), l)\n",
    "        \n",
    "        #找索引\n",
    "        choice = []\n",
    "        for i in topk:\n",
    "            choice.append(l.index(i))\n",
    "        \n",
    "        #算权值\n",
    "        topk_sum = np.sum(topk)\n",
    "        for i in range(len(topk)):\n",
    "            topk[i] /= topk_sum\n",
    "        action = topk\n",
    "        \n",
    "        #softmax\n",
    "#         action = F.softmax(torch.Tensor(topk),0)\n",
    "        print(action)\n",
    "        return choice, action\n",
    "\n",
    "    def optimize(self):\n",
    "        \"\"\"\n",
    "        Samples a random batch from replay memory and performs optimization\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        s1,a1,r1,s2 = self.replay_buffer.sample(5)\n",
    "#         print(s1)\n",
    "#         print(a1)\n",
    "#         print(r1)\n",
    "        if self.learn_step_counter % TARGET_REPLACE_ITER == 0:\n",
    "            self.target_net.load_state_dict(self.eval_net.state_dict())\n",
    "        self.learn_step_counter += 1\n",
    "\n",
    "        # q_eval w.r.t the action in experience\n",
    "        q_eval = self.eval_net.forward(s1) # shape (batch, 1)\n",
    "        q_eval = q_eval.gather(1,a1.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        q_next = self.target_net(s2).detach()     # detach from graph, don't backpropagate\n",
    "        q_next, a_next = q_next.max(1)\n",
    "        q_target = r1 + GAMMA * q_next  # shape (batch, 1)\n",
    "        loss = F.smooth_l1_loss(q_eval.float(), q_target.float())\n",
    "\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward(retain_graph=True)\n",
    "        self.optimizer.step()\n",
    "#         self.soft_update(self.target_net, self.eval_net, 0.001)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MemoryBuffer: # MemoryBuffer类实现的功能：buffer内采样，往buffer里塞（sars）\n",
    "\n",
    "    def __init__(self, size):\n",
    "        self.buffer = deque(maxlen=size) #buffer设置为双端队列\n",
    "        self.maxSize = size\n",
    "        self.len = 0\n",
    "        \n",
    "    def state_reco(self, s):\n",
    "        s_1 = [i[0] for i in s]\n",
    "        s_2 = [i[1] for i in s]\n",
    "        s_3 = [i[2] for i in s]\n",
    "        return [torch.cat(s_1),torch.cat(s_2),torch.cat(s_3)]\n",
    "\n",
    "    def sample(self, count):\n",
    "        \"\"\"\n",
    "        samples a random batch from the replay memory buffer\n",
    "        :param count: batch size\n",
    "        :return: batch (numpy array)\n",
    "        \"\"\"\n",
    "        batch = []\n",
    "        count = min(count, self.len)\n",
    "        batch = random.sample(self.buffer, count) # 随机取样\n",
    "\n",
    "        s_arr = torch.cat([arr[0] for arr in batch],1)\n",
    "        a_arr = torch.tensor([arr[1] for arr in batch])\n",
    "        r_arr = torch.tensor([arr[2] for arr in batch]).reshape(-1,1)\n",
    "        s1_arr = torch.cat([arr[3] for arr in batch],1)\n",
    "\n",
    "        return s_arr, a_arr, r_arr, s1_arr\n",
    "\n",
    "    def len(self):\n",
    "        return self.len\n",
    "\n",
    "    def add(self, s, a, r, s1):\n",
    "        \"\"\"\n",
    "        adds a particular transaction in the memory buffer\n",
    "        :param s: current state\n",
    "        :param a: action taken\n",
    "        :param r: reward received\n",
    "        :param s1: next state\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        transition = (s,a,r,s1)\n",
    "        self.len += 1\n",
    "        if self.len > self.maxSize:\n",
    "            self.len = self.maxSize\n",
    "        self.buffer.append(transition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100\n"
     ]
    }
   ],
   "source": [
    "#每个local的每一层在emb之后拼接起来，再乘以(100+1)，或者用均值。分别对应是400和100\n",
    "parameter_dim = (args.num_users+1) * 100\n",
    "action_dim = args.num_users\n",
    "print(parameter_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss_avg:0.11765153706073761\n",
      "epoch:0, loss_avg:0.15612049400806427\n",
      "epoch:0, loss_avg:0.16926223039627075\n",
      "epoch:0, loss_avg:0.12879692018032074\n",
      "epoch:0, loss_avg:0.13517460227012634\n",
      "epoch:0, loss_avg:0.08772540837526321\n",
      "epoch:0, loss_avg:0.05913291871547699\n",
      "epoch:0, loss_avg:0.03306392952799797\n",
      "epoch:0, loss_avg:0.03374112769961357\n",
      "epoch:0, loss_avg:0.03666183724999428\n",
      "epoch:0, loss_avg:0.030677219852805138\n",
      "epoch:0, loss_avg:0.03224092721939087\n",
      "epoch:0, loss_avg:0.0293237566947937\n",
      "epoch:0, loss_avg:0.046288490295410156\n",
      "epoch:0, loss_avg:0.024646995589137077\n",
      "epoch:0, loss_avg:0.034864284098148346\n",
      "epoch:0, loss_avg:0.03304896876215935\n",
      "epoch:0, loss_avg:0.029352104291319847\n",
      "epoch:0, loss_avg:0.03204641863703728\n",
      "epoch:0, loss_avg:0.02807423286139965\n",
      "epoch:0, loss_avg:0.027360159903764725\n",
      "epoch:0, loss_avg:0.02182955853641033\n",
      "epoch:0, loss_avg:0.026039501652121544\n",
      "epoch:0, loss_avg:0.1689467579126358\n",
      "epoch:0, loss_avg:0.025073111057281494\n",
      "epoch:0, loss_avg:0.024601489305496216\n",
      "epoch:0, loss_avg:0.03744540363550186\n",
      "epoch:0, loss_avg:0.03400874510407448\n",
      "epoch:0, loss_avg:0.027166837826371193\n",
      "epoch:0, loss_avg:0.8973641395568848\n",
      "epoch:0, loss_avg:0.030992720276117325\n",
      "epoch:0, loss_avg:0.026278384029865265\n",
      "epoch:0, loss_avg:0.029715314507484436\n",
      "epoch:0, loss_avg:0.031828634440898895\n",
      "epoch:0, loss_avg:0.033087581396102905\n",
      "epoch:0, loss_avg:0.026897113770246506\n",
      "epoch:0, loss_avg:0.029435914009809494\n",
      "epoch:0, loss_avg:0.03506770357489586\n",
      "epoch:0, loss_avg:0.027786735445261\n",
      "epoch:0, loss_avg:0.028446782380342484\n",
      "epoch:0, loss_avg:0.05935731902718544\n",
      "epoch:0, loss_avg:0.4223547577857971\n",
      "epoch:0, loss_avg:0.028230421245098114\n",
      "epoch:0, loss_avg:0.02649948000907898\n",
      "epoch:0, loss_avg:0.03046666458249092\n",
      "epoch:0, loss_avg:0.03897082060575485\n",
      "epoch:0, loss_avg:0.03613865748047829\n",
      "epoch:0, loss_avg:0.04518839344382286\n",
      "epoch:0, loss_avg:0.03885209932923317\n",
      "epoch:0, loss_avg:0.02760903351008892\n",
      "epoch:0, loss_avg:0.038637734949588776\n",
      "epoch:0, loss_avg:0.028568260371685028\n",
      "epoch:0, loss_avg:0.03204287961125374\n",
      "epoch:0, loss_avg:0.050154056400060654\n",
      "epoch:0, loss_avg:0.026559002697467804\n",
      "epoch:0, loss_avg:0.028167743235826492\n",
      "epoch:0, loss_avg:0.02972259931266308\n",
      "epoch:0, loss_avg:0.0283641517162323\n",
      "epoch:0, loss_avg:0.04079287871718407\n",
      "epoch:0, loss_avg:0.024447612464427948\n",
      "epoch:0, loss_avg:0.025846844539046288\n",
      "epoch:0, loss_avg:0.02656656876206398\n",
      "epoch:0, loss_avg:0.025065667927265167\n",
      "epoch:0, loss_avg:0.02928091026842594\n",
      "epoch:0, loss_avg:0.023162679746747017\n",
      "epoch:0, loss_avg:0.026995783671736717\n",
      "epoch:0, loss_avg:0.03832682967185974\n",
      "epoch:0, loss_avg:0.025613676756620407\n",
      "epoch:0, loss_avg:0.03131291642785072\n",
      "epoch:0, loss_avg:0.025954488664865494\n",
      "epoch:0, loss_avg:0.026395350694656372\n",
      "epoch:0, loss_avg:0.02640676312148571\n",
      "epoch:0, loss_avg:0.0315459743142128\n",
      "epoch:0, loss_avg:0.022142954170703888\n",
      "epoch:0, loss_avg:0.026896046474575996\n",
      "epoch:0, loss_avg:0.034874510020017624\n",
      "epoch:0, loss_avg:0.03478359058499336\n",
      "epoch:0, loss_avg:0.01853502169251442\n",
      "epoch:0, loss_avg:0.026299724355340004\n",
      "epoch:0, loss_avg:0.05337913706898689\n",
      "epoch:0, loss_avg:0.02457290142774582\n",
      "epoch:0, loss_avg:0.02559683844447136\n",
      "epoch:0, loss_avg:0.02803189866244793\n",
      "epoch:0, loss_avg:0.038935188204050064\n",
      "epoch:0, loss_avg:0.03055238351225853\n",
      "epoch:0, loss_avg:0.02077554725110531\n",
      "epoch:0, loss_avg:0.03229217603802681\n",
      "epoch:0, loss_avg:0.01921999081969261\n",
      "epoch:0, loss_avg:0.02272813208401203\n",
      "epoch:0, loss_avg:0.03225643187761307\n",
      "epoch:0, loss_avg:0.031517744064331055\n",
      "epoch:0, loss_avg:0.0187034010887146\n",
      "epoch:0, loss_avg:0.026644134894013405\n",
      "epoch:0, loss_avg:0.021020878106355667\n",
      "epoch:0, loss_avg:0.030810905620455742\n",
      "epoch:0, loss_avg:0.017926208674907684\n",
      "epoch:0, loss_avg:0.021837756037712097\n",
      "epoch:0, loss_avg:0.023103170096874237\n",
      "epoch:0, loss_avg:0.02004779875278473\n",
      "epoch:0, loss_avg:0.01736639440059662\n",
      "epoch:1, loss_avg:0.02990337833762169\n",
      "epoch:1, loss_avg:0.028470121324062347\n",
      "epoch:1, loss_avg:0.02953352779150009\n",
      "epoch:1, loss_avg:0.038175731897354126\n",
      "epoch:1, loss_avg:0.03155586123466492\n",
      "epoch:1, loss_avg:0.023851023986935616\n",
      "epoch:1, loss_avg:0.03109828568994999\n",
      "epoch:1, loss_avg:0.021976089105010033\n",
      "epoch:1, loss_avg:0.032678619027137756\n",
      "epoch:1, loss_avg:0.031503014266490936\n",
      "epoch:1, loss_avg:0.02620169147849083\n",
      "epoch:1, loss_avg:0.026178451254963875\n",
      "epoch:1, loss_avg:0.032569486647844315\n",
      "epoch:1, loss_avg:0.02508542314171791\n",
      "epoch:1, loss_avg:0.027044987305998802\n",
      "epoch:1, loss_avg:0.027973471209406853\n",
      "epoch:1, loss_avg:0.03125055879354477\n",
      "epoch:1, loss_avg:0.029413878917694092\n",
      "epoch:1, loss_avg:0.03182770311832428\n",
      "epoch:1, loss_avg:0.03424685820937157\n",
      "epoch:1, loss_avg:0.026957744732499123\n",
      "epoch:1, loss_avg:0.026765920221805573\n",
      "epoch:1, loss_avg:0.022393912076950073\n",
      "epoch:1, loss_avg:0.02875478006899357\n",
      "epoch:1, loss_avg:0.026199255138635635\n",
      "epoch:1, loss_avg:0.02871345542371273\n",
      "epoch:1, loss_avg:0.026475757360458374\n",
      "epoch:1, loss_avg:0.028769413009285927\n",
      "epoch:1, loss_avg:0.027420982718467712\n",
      "epoch:1, loss_avg:0.021584540605545044\n",
      "epoch:1, loss_avg:0.03311069309711456\n",
      "epoch:1, loss_avg:0.029851172119379044\n",
      "epoch:1, loss_avg:0.021527301520109177\n",
      "epoch:1, loss_avg:0.024305490776896477\n",
      "epoch:1, loss_avg:0.02766904979944229\n",
      "epoch:1, loss_avg:0.027785388752818108\n",
      "epoch:1, loss_avg:0.02913193590939045\n",
      "epoch:1, loss_avg:0.0199811402708292\n",
      "epoch:1, loss_avg:0.030815724283456802\n",
      "epoch:1, loss_avg:0.026614369824528694\n",
      "epoch:1, loss_avg:0.01891263946890831\n",
      "epoch:1, loss_avg:0.034522898495197296\n",
      "epoch:1, loss_avg:0.033873025327920914\n",
      "epoch:1, loss_avg:0.027563054114580154\n",
      "epoch:1, loss_avg:0.025551535189151764\n",
      "epoch:1, loss_avg:0.019222132861614227\n",
      "epoch:1, loss_avg:0.03135683014988899\n",
      "epoch:1, loss_avg:0.027689067646861076\n",
      "epoch:1, loss_avg:0.017712978646159172\n",
      "epoch:1, loss_avg:0.036132775247097015\n",
      "epoch:1, loss_avg:0.021196771413087845\n",
      "epoch:1, loss_avg:0.02383280359208584\n",
      "epoch:1, loss_avg:0.019581133499741554\n",
      "epoch:1, loss_avg:0.02027272991836071\n",
      "epoch:1, loss_avg:0.018652766942977905\n",
      "epoch:1, loss_avg:0.01832493208348751\n",
      "epoch:1, loss_avg:0.02375868521630764\n",
      "epoch:1, loss_avg:0.03278084844350815\n",
      "epoch:1, loss_avg:0.035863541066646576\n",
      "epoch:1, loss_avg:0.01850036531686783\n",
      "epoch:1, loss_avg:0.030808810144662857\n",
      "epoch:1, loss_avg:0.022544654086232185\n",
      "epoch:1, loss_avg:0.018355999141931534\n",
      "epoch:1, loss_avg:0.03387801721692085\n",
      "epoch:1, loss_avg:0.021987460553646088\n",
      "epoch:1, loss_avg:0.020739978179335594\n",
      "epoch:1, loss_avg:0.03124643675982952\n",
      "epoch:1, loss_avg:0.018088603392243385\n",
      "epoch:1, loss_avg:0.01843227446079254\n",
      "epoch:1, loss_avg:0.018251733854413033\n",
      "epoch:1, loss_avg:0.030701275914907455\n",
      "epoch:1, loss_avg:0.030710935592651367\n",
      "epoch:1, loss_avg:0.029827017337083817\n",
      "epoch:1, loss_avg:0.02177644520998001\n",
      "epoch:1, loss_avg:0.022987455129623413\n",
      "epoch:1, loss_avg:0.02725178748369217\n",
      "epoch:1, loss_avg:0.027469247579574585\n",
      "epoch:1, loss_avg:0.025759991258382797\n",
      "epoch:1, loss_avg:0.024063440039753914\n",
      "epoch:1, loss_avg:0.025136832147836685\n",
      "epoch:1, loss_avg:0.026980578899383545\n",
      "epoch:1, loss_avg:0.022507239133119583\n",
      "epoch:1, loss_avg:0.028026141226291656\n",
      "epoch:1, loss_avg:0.02829914726316929\n",
      "epoch:1, loss_avg:0.016861122101545334\n",
      "epoch:1, loss_avg:0.020025495439767838\n",
      "epoch:1, loss_avg:0.03358371555805206\n",
      "epoch:1, loss_avg:0.02769222855567932\n",
      "epoch:1, loss_avg:0.020524192601442337\n",
      "epoch:1, loss_avg:0.024732070043683052\n",
      "epoch:1, loss_avg:0.028272835537791252\n",
      "epoch:1, loss_avg:0.019350765272974968\n",
      "epoch:1, loss_avg:0.023391909897327423\n",
      "epoch:1, loss_avg:0.036756835877895355\n",
      "epoch:1, loss_avg:0.022412193939089775\n",
      "epoch:1, loss_avg:0.03182092681527138\n",
      "epoch:1, loss_avg:0.02071283385157585\n",
      "epoch:1, loss_avg:0.022482581436634064\n",
      "epoch:1, loss_avg:0.01746329665184021\n",
      "epoch:1, loss_avg:0.027685847133398056\n",
      "epoch:2, loss_avg:0.021318383514881134\n",
      "epoch:2, loss_avg:0.02835303544998169\n",
      "epoch:2, loss_avg:0.03461230918765068\n",
      "epoch:2, loss_avg:0.038304075598716736\n",
      "epoch:2, loss_avg:0.0187949500977993\n",
      "epoch:2, loss_avg:0.032660212367773056\n",
      "epoch:2, loss_avg:0.02293930947780609\n",
      "epoch:2, loss_avg:0.02207694761455059\n",
      "epoch:2, loss_avg:0.022002466022968292\n",
      "epoch:2, loss_avg:0.022035900503396988\n",
      "epoch:2, loss_avg:0.030548546463251114\n",
      "epoch:2, loss_avg:0.029068104922771454\n",
      "epoch:2, loss_avg:0.0211426243185997\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2, loss_avg:0.027044083923101425\n",
      "epoch:2, loss_avg:0.01582355611026287\n",
      "epoch:2, loss_avg:0.025116994976997375\n",
      "epoch:2, loss_avg:0.02083859033882618\n",
      "epoch:2, loss_avg:0.019903333857655525\n",
      "epoch:2, loss_avg:0.018602021038532257\n",
      "epoch:2, loss_avg:0.029292967170476913\n",
      "epoch:2, loss_avg:0.029574625194072723\n",
      "epoch:2, loss_avg:0.02783844992518425\n",
      "epoch:2, loss_avg:0.017868153750896454\n",
      "epoch:2, loss_avg:0.03311767429113388\n",
      "epoch:2, loss_avg:0.02899814583361149\n",
      "epoch:2, loss_avg:0.017255766317248344\n",
      "epoch:2, loss_avg:0.017675848677754402\n",
      "epoch:2, loss_avg:0.025935040786862373\n",
      "epoch:2, loss_avg:0.03375428542494774\n",
      "epoch:2, loss_avg:0.03766346722841263\n",
      "epoch:2, loss_avg:0.044325824826955795\n",
      "epoch:2, loss_avg:0.025999797508120537\n",
      "epoch:2, loss_avg:0.02535869926214218\n",
      "epoch:2, loss_avg:0.02247733809053898\n",
      "epoch:2, loss_avg:0.02021358534693718\n",
      "epoch:2, loss_avg:0.023096725344657898\n",
      "epoch:2, loss_avg:0.02285434864461422\n",
      "epoch:2, loss_avg:0.02700837329030037\n",
      "epoch:2, loss_avg:0.02608666755259037\n",
      "epoch:2, loss_avg:0.025051120668649673\n",
      "epoch:2, loss_avg:0.01923149824142456\n",
      "epoch:2, loss_avg:0.02491503581404686\n",
      "epoch:2, loss_avg:0.029610399156808853\n",
      "epoch:2, loss_avg:0.025393452495336533\n",
      "epoch:2, loss_avg:0.03211094066500664\n",
      "epoch:2, loss_avg:0.027775416150689125\n",
      "epoch:2, loss_avg:0.024022819474339485\n",
      "epoch:2, loss_avg:0.01797829009592533\n",
      "epoch:2, loss_avg:0.022757232189178467\n",
      "epoch:2, loss_avg:0.037210337817668915\n",
      "epoch:2, loss_avg:0.03204522654414177\n",
      "epoch:2, loss_avg:0.02996227890253067\n",
      "epoch:2, loss_avg:0.021773913875222206\n",
      "epoch:2, loss_avg:0.02616155333817005\n",
      "epoch:2, loss_avg:0.017367513850331306\n",
      "epoch:2, loss_avg:0.020135030150413513\n",
      "epoch:2, loss_avg:0.03951805457472801\n",
      "epoch:2, loss_avg:0.032905906438827515\n",
      "epoch:2, loss_avg:0.03869495913386345\n",
      "epoch:2, loss_avg:0.02881975844502449\n",
      "epoch:2, loss_avg:0.02941180393099785\n",
      "epoch:2, loss_avg:0.025187073275446892\n",
      "epoch:2, loss_avg:0.03254405036568642\n",
      "epoch:2, loss_avg:0.02590080350637436\n",
      "epoch:2, loss_avg:0.03466771915555\n",
      "epoch:2, loss_avg:0.023734236136078835\n",
      "epoch:2, loss_avg:0.02468639425933361\n",
      "epoch:2, loss_avg:0.02214953862130642\n",
      "epoch:2, loss_avg:0.026411334052681923\n",
      "epoch:2, loss_avg:0.026445912197232246\n",
      "epoch:2, loss_avg:0.02836773172020912\n",
      "epoch:2, loss_avg:0.026250770315527916\n",
      "epoch:2, loss_avg:0.021598516032099724\n",
      "epoch:2, loss_avg:0.018041495233774185\n",
      "epoch:2, loss_avg:0.030486421659588814\n",
      "epoch:2, loss_avg:0.026945609599351883\n",
      "epoch:2, loss_avg:0.02137780375778675\n",
      "epoch:2, loss_avg:0.025876909494400024\n",
      "epoch:2, loss_avg:0.0260697603225708\n",
      "epoch:2, loss_avg:0.0182686485350132\n",
      "epoch:2, loss_avg:0.017298202961683273\n",
      "epoch:2, loss_avg:0.015729842707514763\n",
      "epoch:2, loss_avg:0.02621842920780182\n",
      "epoch:2, loss_avg:0.03138017654418945\n",
      "epoch:2, loss_avg:0.019749842584133148\n",
      "epoch:2, loss_avg:0.029878662899136543\n",
      "epoch:2, loss_avg:0.03419451043009758\n",
      "epoch:2, loss_avg:0.033862002193927765\n",
      "epoch:2, loss_avg:0.02780938893556595\n",
      "epoch:2, loss_avg:0.030841592699289322\n",
      "epoch:2, loss_avg:0.026191487908363342\n",
      "epoch:2, loss_avg:0.0305198784917593\n",
      "epoch:2, loss_avg:0.022324861958622932\n",
      "epoch:2, loss_avg:0.0195306409150362\n",
      "epoch:2, loss_avg:0.01923307776451111\n",
      "epoch:2, loss_avg:0.024136032909154892\n",
      "epoch:2, loss_avg:0.022353479638695717\n",
      "epoch:2, loss_avg:0.024242183193564415\n",
      "epoch:2, loss_avg:0.024822548031806946\n",
      "epoch:2, loss_avg:0.031322721391916275\n",
      "epoch:3, loss_avg:0.01961527392268181\n",
      "epoch:3, loss_avg:0.022121552377939224\n",
      "epoch:3, loss_avg:0.03643655776977539\n",
      "epoch:3, loss_avg:0.022451505064964294\n",
      "epoch:3, loss_avg:0.03178244084119797\n",
      "epoch:3, loss_avg:0.02683335542678833\n",
      "epoch:3, loss_avg:0.02332286722958088\n",
      "epoch:3, loss_avg:0.02959977649152279\n",
      "epoch:3, loss_avg:0.019704047590494156\n",
      "epoch:3, loss_avg:0.018802952021360397\n",
      "epoch:3, loss_avg:0.030310459434986115\n",
      "epoch:3, loss_avg:0.019023511558771133\n",
      "epoch:3, loss_avg:0.044549547135829926\n",
      "epoch:3, loss_avg:0.03072991967201233\n",
      "epoch:3, loss_avg:0.02707195095717907\n",
      "epoch:3, loss_avg:0.025406744331121445\n",
      "epoch:3, loss_avg:0.02416917122900486\n",
      "epoch:3, loss_avg:0.028437986969947815\n",
      "epoch:3, loss_avg:0.03089289925992489\n",
      "epoch:3, loss_avg:0.029101096093654633\n",
      "epoch:3, loss_avg:0.02099214494228363\n",
      "epoch:3, loss_avg:0.034283801913261414\n",
      "epoch:3, loss_avg:0.01953498087823391\n",
      "epoch:3, loss_avg:0.020320605486631393\n",
      "epoch:3, loss_avg:0.039904408156871796\n",
      "epoch:3, loss_avg:0.02622545138001442\n",
      "epoch:3, loss_avg:0.022203322499990463\n",
      "epoch:3, loss_avg:0.03233413025736809\n",
      "epoch:3, loss_avg:0.026145152747631073\n",
      "epoch:3, loss_avg:0.01868017204105854\n",
      "epoch:3, loss_avg:0.014707079157233238\n",
      "epoch:3, loss_avg:0.031720541417598724\n",
      "epoch:3, loss_avg:0.02237018197774887\n",
      "epoch:3, loss_avg:0.021249637007713318\n",
      "epoch:3, loss_avg:0.034167490899562836\n",
      "epoch:3, loss_avg:0.026510873809456825\n",
      "epoch:3, loss_avg:0.024496709927916527\n",
      "epoch:3, loss_avg:0.0180586539208889\n",
      "epoch:3, loss_avg:0.02376566268503666\n",
      "epoch:3, loss_avg:0.03652910143136978\n",
      "epoch:3, loss_avg:0.032372817397117615\n",
      "epoch:3, loss_avg:0.019344618543982506\n",
      "epoch:3, loss_avg:0.024648655205965042\n",
      "epoch:3, loss_avg:0.020492659881711006\n",
      "epoch:3, loss_avg:0.027774518355727196\n",
      "epoch:3, loss_avg:0.03003668040037155\n",
      "epoch:3, loss_avg:0.027628861367702484\n",
      "epoch:3, loss_avg:0.019262317568063736\n",
      "epoch:3, loss_avg:0.03923158720135689\n",
      "epoch:3, loss_avg:0.03612568974494934\n",
      "epoch:3, loss_avg:0.02141740918159485\n",
      "epoch:3, loss_avg:0.02456498146057129\n",
      "epoch:3, loss_avg:0.02309020608663559\n",
      "epoch:3, loss_avg:0.01588790863752365\n",
      "epoch:3, loss_avg:0.026520851999521255\n",
      "epoch:3, loss_avg:0.024755362421274185\n",
      "epoch:3, loss_avg:0.0223984532058239\n",
      "epoch:3, loss_avg:0.04140208661556244\n",
      "epoch:3, loss_avg:0.017523672431707382\n",
      "epoch:3, loss_avg:0.027196690440177917\n",
      "len:10,w:tensor([1.4803, 2.4232, 2.8447, 2.9603, 0.1841, 3.7215, 2.4082, 2.4778, 4.2559,\n",
      "        0.7370], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(4.0012, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:5000,w:tensor([1.6045, 1.2891, 2.7073,  ..., 1.0312, 3.0259, 4.0611],\n",
      "       grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(5.7378, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:20,w:tensor([2.5256e+00, 3.0224e+00, 2.9335e+00, 3.1319e+00, 1.5710e-01, 3.6336e+00,\n",
      "        2.2131e+00, 2.0541e+00, 3.9588e+00, 1.4373e+00, 1.5267e-01, 3.4461e+00,\n",
      "        4.5700e-01, 2.3584e+00, 1.6823e+00, 2.7619e+00, 2.5056e+00, 9.4934e-01,\n",
      "        2.4869e-04, 1.4676e-01], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(7.7142, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:16000,w:tensor([2.9462, 0.1773, 2.4091,  ..., 1.8677, 3.2248, 2.4633],\n",
      "       grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(9.4327, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:50,w:tensor([2.3646, 2.0083, 4.1955, 1.7869, 0.4745, 1.6410, 2.3893, 0.3828, 2.1777,\n",
      "        1.6309, 2.1710, 1.9650, 0.8757, 2.5941, 0.5335, 2.5989, 2.5090, 3.2219,\n",
      "        2.3278, 1.0412, 0.2002, 1.9481, 2.4416, 0.8316, 3.2272, 3.1178, 1.3038,\n",
      "        1.5540, 1.9702, 1.8665, 0.0851, 4.0037, 1.4327, 1.9798, 0.2487, 2.3767,\n",
      "        1.5761, 2.2324, 1.5469, 1.8445, 0.3082, 2.3207, 1.2954, 1.6455, 1.0987,\n",
      "        2.3955, 1.8553, 1.2751, 0.8677, 2.4519], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(11.2365, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:500,w:tensor([1.0729e+00, 5.0056e-01, 2.0922e-01, 1.5389e+00, 2.2800e+00, 5.9521e-01,\n",
      "        9.4050e-01, 2.8110e-01, 8.8546e-01, 1.5375e+00, 2.4102e+00, 1.4199e-01,\n",
      "        1.4320e+00, 1.2341e+00, 1.2214e-07, 2.2018e+00, 1.7731e+00, 2.3536e+00,\n",
      "        3.2150e-01, 2.5032e+00, 2.0722e+00, 2.1286e+00, 8.9864e-01, 2.0929e+00,\n",
      "        4.2736e-02, 9.4148e-01, 2.1869e-01, 1.7263e+00, 3.3951e+00, 2.1830e+00,\n",
      "        1.6089e+00, 1.6196e-02, 4.2554e+00, 3.4034e+00, 3.1296e+00, 2.0365e+00,\n",
      "        3.8645e+00, 1.7870e-04, 1.3001e+00, 3.0819e-01, 4.3258e-02, 2.4035e+00,\n",
      "        1.4660e+00, 2.9252e+00, 1.9579e-01, 1.7595e+00, 5.5494e-01, 2.4640e+00,\n",
      "        3.0745e+00, 1.4167e+00, 2.3632e+00, 5.9949e-01, 1.0790e+00, 2.2913e+00,\n",
      "        1.4015e+00, 3.4146e-03, 2.0298e+00, 1.8823e-02, 1.7791e+00, 1.3605e-02,\n",
      "        3.7191e+00, 2.0070e+00, 2.1524e-01, 2.4488e+00, 1.8930e+00, 1.8360e+00,\n",
      "        2.0754e+00, 4.1993e+00, 1.2563e+00, 1.5503e-01, 6.1308e-01, 3.8045e-01,\n",
      "        1.3455e+00, 2.1968e+00, 2.6059e+00, 1.7784e+00, 2.3498e+00, 1.3936e-02,\n",
      "        3.4553e-01, 4.6456e-01, 5.6109e-01, 4.9695e-01, 4.0696e+00, 1.1713e+00,\n",
      "        1.8279e+00, 4.1143e+00, 1.4725e+00, 2.1427e+00, 3.9676e+00, 6.0155e-01,\n",
      "        1.2304e+00, 4.6976e-01, 1.9327e+00, 6.4429e-01, 2.4439e+00, 2.5149e-01,\n",
      "        1.7535e+00, 1.6007e+00, 1.9323e+00, 1.2889e+00, 1.1490e+00, 1.0881e+00,\n",
      "        3.6284e+00, 2.4287e+00, 2.9035e+00, 3.5622e-02, 2.7187e+00, 1.0051e+00,\n",
      "        3.6314e+00, 2.4786e-01, 9.2207e-01, 4.3982e-01, 6.2494e-01, 1.6791e+00,\n",
      "        1.3647e+00, 9.0298e-01, 1.0494e+00, 7.7230e-01, 5.4063e-02, 3.5040e+00,\n",
      "        2.4031e+00, 2.6714e+00, 9.3760e-01, 1.4835e+00, 1.3652e+00, 2.7394e+00,\n",
      "        3.9358e-01, 4.5488e+00, 1.1000e+00, 1.9864e+00, 3.6208e-02, 1.4050e+00,\n",
      "        2.1376e+00, 2.3174e+00, 2.3396e+00, 9.0585e-01, 2.7672e+00, 4.0711e+00,\n",
      "        2.9411e+00, 1.5751e+00, 2.0641e+00, 1.0283e+00, 3.0024e+00, 1.5326e+00,\n",
      "        2.7957e+00, 9.3001e-01, 5.1410e-02, 2.7684e-01, 2.2010e+00, 1.6488e+00,\n",
      "        1.5726e+00, 6.9971e-01, 2.3503e+00, 1.2463e+00, 2.3197e+00, 1.1156e+00,\n",
      "        1.5214e+00, 6.2316e+00, 9.5698e-01, 7.3954e-01, 9.9845e-01, 3.6805e-02,\n",
      "        1.0028e+00, 2.0482e+00, 4.0825e+00, 4.0164e+00, 2.4021e+00, 2.9607e-01,\n",
      "        6.6501e-01, 1.8582e+00, 1.6251e+00, 9.2966e-01, 8.2118e-01, 2.1357e+00,\n",
      "        4.6367e-03, 3.1945e+00, 9.7469e-01, 4.8155e-01, 4.1998e-02, 8.6469e-01,\n",
      "        3.0670e+00, 1.0903e+00, 9.9917e-01, 1.7307e+00, 2.6471e+00, 1.9529e+00,\n",
      "        1.9834e+00, 1.6073e-01, 2.9444e+00, 3.5281e+00, 5.5529e-01, 1.3824e+00,\n",
      "        1.5521e+00, 2.7610e+00, 1.4474e+00, 3.2208e+00, 3.5183e-01, 7.5903e-01,\n",
      "        2.3111e+00, 7.8782e-01, 1.6587e+00, 1.2816e-01, 1.2149e+00, 1.3832e+00,\n",
      "        3.5238e+00, 2.3713e-01, 1.7845e+00, 3.0278e+00, 2.1628e+00, 3.7857e-02,\n",
      "        2.0878e+00, 1.8101e+00, 5.0270e-01, 2.1451e+00, 2.8750e-01, 2.1332e+00,\n",
      "        1.5056e+00, 9.4505e-03, 3.1135e-03, 1.7488e+00, 2.1424e-01, 1.6575e+00,\n",
      "        2.6862e+00, 3.0689e+00, 1.5472e+00, 1.3408e+00, 1.4878e+00, 5.1020e-01,\n",
      "        2.4935e+00, 3.8691e-01, 4.5846e+00, 1.4852e+00, 2.9300e+00, 6.3985e-01,\n",
      "        3.8913e+00, 2.2261e+00, 1.1383e+00, 1.0633e+00, 1.7199e-02, 2.7916e+00,\n",
      "        7.9729e-01, 1.5849e+00, 2.3043e+00, 2.4280e+00, 1.1387e+00, 2.5370e+00,\n",
      "        3.4344e+00, 3.9078e+00, 2.0667e+00, 1.7733e+00, 5.2438e+00, 1.6623e+00,\n",
      "        1.2662e+00, 1.1491e+00, 1.2517e+00, 6.3887e-02, 1.2763e+00, 2.5485e+00,\n",
      "        7.9920e-01, 2.6744e+00, 1.0096e+00, 7.3632e-02, 6.9669e-01, 2.0906e+00,\n",
      "        3.2933e+00, 1.9741e+00, 2.2072e+00, 1.4392e+00, 4.2544e-03, 2.5517e+00,\n",
      "        1.8134e+00, 2.1169e+00, 4.5764e+00, 3.0168e+00, 1.1171e-02, 1.6101e+00,\n",
      "        3.0101e+00, 2.6712e+00, 1.4047e+00, 3.6023e+00, 1.3246e+00, 1.3423e+00,\n",
      "        1.5089e+00, 2.0799e+00, 2.7018e+00, 2.6267e-01, 3.5763e+00, 1.5399e+00,\n",
      "        1.3660e+00, 1.7350e+00, 1.0258e+00, 9.0777e-01, 4.2865e-03, 5.0189e-01,\n",
      "        3.9911e-02, 2.9698e+00, 1.7673e+00, 2.9888e+00, 1.9494e-01, 1.0390e+00,\n",
      "        2.3809e+00, 3.4340e+00, 1.8321e+00, 7.4925e-02, 4.1665e+00, 2.2026e+00,\n",
      "        3.6418e+00, 1.8915e+00, 4.5704e+00, 1.4923e+00, 7.3351e-01, 1.5960e+00,\n",
      "        2.3629e+00, 1.9832e+00, 2.2281e+00, 9.6776e-01, 2.5149e-01, 9.7742e-01,\n",
      "        1.7290e+00, 2.2850e+00, 1.7378e+00, 3.1763e+00, 1.4870e+00, 2.3229e+00,\n",
      "        1.6598e+00, 1.5854e+00, 1.4237e+00, 1.9174e+00, 8.2193e-01, 3.8928e-01,\n",
      "        1.2733e+00, 4.8007e-02, 3.9318e-01, 8.6607e-01, 3.1320e+00, 1.9976e+00,\n",
      "        1.5435e+00, 9.2637e-01, 1.2842e+00, 1.4291e+00, 7.7510e-01, 7.3741e-01,\n",
      "        2.3690e+00, 1.4596e+00, 2.0771e+00, 1.0361e+00, 2.9505e+00, 8.5890e-03,\n",
      "        3.7451e-04, 6.9973e-01, 1.8916e+00, 2.4431e+00, 1.6555e+00, 2.2588e+00,\n",
      "        7.2002e-01, 1.1675e+00, 1.5305e+00, 1.5636e+00, 7.3285e-02, 8.2551e-01,\n",
      "        3.1317e+00, 1.5003e+00, 1.5695e+00, 2.8154e+00, 2.3155e+00, 3.3477e+00,\n",
      "        2.6611e+00, 3.2467e+00, 2.3762e+00, 9.0811e-01, 2.1660e+00, 1.8088e-01,\n",
      "        7.3908e-02, 3.4439e+00, 3.4405e-01, 5.1875e-01, 1.2913e+00, 1.3390e+00,\n",
      "        1.4660e+00, 7.5069e-01, 6.0552e-01, 3.0721e+00, 9.6601e-01, 1.1476e+00,\n",
      "        3.3470e+00, 4.8188e-01, 7.3223e-01, 1.6005e+00, 1.9371e+00, 1.7363e+00,\n",
      "        2.3740e+00, 3.1742e+00, 1.2183e+00, 2.9154e+00, 1.8578e+00, 1.5164e+00,\n",
      "        1.0326e+00, 1.4524e+00, 1.8348e+00, 1.0763e+00, 1.6428e+00, 3.3535e+00,\n",
      "        4.5307e-02, 1.2206e+00, 4.2593e+00, 6.0848e-02, 1.2939e+00, 3.0200e-01,\n",
      "        2.4990e+00, 8.3807e-01, 1.4248e+00, 1.3784e+00, 1.9676e+00, 2.4270e+00,\n",
      "        1.8541e+00, 8.4272e-01, 3.6814e-01, 1.5998e+00, 1.8924e+00, 5.9749e-01,\n",
      "        6.7910e-01, 4.8221e+00, 4.4774e+00, 7.2341e-01, 2.3175e+00, 2.8851e+00,\n",
      "        2.3938e-01, 9.9067e-01, 8.5113e-01, 1.0908e+00, 2.0614e+00, 1.8347e+00,\n",
      "        2.4282e+00, 2.8226e+00, 1.3612e+00, 9.0910e-05, 2.3312e-01, 1.5083e-01,\n",
      "        3.9032e+00, 1.3054e+00, 2.1886e+00, 8.8285e-01, 3.6335e+00, 2.1071e-02,\n",
      "        1.1675e-02, 6.6852e-01, 2.4330e+00, 2.4667e+00, 9.2887e-01, 2.2936e+00,\n",
      "        2.6577e+00, 8.3732e-01, 1.4011e+00, 4.5559e+00, 1.5541e+00, 5.2367e-01,\n",
      "        2.2614e+00, 2.7436e+00, 2.6305e-01, 1.4900e-01, 1.7470e+00, 5.2311e-02,\n",
      "        1.4303e+00, 3.8051e-01, 1.5062e+00, 1.8009e+00, 1.9151e+00, 3.1553e+00,\n",
      "        2.1889e+00, 5.6897e-01, 8.8481e-01, 4.2465e-01, 1.3310e-02, 1.4225e+00,\n",
      "        7.3416e-01, 2.3869e+00, 1.1832e+00, 2.5806e+00, 1.4325e+00, 3.1044e+00,\n",
      "        4.9354e-01, 9.5020e-01, 3.3557e-01, 2.2634e+00, 4.9303e-01, 4.5589e-01,\n",
      "        5.3214e-01, 6.3957e-01, 7.1300e-01, 1.9410e+00, 1.0455e+00, 6.9001e-01,\n",
      "        4.5602e+00, 1.1829e+00, 9.5428e-01, 1.2018e+00, 1.6029e+00, 4.4630e+00,\n",
      "        1.0563e+00, 3.7117e-01], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(12.8600, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:10,w:tensor([0.6366, 1.6374, 2.3596, 1.5445, 1.3182, 2.8276, 1.3410, 1.3138, 1.9058,\n",
      "        0.9938], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(14.4478, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, loss_avg:14.447818756103516\n",
      "epoch:3, loss_avg:0.03433889523148537\n",
      "epoch:3, loss_avg:0.06007848307490349\n",
      "epoch:3, loss_avg:0.0695423111319542\n",
      "epoch:3, loss_avg:0.04990913346409798\n",
      "epoch:3, loss_avg:0.08095885813236237\n",
      "epoch:3, loss_avg:0.12989704310894012\n",
      "epoch:3, loss_avg:0.15933442115783691\n",
      "epoch:3, loss_avg:0.16625598073005676\n",
      "epoch:3, loss_avg:0.08287408202886581\n",
      "epoch:3, loss_avg:0.037871625274419785\n",
      "epoch:3, loss_avg:0.06732024252414703\n",
      "len:250,w:tensor([2.1272e+00, 1.5535e+01, 1.2381e+01, 5.7089e-01, 4.8070e+00, 1.9452e+00,\n",
      "        1.1866e+00, 4.7518e+00, 2.3588e-03, 2.7088e-01, 2.5425e+00, 9.8655e-01,\n",
      "        1.0730e+00, 5.8563e-02, 1.7269e-01, 2.1666e+00, 1.1557e+00, 7.3303e-01,\n",
      "        7.3134e-01, 9.6758e+00, 1.5524e+01, 1.9600e+00, 6.3352e+00, 9.1173e+00,\n",
      "        4.2635e+01, 3.3359e+00, 4.8691e-03, 1.9493e+00, 8.5579e-02, 6.1648e-01,\n",
      "        3.2558e-02, 7.0723e+00, 1.1218e+00, 1.1222e+00, 7.0669e-02, 6.2865e+00,\n",
      "        1.0310e+01, 5.8648e-01, 2.4902e-03, 7.6164e-06, 7.2385e+00, 4.2623e+00,\n",
      "        1.1853e-01, 1.1618e+01, 1.5896e+00, 5.5089e+00, 4.0886e-02, 9.7161e-01,\n",
      "        4.4188e+00, 4.9284e-01, 6.2701e-01, 7.1423e-03, 1.6841e+00, 5.2013e+00,\n",
      "        7.3922e-03, 1.4940e-01, 1.8260e+00, 2.0495e+00, 1.0502e+00, 2.2947e+01,\n",
      "        6.3422e+00, 3.4858e+00, 8.0075e-01, 1.3138e+01, 1.1278e+01, 4.9085e+00,\n",
      "        6.5855e-01, 1.1424e+00, 8.2931e-02, 2.2397e+00, 1.4443e+01, 3.1485e-03,\n",
      "        3.7866e-01, 2.2664e+00, 1.2413e-01, 3.0882e-02, 5.9148e+00, 2.0737e+00,\n",
      "        2.5329e+00, 7.3836e+00, 1.0317e+00, 4.4462e-01, 1.4710e+00, 4.6071e+00,\n",
      "        5.5208e+00, 1.0810e+00, 7.4538e+00, 9.7417e-01, 1.2005e+00, 3.9085e-01,\n",
      "        1.6400e+01, 6.2326e+00, 6.9262e+00, 3.5281e-02, 5.8967e+00, 4.0009e+00,\n",
      "        1.6438e-01, 4.7904e+00, 2.0131e+01, 2.1173e+01, 9.2630e-01, 2.4007e+00,\n",
      "        1.0594e+00, 3.2562e-02, 6.3116e-01, 3.7039e+00, 1.7909e+00, 2.7603e+00,\n",
      "        2.9051e-01, 1.8710e+00, 3.2599e+00, 7.6898e-03, 9.0342e-01, 6.6049e-01,\n",
      "        3.5089e+00, 4.9449e-01, 9.6091e-01, 1.1131e+00, 1.1411e+00, 2.1287e+01,\n",
      "        1.0033e+00, 9.1489e+00, 1.5735e+00, 2.0361e+00, 1.6133e+00, 1.3783e+01,\n",
      "        9.4592e-01, 3.0573e-01, 5.8426e-01, 2.3372e+00, 6.6101e-01, 2.0241e+00,\n",
      "        6.6464e-02, 2.5337e+00, 2.2514e+00, 2.3846e+00, 8.2078e-01, 4.4401e+00,\n",
      "        7.4507e+00, 1.6127e+00, 6.8148e+00, 5.9556e+00, 9.3589e-01, 1.0692e+00,\n",
      "        7.2799e+00, 1.0398e-02, 7.9811e-01, 7.1033e-01, 9.4062e+00, 5.8344e+00,\n",
      "        2.1076e+00, 2.5399e-01, 3.1022e+00, 9.3034e-01, 2.8050e+00, 2.7873e+00,\n",
      "        3.7028e+00, 4.0145e+01, 3.5927e+00, 1.0981e+00, 2.0795e+00, 1.0765e+01,\n",
      "        1.9683e-02, 5.8106e-01, 4.8179e+00, 5.4291e-04, 1.5682e+00, 1.3077e+01,\n",
      "        1.0293e+00, 2.5774e-01, 9.4247e+00, 4.3716e-01, 6.9554e-02, 1.2956e+00,\n",
      "        5.4659e-01, 1.2323e+00, 1.2771e+01, 3.6165e+00, 3.1033e+00, 1.6940e+00,\n",
      "        3.0041e-02, 1.0242e+01, 1.0873e-01, 1.2344e-05, 1.4598e-01, 1.7056e+00,\n",
      "        2.4151e+00, 7.3223e+00, 2.3780e+00, 1.7790e+01, 9.1227e-01, 4.0796e+00,\n",
      "        4.2309e-01, 9.6208e-02, 4.0844e+01, 1.2312e-02, 1.0687e+00, 2.1781e+00,\n",
      "        7.7265e-01, 1.2063e-01, 8.0302e+00, 6.4531e-02, 1.2410e+00, 1.0871e+01,\n",
      "        8.1131e-01, 1.6268e+01, 1.6532e+00, 1.7208e+00, 6.3900e-01, 4.5992e-01,\n",
      "        2.9559e+00, 2.4614e+00, 7.3542e-04, 3.1852e-01, 8.6353e-01, 1.6771e+00,\n",
      "        8.0517e-02, 8.6483e+00, 1.8701e+00, 7.7058e-01, 4.5840e+00, 1.4694e-02,\n",
      "        1.7973e+00, 7.3247e-01, 7.2074e+00, 2.7348e+00, 1.1603e-01, 1.3479e+01,\n",
      "        1.7660e-01, 6.2124e-01, 6.1526e+00, 8.1027e+00, 3.7184e+00, 1.3048e-01,\n",
      "        3.8624e+00, 8.0347e-01, 1.2691e-01, 8.4716e+00, 4.1340e-02, 7.4638e+00,\n",
      "        1.6421e+00, 2.5527e+00, 8.0463e-01, 2.6211e+00, 1.4230e-02, 1.5378e-01,\n",
      "        4.1498e+00, 3.1996e+00, 6.2492e-01, 2.5937e+00],\n",
      "       grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(3.7954, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:10,w:tensor([ 2.0694,  4.7914, 10.7801,  1.5256,  0.4972,  0.0874,  6.2549,  3.8344,\n",
      "         3.0648,  0.3804], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(7.1240, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:5000,w:tensor([3.3763e+00, 2.8103e-01, 1.6292e+01,  ..., 1.3178e-02, 1.9886e+00,\n",
      "        2.4439e+00], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(11.5764, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:20,w:tensor([ 1.0080,  0.6157,  1.5375,  3.6667,  1.8442,  0.5346,  0.0265,  8.5868,\n",
      "         4.3569,  0.1838,  4.3848,  2.5719,  3.0994,  0.8773,  1.0659,  7.3588,\n",
      "         2.1209, 14.0439,  2.1969,  1.1602], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(14.6384, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:16000,w:tensor([2.7972, 8.3619, 2.1606,  ..., 2.5792, 0.3252, 0.4572],\n",
      "       grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(19.0925, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:50,w:tensor([7.3759e-01, 3.1579e-01, 6.0366e-01, 1.6404e-01, 5.5327e-04, 1.0823e+01,\n",
      "        4.1399e-01, 3.8553e+00, 5.3466e+00, 7.5045e-01, 2.1387e+00, 3.4266e+00,\n",
      "        3.5369e+00, 1.9562e+00, 5.9491e-05, 2.5910e-01, 2.0940e+00, 6.5988e+00,\n",
      "        1.1214e+00, 1.3364e-01, 3.9660e-03, 1.3965e+00, 5.4956e+00, 1.0141e+01,\n",
      "        4.2859e+00, 1.4944e-02, 5.1980e+00, 1.0536e+01, 1.4658e-01, 4.7179e-02,\n",
      "        1.0317e-01, 2.4782e-01, 1.4790e-01, 2.3004e+00, 2.6790e-01, 2.7931e+00,\n",
      "        2.0125e+00, 7.0839e-02, 4.6214e+00, 3.1378e+00, 1.0948e+01, 1.8846e-01,\n",
      "        8.6545e+00, 7.0315e-01, 1.2233e+01, 2.8945e+01, 8.7574e+00, 3.3588e+00,\n",
      "        4.8336e-03, 7.9439e+00], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(22.6722, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:500,w:tensor([4.5121e+00, 1.8708e-01, 6.5542e+00, 1.0211e+01, 6.2985e-02, 3.8332e-01,\n",
      "        1.0067e+01, 3.4803e+00, 1.6862e+00, 5.2461e-01, 6.4697e+00, 2.1751e+00,\n",
      "        1.7471e+00, 1.1664e+01, 5.9181e-01, 1.7747e+00, 4.1979e-02, 2.4955e+00,\n",
      "        1.2799e+00, 1.4395e+01, 7.2789e-02, 4.6543e-02, 1.2342e+00, 1.2070e+00,\n",
      "        9.9889e-02, 6.0110e+00, 1.1075e+01, 4.0088e-02, 2.7379e+00, 1.3079e-02,\n",
      "        1.0182e-01, 2.0078e-01, 8.8125e-02, 3.5617e-01, 1.5225e-01, 1.4856e+01,\n",
      "        1.2735e-01, 9.6209e+00, 4.1297e+00, 2.9009e-01, 1.4238e+01, 3.8489e-01,\n",
      "        4.3089e+00, 1.7892e+00, 1.6233e+01, 7.4649e-01, 3.2662e-01, 5.2995e-01,\n",
      "        3.1356e-01, 2.2424e+00, 2.4984e-02, 1.1413e+00, 2.0423e+00, 1.8413e+00,\n",
      "        3.1028e+01, 1.9760e+01, 5.5817e-02, 2.3197e+01, 2.2104e+00, 1.7525e+00,\n",
      "        1.7138e+00, 2.1377e-01, 1.6622e-01, 5.7326e-02, 1.9465e+00, 3.4941e+00,\n",
      "        1.9190e+01, 1.4478e-04, 5.3843e-01, 6.8599e+00, 2.2751e-01, 3.2770e+00,\n",
      "        2.2437e+00, 8.1993e+00, 2.4080e+00, 4.6289e+00, 3.2965e+00, 2.4882e+00,\n",
      "        4.3216e-01, 3.0801e+00, 4.1008e-01, 7.9395e+00, 3.6867e+00, 5.1851e-01,\n",
      "        2.1987e-01, 1.0576e-01, 8.1588e+00, 6.5263e-01, 9.0021e+00, 2.6545e-01,\n",
      "        3.8359e+00, 9.0486e-02, 9.5388e-02, 1.6042e+00, 1.3485e+01, 3.3500e+00,\n",
      "        3.2766e+00, 2.4856e-01, 2.8892e+00, 2.6549e+00, 1.4286e+01, 2.2379e+00,\n",
      "        5.3251e-01, 2.1930e+00, 5.3777e-01, 3.4609e+00, 5.1207e+00, 5.3754e+00,\n",
      "        1.0391e+01, 9.4720e-01, 1.4382e+00, 1.1554e+01, 2.1837e+00, 3.9742e+00,\n",
      "        1.8399e-02, 1.3173e-02, 2.5933e+01, 8.6006e-01, 1.3523e-01, 2.4210e-01,\n",
      "        7.0403e-01, 5.1840e-02, 1.0384e+00, 1.2037e+01, 1.4085e+01, 1.7315e+00,\n",
      "        1.0813e-02, 7.7757e-01, 2.3097e+00, 1.4601e+01, 8.7953e-01, 6.7017e+00,\n",
      "        3.9915e+00, 8.4660e+00, 2.9113e+00, 1.0944e+00, 4.7440e+00, 8.0570e+00,\n",
      "        1.8874e+00, 1.8039e+01, 5.1837e-02, 6.5663e+00, 4.1818e+00, 9.4990e+00,\n",
      "        5.0013e+00, 7.0211e+00, 6.2313e+00, 6.4001e-01, 1.7012e+01, 4.8164e-01,\n",
      "        2.2187e-01, 3.3408e+00, 1.4888e+00, 1.0395e+01, 2.5035e-05, 1.3474e+00,\n",
      "        2.5660e+00, 1.4688e+01, 4.5389e-03, 1.2476e+00, 4.6075e-01, 2.1968e+00,\n",
      "        4.8129e-01, 2.0538e-03, 5.8687e+00, 8.3404e+00, 8.3926e-01, 6.6173e-01,\n",
      "        1.4553e+00, 2.6029e-01, 2.1956e+00, 3.4588e+00, 5.6782e-01, 4.5964e+00,\n",
      "        1.3740e-01, 2.4693e-01, 4.4089e-01, 4.2589e+00, 1.4485e-01, 9.6823e+00,\n",
      "        3.3358e-01, 2.6666e+01, 9.2917e-01, 5.5438e+00, 8.3190e+00, 6.5587e-01,\n",
      "        6.0865e+00, 1.1188e+01, 8.6959e-01, 6.4681e-01, 2.9882e+01, 6.5563e+00,\n",
      "        4.5543e-01, 1.2282e+00, 2.4838e+01, 3.9206e+00, 6.3126e-02, 6.2731e-01,\n",
      "        1.9411e+00, 5.6520e+00, 1.0728e-01, 2.7283e+01, 4.0399e+01, 3.4473e+00,\n",
      "        5.4312e-01, 3.1809e+00, 1.6805e+00, 1.3021e+00, 2.4163e+01, 3.1867e-01,\n",
      "        4.5433e-02, 9.4165e+00, 7.0462e-01, 7.7578e-03, 1.8327e+00, 1.8953e+00,\n",
      "        1.0993e+01, 6.0260e+00, 1.0235e+01, 8.0320e-02, 1.1369e+01, 6.0841e+00,\n",
      "        1.2952e+00, 1.9892e+00, 7.1582e+00, 4.5521e+00, 2.4590e+00, 1.2957e-02,\n",
      "        1.1790e+01, 6.9422e+00, 1.2120e+01, 1.0815e+00, 3.9996e+00, 7.1048e+00,\n",
      "        1.3649e-01, 3.6848e+00, 1.0847e-01, 2.7539e+00, 1.4459e-01, 2.6748e+00,\n",
      "        5.3394e-01, 1.4865e-02, 7.2133e-01, 5.7457e+00, 2.4074e+00, 4.7453e+00,\n",
      "        7.9236e+00, 1.5418e+00, 1.3554e+00, 1.3023e+01, 1.4434e+01, 2.7682e+00,\n",
      "        1.7556e+00, 2.8324e-02, 1.3057e+00, 1.1600e+01, 8.3151e+00, 2.2431e+00,\n",
      "        8.8168e+00, 7.8786e-01, 4.3232e+00, 2.9656e+01, 1.6964e-01, 1.5731e+01,\n",
      "        2.0124e+00, 2.4660e+01, 3.5591e+00, 2.8052e-02, 1.2578e+01, 1.3563e+00,\n",
      "        2.2152e-01, 1.1593e+00, 2.7059e+00, 4.8486e+00, 4.0290e+00, 5.9458e-01,\n",
      "        9.0940e+00, 1.1534e+00, 1.3405e+01, 9.1088e+00, 3.4065e+00, 6.5410e-01,\n",
      "        1.5559e+01, 1.6780e+00, 1.6194e+01, 2.5191e+01, 5.7619e-04, 2.7264e-01,\n",
      "        1.0930e+00, 3.3052e+00, 1.6304e+00, 4.9837e+00, 1.1527e+00, 1.2460e+00,\n",
      "        5.2432e+00, 8.7227e+00, 1.1262e+01, 5.2425e-04, 2.9388e-01, 3.4331e-01,\n",
      "        4.5692e+00, 4.8367e+00, 1.5752e+00, 6.5798e+00, 2.1130e+01, 1.9168e+00,\n",
      "        7.5418e+00, 3.3484e-02, 2.4687e-01, 3.4077e-03, 2.6507e-02, 7.1992e-01,\n",
      "        4.9309e+00, 5.4174e-01, 6.9120e-01, 1.9119e+01, 2.1146e+00, 7.2628e+00,\n",
      "        3.5563e-01, 2.3585e+01, 6.2020e+00, 4.1425e+00, 8.7008e+00, 1.3436e+00,\n",
      "        1.0898e+01, 4.1831e+00, 7.9837e+00, 8.4810e+00, 4.7760e+00, 1.8030e+00,\n",
      "        8.4220e-01, 2.6389e+00, 5.1897e-01, 1.9861e+01, 2.2055e-01, 4.4763e+00,\n",
      "        2.2714e+00, 7.4176e-01, 2.0949e+00, 4.3094e+00, 5.1695e+00, 7.2662e+00,\n",
      "        1.8158e-02, 4.6237e+00, 2.5618e+00, 1.0673e+01, 9.3855e+00, 1.6070e+01,\n",
      "        3.6051e+01, 1.3508e+00, 6.3496e+00, 6.8040e-02, 1.3576e+01, 6.1521e+00,\n",
      "        1.1221e+01, 5.2892e-01, 4.0092e+00, 1.6411e+00, 4.1390e+01, 3.5373e+00,\n",
      "        2.5608e-02, 8.1465e+00, 1.4909e+00, 4.1194e+00, 1.9383e-01, 4.1214e+00,\n",
      "        3.1276e+01, 1.0840e+00, 7.6160e-01, 4.7458e+00, 2.0682e+00, 7.7345e-01,\n",
      "        1.1922e-02, 2.3481e+00, 1.0172e+00, 2.2178e-02, 9.8790e+00, 4.2963e+00,\n",
      "        4.4069e+00, 2.8612e+00, 3.0390e-02, 5.7403e-01, 1.2512e-01, 1.5092e-01,\n",
      "        1.9513e-03, 1.7806e+00, 1.8616e+00, 4.0057e-01, 9.0079e-01, 9.5159e-01,\n",
      "        2.0352e+00, 5.4472e-01, 9.4889e+00, 5.0487e+00, 9.0898e+00, 2.1855e+01,\n",
      "        8.3666e+00, 5.1455e+00, 2.9026e+01, 7.9603e+00, 3.0421e+00, 4.6163e-02,\n",
      "        1.7108e+00, 7.8499e+00, 5.6316e+00, 1.1479e+00, 4.5458e+00, 2.6342e+00,\n",
      "        1.3244e-02, 7.9701e-01, 1.4519e+01, 5.3166e+00, 2.6818e+00, 5.2782e-02,\n",
      "        7.1163e+00, 1.3729e-01, 1.1328e+00, 4.7486e-01, 1.3602e-01, 5.9969e+00,\n",
      "        7.7872e+00, 1.6917e+00, 3.0328e-01, 9.8439e-01, 2.4863e+00, 1.4017e+01,\n",
      "        6.4946e-01, 1.1448e+00, 1.1938e+00, 1.0553e+00, 1.6860e-06, 5.6209e+00,\n",
      "        4.4403e+00, 1.1613e+01, 3.8980e-01, 2.6152e+00, 2.6195e+01, 3.5173e+00,\n",
      "        2.9116e+01, 1.9332e+01, 3.1877e-01, 2.6121e+00, 1.7594e-01, 7.2629e-01,\n",
      "        3.1352e-01, 5.6524e-01, 1.4650e+01, 8.1746e+00, 5.4309e+00, 5.3066e+00,\n",
      "        3.1093e-01, 7.2186e-01, 1.3680e-02, 4.2184e+00, 5.4824e+00, 1.3636e+00,\n",
      "        2.3930e+01, 4.0440e+00, 9.3040e+00, 4.5134e+00, 4.8544e+00, 4.8443e-03,\n",
      "        2.5921e+00, 2.8316e+00, 6.2541e-01, 1.2667e+01, 3.2084e+00, 2.5738e+00,\n",
      "        1.7231e+01, 1.2760e+00, 1.3837e+00, 2.5894e+01, 1.6568e+01, 1.0310e+01,\n",
      "        4.7058e+00, 1.6943e+00, 2.0849e-01, 1.9175e+00, 1.6755e+01, 2.8441e+00,\n",
      "        4.2001e+00, 5.7214e+01, 2.5515e+00, 4.9333e+00, 2.3602e-01, 7.4993e+00,\n",
      "        4.3489e-01, 1.3159e+00, 3.0406e-02, 2.6305e-01, 1.1492e+01, 4.6392e+00,\n",
      "        1.6705e+00, 5.2192e+00, 3.0212e+00, 3.8618e-01, 1.7286e+01, 1.4247e+00,\n",
      "        2.5488e-03, 4.0609e+00], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(27.7565, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:10,w:tensor([5.4056e+00, 3.8032e-02, 1.1071e+00, 7.3223e+00, 2.2254e+00, 1.6235e+00,\n",
      "        3.0899e-01, 5.6945e-01, 1.5204e+00, 1.0474e-03],\n",
      "       grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(29.7687, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:3, loss_avg:29.76873207092285\n",
      "epoch:3, loss_avg:0.5339899659156799\n",
      "epoch:3, loss_avg:0.8385155200958252\n",
      "epoch:3, loss_avg:0.8665698170661926\n",
      "epoch:3, loss_avg:0.5719996094703674\n",
      "epoch:3, loss_avg:0.22365537285804749\n",
      "epoch:3, loss_avg:0.0839826837182045\n",
      "epoch:3, loss_avg:0.2064957320690155\n",
      "epoch:3, loss_avg:0.41568851470947266\n",
      "epoch:3, loss_avg:0.5073569416999817\n",
      "epoch:3, loss_avg:0.4297260046005249\n",
      "epoch:3, loss_avg:0.21911722421646118\n",
      "epoch:3, loss_avg:0.07380358874797821\n",
      "epoch:3, loss_avg:0.07002899050712585\n",
      "epoch:3, loss_avg:0.18367978930473328\n",
      "epoch:3, loss_avg:0.2530127167701721\n",
      "epoch:3, loss_avg:0.22272944450378418\n",
      "epoch:3, loss_avg:0.15020811557769775\n",
      "epoch:3, loss_avg:0.08754447102546692\n",
      "epoch:3, loss_avg:0.0785725936293602\n",
      "epoch:3, loss_avg:0.10162685811519623\n",
      "epoch:3, loss_avg:0.1329973340034485\n",
      "epoch:3, loss_avg:0.10177411139011383\n",
      "epoch:3, loss_avg:0.08118711411952972\n",
      "epoch:3, loss_avg:0.06293879449367523\n",
      "epoch:3, loss_avg:0.08443368226289749\n",
      "epoch:3, loss_avg:0.10712230950593948\n",
      "epoch:3, loss_avg:0.07454574853181839\n",
      "epoch:4, loss_avg:0.05014660954475403\n",
      "epoch:4, loss_avg:0.036565326154232025\n",
      "epoch:4, loss_avg:0.04535677284002304\n",
      "epoch:4, loss_avg:0.06298750638961792\n",
      "epoch:4, loss_avg:0.08197903633117676\n",
      "epoch:4, loss_avg:0.05171535909175873\n",
      "epoch:4, loss_avg:0.02607412450015545\n",
      "epoch:4, loss_avg:0.04226776212453842\n",
      "epoch:4, loss_avg:0.048744313418865204\n",
      "epoch:4, loss_avg:0.05698501318693161\n",
      "epoch:4, loss_avg:0.04221569001674652\n",
      "epoch:4, loss_avg:0.039072297513484955\n",
      "epoch:4, loss_avg:0.02864229865372181\n",
      "epoch:4, loss_avg:0.030047524720430374\n",
      "epoch:4, loss_avg:0.03260275349020958\n",
      "epoch:4, loss_avg:0.035778455436229706\n",
      "epoch:4, loss_avg:0.02738022990524769\n",
      "epoch:4, loss_avg:0.034615859389305115\n",
      "epoch:4, loss_avg:0.0366772823035717\n",
      "epoch:4, loss_avg:0.039453379809856415\n",
      "epoch:4, loss_avg:0.027050375938415527\n",
      "epoch:4, loss_avg:0.02369212917983532\n",
      "epoch:4, loss_avg:0.019859835505485535\n",
      "epoch:4, loss_avg:0.024898089468479156\n",
      "epoch:4, loss_avg:0.02480999380350113\n",
      "epoch:4, loss_avg:0.035729967057704926\n",
      "epoch:4, loss_avg:0.021732108667492867\n",
      "epoch:4, loss_avg:0.030099425464868546\n",
      "epoch:4, loss_avg:0.03191816061735153\n",
      "epoch:4, loss_avg:0.0409323088824749\n",
      "epoch:4, loss_avg:0.024978630244731903\n",
      "epoch:4, loss_avg:0.02810685709118843\n",
      "epoch:4, loss_avg:0.029133038595318794\n",
      "epoch:4, loss_avg:0.02397965081036091\n",
      "epoch:4, loss_avg:0.037189584225416183\n",
      "epoch:4, loss_avg:0.029263323172926903\n",
      "epoch:4, loss_avg:0.03379788249731064\n",
      "epoch:4, loss_avg:0.01981920748949051\n",
      "epoch:4, loss_avg:0.03342131897807121\n",
      "epoch:4, loss_avg:0.024326827377080917\n",
      "epoch:4, loss_avg:0.035902589559555054\n",
      "epoch:4, loss_avg:0.04369378089904785\n",
      "epoch:4, loss_avg:0.024303698912262917\n",
      "epoch:4, loss_avg:0.02420603111386299\n",
      "epoch:4, loss_avg:0.050081923604011536\n",
      "epoch:4, loss_avg:0.028766335919499397\n",
      "epoch:4, loss_avg:0.025484751909971237\n",
      "epoch:4, loss_avg:0.021649140864610672\n",
      "epoch:4, loss_avg:0.04043301194906235\n",
      "epoch:4, loss_avg:0.03136957064270973\n",
      "epoch:4, loss_avg:0.045526064932346344\n",
      "epoch:4, loss_avg:0.027975087985396385\n",
      "epoch:4, loss_avg:0.027564626187086105\n",
      "epoch:4, loss_avg:0.03235209360718727\n",
      "epoch:4, loss_avg:0.0342269241809845\n",
      "epoch:4, loss_avg:0.024612346664071083\n",
      "epoch:4, loss_avg:0.025098232552409172\n",
      "epoch:4, loss_avg:0.03097052127122879\n",
      "epoch:4, loss_avg:0.02262708730995655\n",
      "epoch:4, loss_avg:0.03501882031559944\n",
      "epoch:4, loss_avg:0.03708958253264427\n",
      "epoch:4, loss_avg:0.030690647661685944\n",
      "epoch:4, loss_avg:0.026342328637838364\n",
      "epoch:4, loss_avg:0.03800705820322037\n",
      "epoch:4, loss_avg:0.030642027035355568\n",
      "epoch:4, loss_avg:0.02424084208905697\n",
      "epoch:4, loss_avg:0.022135745733976364\n",
      "epoch:4, loss_avg:0.034192945808172226\n",
      "epoch:4, loss_avg:0.03110891953110695\n",
      "epoch:4, loss_avg:0.029056042432785034\n",
      "epoch:4, loss_avg:0.04033076763153076\n",
      "epoch:4, loss_avg:0.027781419456005096\n",
      "epoch:4, loss_avg:0.029958801344037056\n",
      "epoch:4, loss_avg:0.033211223781108856\n",
      "epoch:4, loss_avg:0.03164748474955559\n",
      "epoch:4, loss_avg:0.0252373106777668\n",
      "epoch:4, loss_avg:0.025391587987542152\n",
      "epoch:4, loss_avg:0.024463947862386703\n",
      "epoch:4, loss_avg:0.03650546446442604\n",
      "epoch:4, loss_avg:0.031120462343096733\n",
      "epoch:4, loss_avg:0.034615930169820786\n",
      "epoch:4, loss_avg:0.021052487194538116\n",
      "epoch:4, loss_avg:0.033036548644304276\n",
      "epoch:4, loss_avg:0.019350208342075348\n",
      "epoch:4, loss_avg:0.02512306161224842\n",
      "epoch:4, loss_avg:0.01960688643157482\n",
      "epoch:4, loss_avg:0.02232915349304676\n",
      "epoch:4, loss_avg:0.0356961265206337\n",
      "epoch:4, loss_avg:0.026084352284669876\n",
      "epoch:4, loss_avg:0.02070927619934082\n",
      "epoch:4, loss_avg:0.025336522608995438\n",
      "epoch:4, loss_avg:0.03231624886393547\n",
      "epoch:4, loss_avg:0.03547060117125511\n",
      "epoch:4, loss_avg:0.023751022294163704\n",
      "epoch:4, loss_avg:0.03536684438586235\n",
      "epoch:4, loss_avg:0.0311663206666708\n",
      "epoch:4, loss_avg:0.021995071321725845\n",
      "epoch:4, loss_avg:0.028447089716792107\n",
      "epoch:4, loss_avg:0.030366485938429832\n",
      "epoch:4, loss_avg:0.02654741145670414\n",
      "epoch:5, loss_avg:0.026533164083957672\n",
      "epoch:5, loss_avg:0.02093721181154251\n",
      "epoch:5, loss_avg:0.01939433254301548\n",
      "epoch:5, loss_avg:0.03591296076774597\n",
      "epoch:5, loss_avg:0.02715981751680374\n",
      "epoch:5, loss_avg:0.02057492733001709\n",
      "epoch:5, loss_avg:0.025949910283088684\n",
      "epoch:5, loss_avg:0.03527249023318291\n",
      "epoch:5, loss_avg:0.035980358719825745\n",
      "epoch:5, loss_avg:0.024657633155584335\n",
      "epoch:5, loss_avg:0.03558914735913277\n",
      "epoch:5, loss_avg:0.038930974900722504\n",
      "epoch:5, loss_avg:0.03403904289007187\n",
      "epoch:5, loss_avg:0.03499613702297211\n",
      "epoch:5, loss_avg:0.030973276123404503\n",
      "epoch:5, loss_avg:0.03265293315052986\n",
      "epoch:5, loss_avg:0.028266793116927147\n",
      "epoch:5, loss_avg:0.02583826333284378\n",
      "epoch:5, loss_avg:0.02110649272799492\n",
      "epoch:5, loss_avg:0.02451140433549881\n",
      "epoch:5, loss_avg:0.035393912345170975\n",
      "epoch:5, loss_avg:0.021242205053567886\n",
      "epoch:5, loss_avg:0.019467739388346672\n",
      "epoch:5, loss_avg:0.03169240802526474\n",
      "epoch:5, loss_avg:0.02918401174247265\n",
      "epoch:5, loss_avg:0.024535808712244034\n",
      "epoch:5, loss_avg:0.02212624065577984\n",
      "epoch:5, loss_avg:0.026682795956730843\n",
      "epoch:5, loss_avg:0.018014194443821907\n",
      "epoch:5, loss_avg:0.03033415600657463\n",
      "epoch:5, loss_avg:0.03105495683848858\n",
      "epoch:5, loss_avg:0.0368058905005455\n",
      "epoch:5, loss_avg:0.025934813544154167\n",
      "epoch:5, loss_avg:0.03537847101688385\n",
      "epoch:5, loss_avg:0.02031574957072735\n",
      "epoch:5, loss_avg:0.029397860169410706\n",
      "epoch:5, loss_avg:0.029960479587316513\n",
      "epoch:5, loss_avg:0.03505738824605942\n",
      "epoch:5, loss_avg:0.025996971875429153\n",
      "epoch:5, loss_avg:0.02770669385790825\n",
      "epoch:5, loss_avg:0.022953316569328308\n",
      "epoch:5, loss_avg:0.0324893482029438\n",
      "epoch:5, loss_avg:0.030669670552015305\n",
      "epoch:5, loss_avg:0.02389960177242756\n",
      "epoch:5, loss_avg:0.02541566640138626\n",
      "epoch:5, loss_avg:0.02570301853120327\n",
      "epoch:5, loss_avg:0.039027318358421326\n",
      "epoch:5, loss_avg:0.027989717200398445\n",
      "epoch:5, loss_avg:0.02681547962129116\n",
      "epoch:5, loss_avg:0.031188607215881348\n",
      "epoch:5, loss_avg:0.034666553139686584\n",
      "epoch:5, loss_avg:0.029504239559173584\n",
      "epoch:5, loss_avg:0.020196279510855675\n",
      "epoch:5, loss_avg:0.032081905752420425\n",
      "epoch:5, loss_avg:0.0343555249273777\n",
      "epoch:5, loss_avg:0.021471761167049408\n",
      "epoch:5, loss_avg:0.025139981880784035\n",
      "epoch:5, loss_avg:0.024782583117485046\n",
      "epoch:5, loss_avg:0.025675617158412933\n",
      "epoch:5, loss_avg:0.02829487808048725\n",
      "epoch:5, loss_avg:0.033421590924263\n",
      "epoch:5, loss_avg:0.028293831273913383\n",
      "epoch:5, loss_avg:0.029734620824456215\n",
      "epoch:5, loss_avg:0.020111622288823128\n",
      "epoch:5, loss_avg:0.03341888636350632\n",
      "epoch:5, loss_avg:0.031789667904376984\n",
      "epoch:5, loss_avg:0.018009984865784645\n",
      "epoch:5, loss_avg:0.02828080952167511\n",
      "epoch:5, loss_avg:0.022602960467338562\n",
      "epoch:5, loss_avg:0.0383262038230896\n",
      "epoch:5, loss_avg:0.024954114109277725\n",
      "epoch:5, loss_avg:0.03942182660102844\n",
      "epoch:5, loss_avg:0.03333120048046112\n",
      "epoch:5, loss_avg:0.02275680936872959\n",
      "epoch:5, loss_avg:0.027256805449724197\n",
      "epoch:5, loss_avg:0.02161485143005848\n",
      "epoch:5, loss_avg:0.02483246847987175\n",
      "epoch:5, loss_avg:0.025015421211719513\n",
      "epoch:5, loss_avg:0.024052614346146584\n",
      "epoch:5, loss_avg:0.02934666909277439\n",
      "epoch:5, loss_avg:0.03392734378576279\n",
      "epoch:5, loss_avg:0.027192922309041023\n",
      "epoch:5, loss_avg:0.029772603884339333\n",
      "epoch:5, loss_avg:0.029530515894293785\n",
      "epoch:5, loss_avg:0.030710836872458458\n",
      "epoch:5, loss_avg:0.022885946556925774\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:5, loss_avg:0.026787608861923218\n",
      "epoch:5, loss_avg:0.02199394255876541\n",
      "epoch:5, loss_avg:0.021833879873156548\n",
      "epoch:5, loss_avg:0.03173618018627167\n",
      "epoch:5, loss_avg:0.029743172228336334\n",
      "epoch:5, loss_avg:0.020397311076521873\n",
      "epoch:5, loss_avg:0.021688148379325867\n",
      "epoch:5, loss_avg:0.019113799557089806\n",
      "epoch:5, loss_avg:0.02561766281723976\n",
      "epoch:5, loss_avg:0.026842957362532616\n",
      "epoch:5, loss_avg:0.023107344284653664\n",
      "epoch:5, loss_avg:0.0386764220893383\n",
      "epoch:5, loss_avg:0.022374533116817474\n",
      "epoch:5, loss_avg:0.03448573872447014\n",
      "epoch:6, loss_avg:0.032405588775873184\n",
      "epoch:6, loss_avg:0.024789676070213318\n",
      "epoch:6, loss_avg:0.02291075512766838\n",
      "epoch:6, loss_avg:0.025056947022676468\n",
      "epoch:6, loss_avg:0.02115209773182869\n",
      "epoch:6, loss_avg:0.029216155409812927\n",
      "epoch:6, loss_avg:0.03404322266578674\n",
      "epoch:6, loss_avg:0.03394370898604393\n",
      "epoch:6, loss_avg:0.03845585882663727\n",
      "epoch:6, loss_avg:0.027736550197005272\n",
      "epoch:6, loss_avg:0.032433219254016876\n",
      "epoch:6, loss_avg:0.024425523355603218\n",
      "epoch:6, loss_avg:0.029704954475164413\n",
      "epoch:6, loss_avg:0.028164513409137726\n",
      "epoch:6, loss_avg:0.01978583261370659\n",
      "epoch:6, loss_avg:0.029282912611961365\n",
      "epoch:6, loss_avg:0.024608716368675232\n",
      "epoch:6, loss_avg:0.02681783400475979\n",
      "epoch:6, loss_avg:0.022069036960601807\n",
      "epoch:6, loss_avg:0.024741720408201218\n",
      "epoch:6, loss_avg:0.0210894625633955\n",
      "epoch:6, loss_avg:0.0286369938403368\n",
      "epoch:6, loss_avg:0.03259623050689697\n",
      "epoch:6, loss_avg:0.031520962715148926\n",
      "epoch:6, loss_avg:0.03242763131856918\n",
      "epoch:6, loss_avg:0.028174275532364845\n",
      "epoch:6, loss_avg:0.024662114679813385\n",
      "epoch:6, loss_avg:0.023794833570718765\n",
      "epoch:6, loss_avg:0.032216593623161316\n",
      "epoch:6, loss_avg:0.038869887590408325\n",
      "epoch:6, loss_avg:0.030254114419221878\n",
      "epoch:6, loss_avg:0.029043175280094147\n",
      "epoch:6, loss_avg:0.02440677024424076\n",
      "epoch:6, loss_avg:0.039427462965250015\n",
      "epoch:6, loss_avg:0.028239114210009575\n",
      "epoch:6, loss_avg:0.03025907278060913\n",
      "epoch:6, loss_avg:0.03668807074427605\n",
      "epoch:6, loss_avg:0.03201724961400032\n",
      "epoch:6, loss_avg:0.027079837396740913\n",
      "epoch:6, loss_avg:0.02103164978325367\n",
      "epoch:6, loss_avg:0.03587653487920761\n",
      "epoch:6, loss_avg:0.02338014543056488\n",
      "epoch:6, loss_avg:0.02346920594573021\n",
      "epoch:6, loss_avg:0.02379731461405754\n",
      "epoch:6, loss_avg:0.03428196161985397\n",
      "epoch:6, loss_avg:0.03345055505633354\n",
      "epoch:6, loss_avg:0.016065895557403564\n",
      "epoch:6, loss_avg:0.024018224328756332\n",
      "epoch:6, loss_avg:0.02974363975226879\n",
      "epoch:6, loss_avg:0.02327645570039749\n",
      "epoch:6, loss_avg:0.0287871602922678\n",
      "epoch:6, loss_avg:0.03865697234869003\n",
      "epoch:6, loss_avg:0.024617359042167664\n",
      "epoch:6, loss_avg:0.036796316504478455\n",
      "epoch:6, loss_avg:0.022551391273736954\n",
      "epoch:6, loss_avg:0.028186682611703873\n",
      "epoch:6, loss_avg:0.028773214668035507\n",
      "epoch:6, loss_avg:0.041002679616212845\n",
      "epoch:6, loss_avg:0.02621809020638466\n",
      "epoch:6, loss_avg:0.032329823821783066\n",
      "epoch:6, loss_avg:0.04138588532805443\n",
      "epoch:6, loss_avg:0.03063214011490345\n",
      "epoch:6, loss_avg:0.029183853417634964\n",
      "epoch:6, loss_avg:0.027844872325658798\n",
      "epoch:6, loss_avg:0.034884531050920486\n",
      "epoch:6, loss_avg:0.019388096407055855\n",
      "epoch:6, loss_avg:0.036846209317445755\n",
      "epoch:6, loss_avg:0.0499894954264164\n",
      "epoch:6, loss_avg:0.0330420657992363\n",
      "epoch:6, loss_avg:0.042653005570173264\n",
      "epoch:6, loss_avg:0.02927306480705738\n",
      "epoch:6, loss_avg:0.02744574286043644\n",
      "epoch:6, loss_avg:0.025003677234053612\n",
      "epoch:6, loss_avg:0.033384498208761215\n",
      "epoch:6, loss_avg:0.023084428161382675\n",
      "epoch:6, loss_avg:0.02930321916937828\n",
      "epoch:6, loss_avg:0.023942453786730766\n",
      "epoch:6, loss_avg:0.03766285628080368\n",
      "epoch:6, loss_avg:0.021978730335831642\n",
      "epoch:6, loss_avg:0.022733058780431747\n",
      "epoch:6, loss_avg:0.031871404498815536\n",
      "epoch:6, loss_avg:0.03217453509569168\n",
      "epoch:6, loss_avg:0.021502472460269928\n",
      "epoch:6, loss_avg:0.021526290103793144\n",
      "epoch:6, loss_avg:0.032744619995355606\n",
      "epoch:6, loss_avg:0.027103373780846596\n",
      "epoch:6, loss_avg:0.022177839651703835\n",
      "epoch:6, loss_avg:0.03239770978689194\n",
      "epoch:6, loss_avg:0.02165820449590683\n",
      "epoch:6, loss_avg:0.03165232017636299\n",
      "epoch:6, loss_avg:0.02486244961619377\n",
      "epoch:6, loss_avg:0.03017386421561241\n",
      "epoch:6, loss_avg:0.033500686287879944\n",
      "epoch:6, loss_avg:0.02568596415221691\n",
      "epoch:6, loss_avg:0.03809354826807976\n",
      "epoch:6, loss_avg:0.027589313685894012\n",
      "epoch:6, loss_avg:0.028834199532866478\n",
      "epoch:6, loss_avg:0.03670952841639519\n",
      "epoch:6, loss_avg:0.037818264216184616\n",
      "epoch:6, loss_avg:0.02056051976978779\n",
      "epoch:7, loss_avg:0.03060225583612919\n",
      "epoch:7, loss_avg:0.025495357811450958\n",
      "epoch:7, loss_avg:0.030247356742620468\n",
      "epoch:7, loss_avg:0.030079925432801247\n",
      "epoch:7, loss_avg:0.03006906807422638\n",
      "epoch:7, loss_avg:0.025248277932405472\n",
      "epoch:7, loss_avg:0.02466590143740177\n",
      "epoch:7, loss_avg:0.03149230033159256\n",
      "epoch:7, loss_avg:0.02486635558307171\n",
      "epoch:7, loss_avg:0.028449464589357376\n",
      "epoch:7, loss_avg:0.018107574433088303\n",
      "epoch:7, loss_avg:0.016266843304038048\n",
      "epoch:7, loss_avg:0.03291681781411171\n",
      "epoch:7, loss_avg:0.02092510461807251\n",
      "epoch:7, loss_avg:0.018847797065973282\n",
      "epoch:7, loss_avg:0.0416078083217144\n",
      "epoch:7, loss_avg:0.020780885592103004\n",
      "epoch:7, loss_avg:0.03223540261387825\n",
      "epoch:7, loss_avg:0.030454332008957863\n",
      "epoch:7, loss_avg:0.017513586208224297\n",
      "epoch:7, loss_avg:0.033601339906454086\n",
      "epoch:7, loss_avg:0.02709420770406723\n",
      "epoch:7, loss_avg:0.019718311727046967\n",
      "epoch:7, loss_avg:0.024096379056572914\n",
      "epoch:7, loss_avg:0.03475144878029823\n",
      "epoch:7, loss_avg:0.0279809832572937\n",
      "epoch:7, loss_avg:0.030169183388352394\n",
      "epoch:7, loss_avg:0.029864560812711716\n",
      "epoch:7, loss_avg:0.03477366641163826\n",
      "epoch:7, loss_avg:0.02157874032855034\n",
      "epoch:7, loss_avg:0.023101842030882835\n",
      "epoch:7, loss_avg:0.0332653783261776\n",
      "epoch:7, loss_avg:0.021087005734443665\n",
      "epoch:7, loss_avg:0.030053211376070976\n",
      "epoch:7, loss_avg:0.017537487670779228\n",
      "epoch:7, loss_avg:0.028827575966715813\n",
      "epoch:7, loss_avg:0.023523585870862007\n",
      "epoch:7, loss_avg:0.0273855309933424\n",
      "epoch:7, loss_avg:0.024662964046001434\n",
      "epoch:7, loss_avg:0.021908409893512726\n",
      "epoch:7, loss_avg:0.02388998121023178\n",
      "epoch:7, loss_avg:0.025241727009415627\n",
      "epoch:7, loss_avg:0.0399797223508358\n",
      "epoch:7, loss_avg:0.027579672634601593\n",
      "epoch:7, loss_avg:0.020583268254995346\n",
      "epoch:7, loss_avg:0.02537960186600685\n",
      "epoch:7, loss_avg:0.035697340965270996\n",
      "epoch:7, loss_avg:0.02577965520322323\n",
      "epoch:7, loss_avg:0.02501855418086052\n",
      "epoch:7, loss_avg:0.034190692007541656\n",
      "epoch:7, loss_avg:0.04046943411231041\n",
      "epoch:7, loss_avg:0.0277759600430727\n",
      "epoch:7, loss_avg:0.03269395977258682\n",
      "epoch:7, loss_avg:0.020678531378507614\n",
      "epoch:7, loss_avg:0.02657761052250862\n",
      "epoch:7, loss_avg:0.03377816826105118\n",
      "epoch:7, loss_avg:0.028320638462901115\n",
      "epoch:7, loss_avg:0.021747712045907974\n",
      "epoch:7, loss_avg:0.0254640132188797\n",
      "epoch:7, loss_avg:0.02233668603003025\n",
      "epoch:7, loss_avg:0.021289866417646408\n",
      "epoch:7, loss_avg:0.017410684376955032\n",
      "epoch:7, loss_avg:0.02070835791528225\n",
      "epoch:7, loss_avg:0.021779371425509453\n",
      "epoch:7, loss_avg:0.020522398874163628\n",
      "epoch:7, loss_avg:0.024090413004159927\n",
      "epoch:7, loss_avg:0.01888464204967022\n",
      "epoch:7, loss_avg:0.025458326563239098\n",
      "epoch:7, loss_avg:0.029384350404143333\n",
      "epoch:7, loss_avg:0.024983271956443787\n",
      "epoch:7, loss_avg:0.03644658997654915\n",
      "epoch:7, loss_avg:0.029104072600603104\n",
      "epoch:7, loss_avg:0.025659458711743355\n",
      "epoch:7, loss_avg:0.0207205917686224\n",
      "epoch:7, loss_avg:0.02555013634264469\n",
      "epoch:7, loss_avg:0.0233201514929533\n",
      "epoch:7, loss_avg:0.020089926198124886\n",
      "epoch:7, loss_avg:0.02984895557165146\n",
      "epoch:7, loss_avg:0.03968300670385361\n",
      "epoch:7, loss_avg:0.026609009131789207\n",
      "epoch:7, loss_avg:0.021334748715162277\n",
      "epoch:7, loss_avg:0.03501161187887192\n",
      "epoch:7, loss_avg:0.023181404918432236\n",
      "epoch:7, loss_avg:0.02981543354690075\n",
      "epoch:7, loss_avg:0.03373095765709877\n",
      "epoch:7, loss_avg:0.02730383910238743\n",
      "epoch:7, loss_avg:0.024072129279375076\n",
      "epoch:7, loss_avg:0.020618783310055733\n",
      "epoch:7, loss_avg:0.02511562965810299\n",
      "epoch:7, loss_avg:0.03299502283334732\n",
      "epoch:7, loss_avg:0.029447041451931\n",
      "epoch:7, loss_avg:0.021492552012205124\n",
      "epoch:7, loss_avg:0.01896527409553528\n",
      "epoch:7, loss_avg:0.02522319182753563\n",
      "epoch:7, loss_avg:0.03905836120247841\n",
      "epoch:7, loss_avg:0.020989568904042244\n",
      "epoch:7, loss_avg:0.023668358102440834\n",
      "epoch:7, loss_avg:0.02446044236421585\n",
      "epoch:7, loss_avg:0.035551462322473526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:7, loss_avg:0.02338111586868763\n",
      "epoch:8, loss_avg:0.016840705648064613\n",
      "epoch:8, loss_avg:0.024840380996465683\n",
      "epoch:8, loss_avg:0.028746772557497025\n",
      "epoch:8, loss_avg:0.03975165635347366\n",
      "epoch:8, loss_avg:0.02925858646631241\n",
      "epoch:8, loss_avg:0.029707351699471474\n",
      "epoch:8, loss_avg:0.02214149758219719\n",
      "epoch:8, loss_avg:0.026620924472808838\n",
      "epoch:8, loss_avg:0.03282255679368973\n",
      "epoch:8, loss_avg:0.02751324325799942\n",
      "epoch:8, loss_avg:0.020535286515951157\n",
      "epoch:8, loss_avg:0.02649335004389286\n",
      "epoch:8, loss_avg:0.021418137475848198\n",
      "epoch:8, loss_avg:0.02924240753054619\n",
      "epoch:8, loss_avg:0.03362872824072838\n",
      "epoch:8, loss_avg:0.038606759160757065\n",
      "epoch:8, loss_avg:0.031401317566633224\n",
      "epoch:8, loss_avg:0.03226163983345032\n",
      "epoch:8, loss_avg:0.026493331417441368\n",
      "epoch:8, loss_avg:0.03173384070396423\n",
      "epoch:8, loss_avg:0.03464833274483681\n",
      "epoch:8, loss_avg:0.021564297378063202\n",
      "epoch:8, loss_avg:0.031065991148352623\n",
      "epoch:8, loss_avg:0.03712431341409683\n",
      "epoch:8, loss_avg:0.02490568533539772\n",
      "epoch:8, loss_avg:0.029911348596215248\n",
      "epoch:8, loss_avg:0.027294689789414406\n",
      "epoch:8, loss_avg:0.03313327953219414\n",
      "epoch:8, loss_avg:0.04466699808835983\n",
      "epoch:8, loss_avg:0.03448554873466492\n",
      "epoch:8, loss_avg:0.01607050746679306\n",
      "epoch:8, loss_avg:0.022357070818543434\n",
      "epoch:8, loss_avg:0.03372461721301079\n",
      "epoch:8, loss_avg:0.021646514534950256\n",
      "epoch:8, loss_avg:0.0266256146132946\n",
      "epoch:8, loss_avg:0.02066604234278202\n",
      "epoch:8, loss_avg:0.03291710466146469\n",
      "epoch:8, loss_avg:0.023261340335011482\n",
      "epoch:8, loss_avg:0.025910094380378723\n",
      "epoch:8, loss_avg:0.029952313750982285\n",
      "epoch:8, loss_avg:0.022356171160936356\n",
      "epoch:8, loss_avg:0.02688303031027317\n",
      "epoch:8, loss_avg:0.033429477363824844\n",
      "epoch:8, loss_avg:0.025310399010777473\n",
      "epoch:8, loss_avg:0.03952673077583313\n",
      "epoch:8, loss_avg:0.022272666916251183\n",
      "epoch:8, loss_avg:0.019578292965888977\n",
      "epoch:8, loss_avg:0.032838478684425354\n",
      "epoch:8, loss_avg:0.03253542259335518\n",
      "epoch:8, loss_avg:0.028059568256139755\n",
      "epoch:8, loss_avg:0.027468012645840645\n",
      "epoch:8, loss_avg:0.02584023028612137\n",
      "epoch:8, loss_avg:0.02105565182864666\n",
      "epoch:8, loss_avg:0.02366679720580578\n",
      "epoch:8, loss_avg:0.021156124770641327\n",
      "epoch:8, loss_avg:0.017837129533290863\n",
      "epoch:8, loss_avg:0.025972377508878708\n",
      "epoch:8, loss_avg:0.018185626715421677\n",
      "epoch:8, loss_avg:0.038080159574747086\n",
      "epoch:8, loss_avg:0.02236628346145153\n",
      "epoch:8, loss_avg:0.020549271255731583\n",
      "epoch:8, loss_avg:0.01619368977844715\n",
      "epoch:8, loss_avg:0.04727911576628685\n",
      "epoch:8, loss_avg:0.021387791261076927\n",
      "epoch:8, loss_avg:0.028185341507196426\n",
      "epoch:8, loss_avg:0.03243735060095787\n",
      "epoch:8, loss_avg:0.024301588535308838\n",
      "epoch:8, loss_avg:0.02081485092639923\n",
      "epoch:8, loss_avg:0.02097889967262745\n",
      "epoch:8, loss_avg:0.02271169424057007\n",
      "epoch:8, loss_avg:0.020115874707698822\n",
      "epoch:8, loss_avg:0.024189859628677368\n",
      "epoch:8, loss_avg:0.04074021428823471\n",
      "epoch:8, loss_avg:0.02527831867337227\n",
      "epoch:8, loss_avg:0.030655501410365105\n",
      "epoch:8, loss_avg:0.020748227834701538\n",
      "epoch:8, loss_avg:0.02311798371374607\n",
      "epoch:8, loss_avg:0.023119116201996803\n",
      "epoch:8, loss_avg:0.036627888679504395\n",
      "epoch:8, loss_avg:0.026656216010451317\n",
      "epoch:8, loss_avg:0.02984081394970417\n",
      "epoch:8, loss_avg:0.01956537365913391\n",
      "epoch:8, loss_avg:0.04287802055478096\n",
      "epoch:8, loss_avg:0.02177238091826439\n",
      "epoch:8, loss_avg:0.0360468327999115\n",
      "epoch:8, loss_avg:0.026199594140052795\n",
      "epoch:8, loss_avg:0.02823381870985031\n",
      "epoch:8, loss_avg:0.023909300565719604\n",
      "epoch:8, loss_avg:0.021585894748568535\n",
      "epoch:8, loss_avg:0.02645629458129406\n",
      "epoch:8, loss_avg:0.02861045114696026\n",
      "epoch:8, loss_avg:0.02639414370059967\n",
      "epoch:8, loss_avg:0.020070312544703484\n",
      "epoch:8, loss_avg:0.03096863627433777\n",
      "epoch:8, loss_avg:0.02382400631904602\n",
      "epoch:8, loss_avg:0.04213639348745346\n",
      "epoch:8, loss_avg:0.029133222997188568\n",
      "epoch:8, loss_avg:0.023560188710689545\n",
      "epoch:8, loss_avg:0.03335300832986832\n",
      "epoch:8, loss_avg:0.035531070083379745\n",
      "epoch:9, loss_avg:0.02930597960948944\n",
      "epoch:9, loss_avg:0.01653415895998478\n",
      "epoch:9, loss_avg:0.024194980040192604\n",
      "epoch:9, loss_avg:0.02767694555222988\n",
      "epoch:9, loss_avg:0.0195230171084404\n",
      "epoch:9, loss_avg:0.01908944547176361\n",
      "epoch:9, loss_avg:0.03463678061962128\n",
      "epoch:9, loss_avg:0.03194519877433777\n",
      "epoch:9, loss_avg:0.03182994946837425\n",
      "epoch:9, loss_avg:0.020176393911242485\n",
      "epoch:9, loss_avg:0.02636018395423889\n",
      "epoch:9, loss_avg:0.017697539180517197\n",
      "epoch:9, loss_avg:0.029279567301273346\n",
      "epoch:9, loss_avg:0.024904297664761543\n",
      "epoch:9, loss_avg:0.032028134912252426\n",
      "epoch:9, loss_avg:0.017883887514472008\n",
      "epoch:9, loss_avg:0.029294034466147423\n",
      "epoch:9, loss_avg:0.0250525064766407\n",
      "epoch:9, loss_avg:0.024813499301671982\n",
      "epoch:9, loss_avg:0.023480195552110672\n",
      "epoch:9, loss_avg:0.03450895473361015\n",
      "epoch:9, loss_avg:0.025727232918143272\n",
      "epoch:9, loss_avg:0.020885439589619637\n",
      "epoch:9, loss_avg:0.027017420157790184\n",
      "epoch:9, loss_avg:0.021926309913396835\n",
      "epoch:9, loss_avg:0.022930772975087166\n",
      "epoch:9, loss_avg:0.02870233915746212\n",
      "epoch:9, loss_avg:0.01856905408203602\n",
      "epoch:9, loss_avg:0.03476428613066673\n",
      "epoch:9, loss_avg:0.027082068845629692\n",
      "epoch:9, loss_avg:0.028951914981007576\n",
      "epoch:9, loss_avg:0.035860199481248856\n",
      "epoch:9, loss_avg:0.028617825359106064\n",
      "epoch:9, loss_avg:0.02687409147620201\n",
      "epoch:9, loss_avg:0.03329330310225487\n",
      "epoch:9, loss_avg:0.027286091819405556\n",
      "epoch:9, loss_avg:0.029839973896741867\n",
      "epoch:9, loss_avg:0.029382845386862755\n",
      "epoch:9, loss_avg:0.03402911126613617\n",
      "epoch:9, loss_avg:0.027801847085356712\n",
      "epoch:9, loss_avg:0.026143431663513184\n",
      "epoch:9, loss_avg:0.03233431279659271\n",
      "epoch:9, loss_avg:0.03365139290690422\n",
      "epoch:9, loss_avg:0.022716963663697243\n",
      "epoch:9, loss_avg:0.04029468446969986\n",
      "epoch:9, loss_avg:0.026819558814167976\n",
      "epoch:9, loss_avg:0.028952134773135185\n",
      "epoch:9, loss_avg:0.027428993955254555\n",
      "epoch:9, loss_avg:0.028242550790309906\n",
      "epoch:9, loss_avg:0.028326427564024925\n",
      "epoch:9, loss_avg:0.02898568846285343\n",
      "epoch:9, loss_avg:0.026391921564936638\n",
      "epoch:9, loss_avg:0.02351120486855507\n",
      "epoch:9, loss_avg:0.02362666465342045\n",
      "epoch:9, loss_avg:0.025293683633208275\n",
      "epoch:9, loss_avg:0.03287043049931526\n",
      "epoch:9, loss_avg:0.02408628910779953\n",
      "epoch:9, loss_avg:0.028455430641770363\n",
      "epoch:9, loss_avg:0.02604912407696247\n",
      "epoch:9, loss_avg:0.023607809096574783\n",
      "epoch:9, loss_avg:0.026429008692502975\n",
      "epoch:9, loss_avg:0.021830258890986443\n",
      "epoch:9, loss_avg:0.023750176653265953\n",
      "epoch:9, loss_avg:0.026654569432139397\n",
      "epoch:9, loss_avg:0.03114599734544754\n",
      "epoch:9, loss_avg:0.0254649855196476\n",
      "epoch:9, loss_avg:0.025708762928843498\n",
      "epoch:9, loss_avg:0.02260410226881504\n",
      "epoch:9, loss_avg:0.021201515570282936\n",
      "epoch:9, loss_avg:0.03000790625810623\n",
      "epoch:9, loss_avg:0.021352576091885567\n",
      "epoch:9, loss_avg:0.02671535313129425\n",
      "epoch:9, loss_avg:0.01980985514819622\n",
      "epoch:9, loss_avg:0.019950276240706444\n",
      "epoch:9, loss_avg:0.02120785042643547\n",
      "epoch:9, loss_avg:0.03067622520029545\n",
      "epoch:9, loss_avg:0.03951484337449074\n",
      "epoch:9, loss_avg:0.03145253285765648\n",
      "epoch:9, loss_avg:0.02784796804189682\n",
      "epoch:9, loss_avg:0.03137148171663284\n",
      "epoch:9, loss_avg:0.0224659014493227\n",
      "epoch:9, loss_avg:0.020254453644156456\n",
      "epoch:9, loss_avg:0.029693348333239555\n",
      "epoch:9, loss_avg:0.019083978608250618\n",
      "epoch:9, loss_avg:0.033433545380830765\n",
      "epoch:9, loss_avg:0.03077945113182068\n",
      "epoch:9, loss_avg:0.02749478444457054\n",
      "epoch:9, loss_avg:0.020352117717266083\n",
      "epoch:9, loss_avg:0.023125039413571358\n",
      "epoch:9, loss_avg:0.02742108516395092\n",
      "epoch:9, loss_avg:0.021846652030944824\n",
      "epoch:9, loss_avg:0.028841866180300713\n",
      "epoch:9, loss_avg:0.03656446188688278\n",
      "epoch:9, loss_avg:0.027175264433026314\n",
      "epoch:9, loss_avg:0.02873208560049534\n",
      "epoch:9, loss_avg:0.022480590268969536\n",
      "epoch:9, loss_avg:0.026364240795373917\n",
      "epoch:9, loss_avg:0.0363798663020134\n",
      "epoch:9, loss_avg:0.02280888333916664\n",
      "epoch:9, loss_avg:0.022022627294063568\n"
     ]
    }
   ],
   "source": [
    "layer_dict = {}\n",
    "layer_name = []\n",
    "count = 0\n",
    "for name in w_glob.keys():\n",
    "    if count % 2 == 0:\n",
    "        layer_name.append(name.split('.',1)[0])\n",
    "    count += 1\n",
    "    \n",
    "for i in layer_name:\n",
    "#     layer_dict[i] = CNNCifarEmb(torch.cat([w_glob[i+'.weight'].reshape(1,-1), w_glob[i+'.bias'].reshape(1,-1)], 1).numel())\n",
    "    layer_dict[i] = CNNMnistEmb(torch.cat([w_glob[i+'.weight'].reshape(1,-1), w_glob[i+'.bias'].reshape(1,-1)], 1).numel())\n",
    "# emb_reverse = CNNCifarEmbReverse(args)\n",
    "emb_reverse = CNNMnistEmbReverse(args)\n",
    "# print(w_glob[i+'.weight'].reshape(1,-1).numpy()[0])\n",
    "\n",
    "# print(w_glob[i+'.bias'].numpy())\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':layer_dict[layer_name[0]].parameters()},\n",
    "    {'params':layer_dict[layer_name[1]].parameters()},\n",
    "    {'params':layer_dict[layer_name[2]].parameters()},\n",
    "    {'params':layer_dict[layer_name[3]].parameters()},\n",
    "#     {'params':layer_dict[layer_name[4]].parameters()},\n",
    "    {'params':emb_reverse.parameters()}\n",
    "] ,0.01)\n",
    "\n",
    "for iter in range(args.emb_train_epochs):\n",
    "    idxs_users = np.random.choice(range(args.num_users), 100, replace=False)\n",
    "    \n",
    "    for idx in idxs_users:\n",
    "#         local = LocalUpdate_divide(args=args, dataset=dataset_train, idxs=dict_users[0][idx])\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "#         w, loss, loss_list = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1).to(args.device))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1).to(args.device))\n",
    "        avg_emb_feature = emb_feature/4\n",
    "        transform_w = emb_reverse.forward(avg_emb_feature)\n",
    "        loss_w = [sum((w[i].reshape(1,-1) - transform_w[i].reshape(1,-1)) ** 2) for i in w_glob.keys()]\n",
    "        loss_avg = 0\n",
    "        loss_check_dict = {}\n",
    "        for i in range(len(loss_w)):\n",
    "            loss_avg += sum(loss_w[i])/len(loss_w[i])\n",
    "            loss_check_dict[len(loss_w[i])] = loss_w[i]\n",
    "            if loss_avg.item() > 3.0:\n",
    "                print('len:{},w:{}'.format(len(loss_w[i]), loss_check_dict[len(loss_w[i])]))\n",
    "                print('loss_sum:', loss_avg)\n",
    "                print('\\n***************')\n",
    "        optimizer.zero_grad()\n",
    "        loss_avg.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        print('epoch:{}, loss_avg:{}'.format(iter, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = MemoryBuffer(500)\n",
    "dqn = DQN(parameter_dim, action_dim, replay_buffer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FedPareto(w, action, choice):\n",
    "    w_chosen = []\n",
    "    for i in choice:\n",
    "        w_chosen.append(w[i])\n",
    "    w_avg = copy.deepcopy(w_chosen[0])\n",
    "    for k in w_avg.keys():\n",
    "        for i in range(0, len(w_chosen)):\n",
    "            if i==0:\n",
    "                w_avg[k] = action[i] * w_chosen[i][k]\n",
    "            else:\n",
    "                w_avg[k] += action[i] * w_chosen[i][k]\n",
    "    return w_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "def test_img(net_g, datatest, args):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=args.bs)\n",
    "    l = len(data_loader)\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        if args.gpu != -1:\n",
    "            data, target = data, target\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if args.verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag\n",
      "44\n",
      "tensor(11.7467)\n",
      "Round   0, Average loss -0.454\n",
      "saved\n",
      "flag\n",
      "44\n",
      "tensor(9.9300)\n",
      "Round   1, Average loss -0.461\n",
      "flag\n",
      "44\n",
      "tensor(64.9533)\n",
      "Round   2, Average loss -0.210\n",
      "flag\n",
      "44\n",
      "tensor(82.1650)\n",
      "Round   3, Average loss -0.110\n",
      "flag\n",
      "44\n",
      "tensor(86.4650)\n",
      "Round   4, Average loss -0.083\n",
      "flag\n",
      "44\n",
      "tensor(87.0400)\n",
      "Round   5, Average loss -0.080\n",
      "flag\n",
      "21\n",
      "tensor(87.4500)\n",
      "Round   6, Average loss -0.077\n",
      "flag\n",
      "93\n",
      "tensor(85.5750)\n",
      "Round   7, Average loss -0.089\n",
      "flag\n",
      "9\n",
      "tensor(86.7367)\n",
      "Round   8, Average loss -0.081\n",
      "flag\n",
      "70\n",
      "tensor(85.8133)\n",
      "Round   9, Average loss -0.087\n",
      "flag\n",
      "70\n",
      "tensor(86.1883)\n",
      "Round  10, Average loss -0.085\n",
      "saved\n",
      "flag\n",
      "70\n",
      "tensor(85.8583)\n",
      "Round  11, Average loss -0.087\n",
      "flag\n",
      "10\n",
      "tensor(86.5917)\n",
      "Round  12, Average loss -0.082\n",
      "flag\n",
      "82\n",
      "tensor(86.1050)\n",
      "Round  13, Average loss -0.086\n",
      "flag\n",
      "13\n",
      "tensor(87.9217)\n",
      "Round  14, Average loss -0.074\n",
      "flag\n",
      "85\n",
      "tensor(89.5517)\n",
      "Round  15, Average loss -0.063\n",
      "flag\n",
      "85\n",
      "tensor(88.5350)\n",
      "Round  16, Average loss -0.070\n",
      "flag\n",
      "85\n",
      "tensor(89.1350)\n",
      "Round  17, Average loss -0.066\n",
      "flag\n",
      "85\n",
      "tensor(89.6750)\n",
      "Round  18, Average loss -0.063\n",
      "flag\n",
      "9\n",
      "tensor(86.9700)\n",
      "Round  19, Average loss -0.080\n",
      "flag\n",
      "9\n",
      "tensor(86.9967)\n",
      "Round  20, Average loss -0.080\n",
      "saved\n",
      "flag\n",
      "9\n",
      "tensor(89.1550)\n",
      "Round  21, Average loss -0.066\n",
      "flag\n",
      "67\n",
      "tensor(88.5083)\n",
      "Round  22, Average loss -0.070\n",
      "flag\n",
      "67\n",
      "tensor(87.5700)\n",
      "Round  23, Average loss -0.076\n",
      "flag\n",
      "32\n",
      "tensor(89.4050)\n",
      "Round  24, Average loss -0.064\n",
      "flag\n",
      "32\n",
      "tensor(87.6550)\n",
      "Round  25, Average loss -0.076\n",
      "flag\n",
      "53\n",
      "tensor(84.3533)\n",
      "Round  26, Average loss -0.097\n",
      "flag\n",
      "81\n",
      "tensor(86.9433)\n",
      "Round  27, Average loss -0.080\n",
      "flag\n",
      "76\n",
      "tensor(85.4567)\n",
      "Round  28, Average loss -0.090\n",
      "flag\n",
      "76\n",
      "tensor(86.6367)\n",
      "Round  29, Average loss -0.082\n",
      "flag\n",
      "36\n",
      "tensor(89.4167)\n",
      "Round  30, Average loss -0.064\n",
      "saved\n",
      "flag\n",
      "52\n",
      "tensor(88.4650)\n",
      "Round  31, Average loss -0.070\n",
      "flag\n",
      "52\n",
      "tensor(88.0650)\n",
      "Round  32, Average loss -0.073\n",
      "flag\n",
      "70\n",
      "tensor(85.4267)\n",
      "Round  33, Average loss -0.090\n",
      "flag\n",
      "19\n",
      "tensor(85.2600)\n",
      "Round  34, Average loss -0.091\n",
      "flag\n",
      "22\n",
      "tensor(87.9917)\n",
      "Round  35, Average loss -0.073\n",
      "flag\n",
      "67\n",
      "tensor(80.9450)\n",
      "Round  36, Average loss -0.118\n",
      "flag\n",
      "17\n",
      "tensor(88.5200)\n",
      "Round  37, Average loss -0.070\n",
      "flag\n",
      "59\n",
      "tensor(72.4133)\n",
      "Round  38, Average loss -0.168\n",
      "flag\n",
      "44\n",
      "tensor(74.5017)\n",
      "Round  39, Average loss -0.156\n",
      "flag\n",
      "44\n",
      "tensor(9.5133)\n",
      "Round  40, Average loss -0.462\n",
      "saved\n",
      "flag\n",
      "95\n",
      "tensor(66.2183)\n",
      "Round  41, Average loss -0.203\n",
      "flag\n",
      "95\n",
      "tensor(76.3850)\n",
      "Round  42, Average loss -0.145\n",
      "flag\n",
      "3\n",
      "tensor(81.8967)\n",
      "Round  43, Average loss -0.112\n",
      "flag\n",
      "3\n",
      "tensor(85.2950)\n",
      "Round  44, Average loss -0.091\n",
      "flag\n",
      "50\n",
      "tensor(82.2300)\n",
      "Round  45, Average loss -0.110\n",
      "flag\n",
      "17\n",
      "tensor(87.6983)\n",
      "Round  46, Average loss -0.075\n",
      "flag\n",
      "82\n",
      "tensor(86.2183)\n",
      "Round  47, Average loss -0.085\n",
      "flag\n",
      "54\n",
      "tensor(86.2550)\n",
      "Round  48, Average loss -0.085\n",
      "flag\n",
      "32\n",
      "tensor(87.2283)\n",
      "Round  49, Average loss -0.078\n",
      "flag\n",
      "24\n",
      "tensor(86.5250)\n",
      "Round  50, Average loss -0.083\n",
      "saved\n",
      "flag\n",
      "96\n",
      "tensor(86.6733)\n",
      "Round  51, Average loss -0.082\n",
      "flag\n",
      "19\n",
      "tensor(86.0850)\n",
      "Round  52, Average loss -0.086\n",
      "flag\n",
      "19\n",
      "tensor(86.8167)\n",
      "Round  53, Average loss -0.081\n",
      "flag\n",
      "51\n",
      "tensor(86.2000)\n",
      "Round  54, Average loss -0.085\n",
      "flag\n",
      "54\n",
      "tensor(85.9650)\n",
      "Round  55, Average loss -0.086\n",
      "flag\n",
      "54\n",
      "tensor(84.0133)\n",
      "Round  56, Average loss -0.099\n",
      "flag\n",
      "8\n",
      "tensor(87.1717)\n",
      "Round  57, Average loss -0.079\n",
      "flag\n",
      "8\n",
      "tensor(85.6867)\n",
      "Round  58, Average loss -0.088\n",
      "flag\n",
      "8\n",
      "tensor(89.0733)\n",
      "Round  59, Average loss -0.066\n",
      "flag\n",
      "8\n",
      "tensor(88.3200)\n",
      "Round  60, Average loss -0.071\n",
      "saved\n",
      "flag\n",
      "10\n",
      "tensor(88.9783)\n",
      "Round  61, Average loss -0.067\n",
      "flag\n",
      "10\n",
      "tensor(88.4167)\n",
      "Round  62, Average loss -0.071\n",
      "flag\n",
      "96\n",
      "tensor(89.9833)\n",
      "Round  63, Average loss -0.061\n",
      "flag\n",
      "56\n",
      "tensor(88.8550)\n",
      "Round  64, Average loss -0.068\n",
      "flag\n",
      "56\n",
      "tensor(87.4750)\n",
      "Round  65, Average loss -0.077\n",
      "flag\n",
      "30\n",
      "tensor(88.0333)\n",
      "Round  66, Average loss -0.073\n",
      "flag\n",
      "30\n",
      "tensor(87.2433)\n",
      "Round  67, Average loss -0.078\n",
      "flag\n",
      "30\n",
      "tensor(88.3500)\n",
      "Round  68, Average loss -0.071\n",
      "flag\n",
      "93\n",
      "tensor(87.8150)\n",
      "Round  69, Average loss -0.075\n",
      "flag\n",
      "75\n",
      "tensor(87.3317)\n",
      "Round  70, Average loss -0.078\n",
      "saved\n",
      "flag\n",
      "75\n",
      "tensor(86.2950)\n",
      "Round  71, Average loss -0.084\n",
      "flag\n",
      "85\n",
      "tensor(88.6117)\n",
      "Round  72, Average loss -0.069\n",
      "flag\n",
      "85\n",
      "tensor(86.1183)\n",
      "Round  73, Average loss -0.085\n",
      "flag\n",
      "95\n",
      "tensor(85.8917)\n",
      "Round  74, Average loss -0.087\n",
      "flag\n",
      "39\n",
      "tensor(87.3350)\n",
      "Round  75, Average loss -0.078\n",
      "flag\n",
      "41\n",
      "tensor(85.6600)\n",
      "Round  76, Average loss -0.088\n",
      "flag\n",
      "41\n",
      "tensor(85.7017)\n",
      "Round  77, Average loss -0.088\n",
      "flag\n",
      "17\n",
      "tensor(83.3283)\n",
      "Round  78, Average loss -0.103\n",
      "flag\n",
      "9\n",
      "tensor(83.4233)\n",
      "Round  79, Average loss -0.102\n",
      "flag\n",
      "9\n",
      "tensor(9.0667)\n",
      "Round  80, Average loss -0.464\n",
      "saved\n",
      "flag\n",
      "91\n",
      "tensor(46.6383)\n",
      "Round  81, Average loss -0.304\n",
      "flag\n",
      "95\n",
      "tensor(67.8033)\n",
      "Round  82, Average loss -0.194\n",
      "flag\n",
      "95\n",
      "tensor(76.7283)\n",
      "Round  83, Average loss -0.143\n",
      "flag\n",
      "95\n",
      "tensor(84.6400)\n",
      "Round  84, Average loss -0.095\n",
      "flag\n",
      "79\n",
      "tensor(89.0117)\n",
      "Round  85, Average loss -0.067\n",
      "flag\n",
      "17\n",
      "tensor(88.2400)\n",
      "Round  86, Average loss -0.072\n",
      "flag\n",
      "18\n",
      "tensor(87.0617)\n",
      "Round  87, Average loss -0.079\n",
      "flag\n",
      "85\n",
      "tensor(89.0600)\n",
      "Round  88, Average loss -0.067\n",
      "flag\n",
      "75\n",
      "tensor(89.7300)\n",
      "Round  89, Average loss -0.062\n",
      "flag\n",
      "95\n",
      "tensor(89.1250)\n",
      "Round  90, Average loss -0.066\n",
      "saved\n",
      "flag\n",
      "67\n",
      "tensor(90.5467)\n",
      "Round  91, Average loss -0.057\n",
      "flag\n",
      "67\n",
      "tensor(91.2067)\n",
      "Round  92, Average loss -0.053\n",
      "flag\n",
      "8\n",
      "tensor(91.3367)\n",
      "Round  93, Average loss -0.052\n",
      "flag\n",
      "67\n",
      "tensor(88.7867)\n",
      "Round  94, Average loss -0.068\n",
      "flag\n",
      "53\n",
      "tensor(90.5650)\n",
      "Round  95, Average loss -0.057\n",
      "flag\n",
      "76\n",
      "tensor(91.7350)\n",
      "Round  96, Average loss -0.049\n",
      "flag\n",
      "24\n",
      "tensor(89.6917)\n",
      "Round  97, Average loss -0.062\n",
      "flag\n",
      "60\n",
      "tensor(89.8650)\n",
      "Round  98, Average loss -0.061\n",
      "flag\n",
      "61\n",
      "tensor(87.7717)\n",
      "Round  99, Average loss -0.075\n",
      "flag\n",
      "84\n",
      "tensor(90.9750)\n",
      "Round 100, Average loss -0.054\n",
      "saved\n",
      "flag\n",
      "84\n",
      "tensor(90.9800)\n",
      "Round 101, Average loss -0.054\n",
      "flag\n",
      "96\n",
      "tensor(90.7417)\n",
      "Round 102, Average loss -0.056\n",
      "flag\n",
      "33\n",
      "tensor(88.5017)\n",
      "Round 103, Average loss -0.070\n",
      "flag\n",
      "42\n",
      "tensor(88.6267)\n",
      "Round 104, Average loss -0.069\n",
      "flag\n",
      "30\n",
      "tensor(88.2000)\n",
      "Round 105, Average loss -0.072\n",
      "flag\n",
      "62\n",
      "tensor(88.5133)\n",
      "Round 106, Average loss -0.070\n",
      "flag\n",
      "62\n",
      "tensor(85.8567)\n",
      "Round 107, Average loss -0.087\n",
      "flag\n",
      "84\n",
      "tensor(89.4883)\n",
      "Round 108, Average loss -0.064\n",
      "flag\n",
      "76\n",
      "tensor(89.7417)\n",
      "Round 109, Average loss -0.062\n",
      "flag\n",
      "76\n",
      "tensor(87.4333)\n",
      "Round 110, Average loss -0.077\n",
      "saved\n",
      "flag\n",
      "7\n",
      "tensor(87.6483)\n",
      "Round 111, Average loss -0.076\n",
      "flag\n",
      "29\n",
      "tensor(88.4433)\n",
      "Round 112, Average loss -0.071\n",
      "flag\n",
      "29\n",
      "tensor(87.1233)\n",
      "Round 113, Average loss -0.079\n",
      "flag\n",
      "29\n",
      "tensor(82.2500)\n",
      "Round 114, Average loss -0.110\n",
      "flag\n",
      "33\n",
      "tensor(87.3967)\n",
      "Round 115, Average loss -0.077\n",
      "flag\n",
      "33\n",
      "tensor(89.9367)\n",
      "Round 116, Average loss -0.061\n",
      "flag\n",
      "48\n",
      "tensor(90.3967)\n",
      "Round 117, Average loss -0.058\n",
      "flag\n",
      "48\n",
      "tensor(86.4283)\n",
      "Round 118, Average loss -0.083\n",
      "flag\n",
      "7\n",
      "tensor(87.3883)\n",
      "Round 119, Average loss -0.077\n",
      "flag\n",
      "7\n",
      "tensor(10.4917)\n",
      "Round 120, Average loss -0.459\n",
      "saved\n",
      "flag\n",
      "29\n",
      "tensor(52.7617)\n",
      "Round 121, Average loss -0.274\n",
      "flag\n",
      "92\n",
      "tensor(67.5500)\n",
      "Round 122, Average loss -0.196\n",
      "flag\n",
      "26\n",
      "tensor(73.2950)\n",
      "Round 123, Average loss -0.163\n",
      "flag\n",
      "29\n",
      "tensor(75.5700)\n",
      "Round 124, Average loss -0.150\n",
      "flag\n",
      "29\n",
      "tensor(76.2617)\n",
      "Round 125, Average loss -0.146\n",
      "flag\n",
      "29\n",
      "tensor(77.8683)\n",
      "Round 126, Average loss -0.136\n",
      "flag\n",
      "53\n",
      "tensor(79.6017)\n",
      "Round 127, Average loss -0.126\n",
      "flag\n",
      "95\n",
      "tensor(78.2300)\n",
      "Round 128, Average loss -0.134\n",
      "flag\n",
      "48\n",
      "tensor(77.4467)\n",
      "Round 129, Average loss -0.139\n",
      "flag\n",
      "48\n",
      "tensor(76.7833)\n",
      "Round 130, Average loss -0.143\n",
      "saved\n",
      "flag\n",
      "82\n",
      "tensor(79.7267)\n",
      "Round 131, Average loss -0.125\n",
      "flag\n",
      "48\n",
      "tensor(75.3033)\n",
      "Round 132, Average loss -0.151\n",
      "flag\n",
      "48\n",
      "tensor(80.7100)\n",
      "Round 133, Average loss -0.119\n",
      "flag\n",
      "78\n",
      "tensor(80.7583)\n",
      "Round 134, Average loss -0.119\n",
      "flag\n",
      "78\n",
      "tensor(81.8367)\n",
      "Round 135, Average loss -0.112\n",
      "flag\n",
      "29\n",
      "tensor(81.0183)\n",
      "Round 136, Average loss -0.117\n",
      "flag\n",
      "29\n",
      "tensor(74.1833)\n",
      "Round 137, Average loss -0.158\n",
      "flag\n",
      "29\n",
      "tensor(81.5333)\n",
      "Round 138, Average loss -0.114\n",
      "flag\n",
      "9\n",
      "tensor(82.7717)\n",
      "Round 139, Average loss -0.106\n",
      "flag\n",
      "53\n",
      "tensor(79.8083)\n",
      "Round 140, Average loss -0.125\n",
      "saved\n",
      "flag\n",
      "48\n",
      "tensor(79.2217)\n",
      "Round 141, Average loss -0.128\n",
      "flag\n",
      "48\n",
      "tensor(82.5150)\n",
      "Round 142, Average loss -0.108\n",
      "flag\n",
      "29\n",
      "tensor(75.1417)\n",
      "Round 143, Average loss -0.152\n",
      "flag\n",
      "25\n",
      "tensor(73.0983)\n",
      "Round 144, Average loss -0.164\n",
      "flag\n",
      "33\n",
      "tensor(78.4917)\n",
      "Round 145, Average loss -0.133\n",
      "flag\n",
      "29\n",
      "tensor(81.)\n",
      "Round 146, Average loss -0.117\n",
      "flag\n",
      "9\n",
      "tensor(81.0250)\n",
      "Round 147, Average loss -0.117\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag\n",
      "9\n",
      "tensor(80.6867)\n",
      "Round 148, Average loss -0.119\n",
      "flag\n",
      "48\n",
      "tensor(83.7817)\n",
      "Round 149, Average loss -0.100\n",
      "flag\n",
      "30\n",
      "tensor(80.5500)\n",
      "Round 150, Average loss -0.120\n",
      "saved\n",
      "flag\n",
      "30\n",
      "tensor(80.0700)\n",
      "Round 151, Average loss -0.123\n",
      "flag\n",
      "62\n",
      "tensor(79.1567)\n",
      "Round 152, Average loss -0.129\n",
      "flag\n",
      "0\n",
      "tensor(76.5300)\n",
      "Round 153, Average loss -0.144\n",
      "flag\n",
      "76\n",
      "tensor(73.1750)\n",
      "Round 154, Average loss -0.164\n",
      "flag\n",
      "50\n",
      "tensor(75.9667)\n",
      "Round 155, Average loss -0.148\n",
      "flag\n",
      "7\n",
      "tensor(71.5967)\n",
      "Round 156, Average loss -0.173\n",
      "flag\n",
      "7\n",
      "tensor(81.1183)\n",
      "Round 157, Average loss -0.117\n",
      "flag\n",
      "30\n",
      "tensor(79.2133)\n",
      "Round 158, Average loss -0.128\n",
      "flag\n",
      "59\n",
      "tensor(63.9217)\n",
      "Round 159, Average loss -0.216\n",
      "flag\n",
      "88\n",
      "tensor(9.7367)\n",
      "Round 160, Average loss -0.461\n",
      "saved\n",
      "flag\n",
      "88\n",
      "tensor(28.1217)\n",
      "Round 161, Average loss -0.388\n",
      "flag\n",
      "26\n",
      "tensor(80.4433)\n",
      "Round 162, Average loss -0.121\n",
      "flag\n",
      "26\n",
      "tensor(80.4750)\n",
      "Round 163, Average loss -0.121\n",
      "flag\n",
      "5\n",
      "tensor(83.0533)\n",
      "Round 164, Average loss -0.105\n",
      "flag\n",
      "5\n",
      "tensor(84.8650)\n",
      "Round 165, Average loss -0.093\n",
      "flag\n",
      "47\n",
      "tensor(87.1467)\n",
      "Round 166, Average loss -0.079\n",
      "flag\n",
      "3\n",
      "tensor(87.0383)\n",
      "Round 167, Average loss -0.080\n",
      "flag\n",
      "60\n",
      "tensor(87.8117)\n",
      "Round 168, Average loss -0.075\n",
      "flag\n",
      "97\n",
      "tensor(88.8567)\n",
      "Round 169, Average loss -0.068\n",
      "flag\n",
      "54\n",
      "tensor(87.2583)\n",
      "Round 170, Average loss -0.078\n",
      "saved\n",
      "flag\n",
      "29\n",
      "tensor(89.7967)\n",
      "Round 171, Average loss -0.062\n",
      "flag\n",
      "98\n",
      "tensor(89.1433)\n",
      "Round 172, Average loss -0.066\n",
      "flag\n",
      "41\n",
      "tensor(89.7950)\n",
      "Round 173, Average loss -0.062\n",
      "flag\n",
      "73\n",
      "tensor(88.4150)\n",
      "Round 174, Average loss -0.071\n",
      "flag\n",
      "54\n",
      "tensor(88.9933)\n",
      "Round 175, Average loss -0.067\n",
      "flag\n",
      "15\n",
      "tensor(87.0017)\n",
      "Round 176, Average loss -0.080\n",
      "flag\n",
      "15\n",
      "tensor(89.3817)\n",
      "Round 177, Average loss -0.064\n",
      "flag\n",
      "62\n",
      "tensor(87.6933)\n",
      "Round 178, Average loss -0.075\n",
      "flag\n",
      "62\n",
      "tensor(88.6600)\n",
      "Round 179, Average loss -0.069\n",
      "flag\n",
      "62\n",
      "tensor(87.6433)\n",
      "Round 180, Average loss -0.076\n",
      "saved\n",
      "flag\n",
      "3\n",
      "tensor(88.0333)\n",
      "Round 181, Average loss -0.073\n",
      "flag\n",
      "33\n",
      "tensor(87.3317)\n",
      "Round 182, Average loss -0.078\n",
      "flag\n",
      "83\n",
      "tensor(88.2433)\n",
      "Round 183, Average loss -0.072\n",
      "flag\n",
      "83\n",
      "tensor(88.2867)\n",
      "Round 184, Average loss -0.072\n",
      "flag\n",
      "83\n",
      "tensor(88.9117)\n",
      "Round 185, Average loss -0.068\n",
      "flag\n",
      "83\n",
      "tensor(89.8717)\n",
      "Round 186, Average loss -0.061\n",
      "flag\n",
      "89\n",
      "tensor(89.1033)\n",
      "Round 187, Average loss -0.066\n",
      "flag\n",
      "51\n",
      "tensor(87.7367)\n",
      "Round 188, Average loss -0.075\n",
      "flag\n",
      "41\n",
      "tensor(86.9050)\n",
      "Round 189, Average loss -0.080\n",
      "flag\n",
      "41\n",
      "tensor(87.2650)\n",
      "Round 190, Average loss -0.078\n",
      "saved\n",
      "flag\n",
      "31\n",
      "tensor(83.9650)\n",
      "Round 191, Average loss -0.099\n",
      "flag\n",
      "92\n",
      "tensor(85.2650)\n",
      "Round 192, Average loss -0.091\n",
      "flag\n",
      "48\n",
      "tensor(84.2483)\n",
      "Round 193, Average loss -0.097\n",
      "flag\n",
      "48\n",
      "tensor(74.3633)\n",
      "Round 194, Average loss -0.157\n",
      "flag\n",
      "83\n",
      "tensor(78.7283)\n",
      "Round 195, Average loss -0.131\n",
      "flag\n",
      "3\n",
      "tensor(87.0050)\n",
      "Round 196, Average loss -0.080\n",
      "flag\n",
      "48\n",
      "tensor(80.7517)\n",
      "Round 197, Average loss -0.119\n",
      "flag\n",
      "33\n",
      "tensor(75.5867)\n",
      "Round 198, Average loss -0.150\n",
      "flag\n",
      "92\n",
      "tensor(79.0550)\n",
      "Round 199, Average loss -0.129\n",
      "flag\n",
      "29\n",
      "tensor(12.8500)\n",
      "Round 200, Average loss -0.450\n",
      "saved\n",
      "flag\n",
      "53\n",
      "tensor(37.1650)\n",
      "Round 201, Average loss -0.349\n",
      "flag\n",
      "3\n",
      "tensor(66.0917)\n",
      "Round 202, Average loss -0.204\n",
      "flag\n",
      "83\n",
      "tensor(76.8967)\n",
      "Round 203, Average loss -0.142\n",
      "flag\n",
      "48\n",
      "tensor(82.7033)\n",
      "Round 204, Average loss -0.107\n",
      "flag\n",
      "52\n",
      "tensor(86.5533)\n",
      "Round 205, Average loss -0.083\n",
      "flag\n",
      "23\n",
      "tensor(87.6850)\n",
      "Round 206, Average loss -0.075\n",
      "flag\n",
      "7\n",
      "tensor(86.4433)\n",
      "Round 207, Average loss -0.083\n",
      "flag\n",
      "7\n",
      "tensor(88.2950)\n",
      "Round 208, Average loss -0.072\n",
      "flag\n",
      "7\n",
      "tensor(87.2967)\n",
      "Round 209, Average loss -0.078\n",
      "flag\n",
      "7\n",
      "tensor(90.7100)\n",
      "Round 210, Average loss -0.056\n",
      "saved\n",
      "flag\n",
      "3\n",
      "tensor(90.2933)\n",
      "Round 211, Average loss -0.059\n",
      "flag\n",
      "48\n",
      "tensor(91.3950)\n",
      "Round 212, Average loss -0.051\n",
      "flag\n",
      "53\n",
      "tensor(90.5733)\n",
      "Round 213, Average loss -0.057\n",
      "flag\n",
      "53\n",
      "tensor(89.5750)\n",
      "Round 214, Average loss -0.063\n",
      "flag\n",
      "29\n",
      "tensor(89.9383)\n",
      "Round 215, Average loss -0.061\n",
      "flag\n",
      "29\n",
      "tensor(88.9850)\n",
      "Round 216, Average loss -0.067\n",
      "flag\n",
      "29\n",
      "tensor(89.4883)\n",
      "Round 217, Average loss -0.064\n",
      "flag\n",
      "92\n",
      "tensor(90.4800)\n",
      "Round 218, Average loss -0.057\n",
      "flag\n",
      "7\n",
      "tensor(89.6033)\n",
      "Round 219, Average loss -0.063\n",
      "flag\n",
      "14\n",
      "tensor(88.9400)\n",
      "Round 220, Average loss -0.067\n",
      "saved\n",
      "flag\n",
      "5\n",
      "tensor(90.3750)\n",
      "Round 221, Average loss -0.058\n",
      "flag\n",
      "5\n",
      "tensor(89.5033)\n",
      "Round 222, Average loss -0.064\n",
      "flag\n",
      "96\n",
      "tensor(90.0500)\n",
      "Round 223, Average loss -0.060\n",
      "flag\n",
      "66\n",
      "tensor(88.3533)\n",
      "Round 224, Average loss -0.071\n",
      "flag\n",
      "28\n",
      "tensor(88.1100)\n",
      "Round 225, Average loss -0.073\n",
      "flag\n",
      "92\n",
      "tensor(89.9283)\n",
      "Round 226, Average loss -0.061\n",
      "flag\n",
      "48\n",
      "tensor(89.8967)\n",
      "Round 227, Average loss -0.061\n",
      "flag\n",
      "54\n",
      "tensor(88.1100)\n",
      "Round 228, Average loss -0.073\n",
      "flag\n",
      "29\n",
      "tensor(88.3700)\n",
      "Round 229, Average loss -0.071\n",
      "flag\n",
      "29\n",
      "tensor(88.0617)\n",
      "Round 230, Average loss -0.073\n",
      "saved\n",
      "flag\n",
      "29\n",
      "tensor(89.1050)\n",
      "Round 231, Average loss -0.066\n",
      "flag\n",
      "85\n",
      "tensor(87.7133)\n",
      "Round 232, Average loss -0.075\n",
      "flag\n",
      "28\n",
      "tensor(82.9900)\n",
      "Round 233, Average loss -0.105\n",
      "flag\n",
      "3\n",
      "tensor(88.8983)\n",
      "Round 234, Average loss -0.068\n",
      "flag\n",
      "48\n",
      "tensor(87.8600)\n",
      "Round 235, Average loss -0.074\n",
      "flag\n",
      "29\n",
      "tensor(81.8150)\n",
      "Round 236, Average loss -0.112\n",
      "flag\n",
      "19\n",
      "tensor(84.0983)\n",
      "Round 237, Average loss -0.098\n",
      "flag\n",
      "3\n",
      "tensor(83.6000)\n",
      "Round 238, Average loss -0.101\n",
      "flag\n",
      "3\n",
      "tensor(84.0567)\n",
      "Round 239, Average loss -0.098\n",
      "flag\n",
      "3\n",
      "tensor(11.2350)\n",
      "Round 240, Average loss -0.456\n",
      "saved\n",
      "flag\n",
      "1\n",
      "tensor(77.0333)\n",
      "Round 241, Average loss -0.141\n",
      "flag\n",
      "56\n",
      "tensor(83.9033)\n",
      "Round 242, Average loss -0.099\n",
      "flag\n",
      "3\n",
      "tensor(83.8917)\n",
      "Round 243, Average loss -0.099\n",
      "flag\n",
      "3\n",
      "tensor(83.6500)\n",
      "Round 244, Average loss -0.101\n",
      "flag\n",
      "28\n",
      "tensor(86.5150)\n",
      "Round 245, Average loss -0.083\n",
      "flag\n",
      "29\n",
      "tensor(87.5883)\n",
      "Round 246, Average loss -0.076\n",
      "flag\n",
      "7\n",
      "tensor(87.8250)\n",
      "Round 247, Average loss -0.075\n",
      "flag\n",
      "48\n",
      "tensor(89.0583)\n",
      "Round 248, Average loss -0.067\n",
      "flag\n",
      "29\n",
      "tensor(87.6483)\n",
      "Round 249, Average loss -0.076\n",
      "flag\n",
      "53\n",
      "tensor(86.9217)\n",
      "Round 250, Average loss -0.080\n",
      "saved\n",
      "flag\n",
      "7\n",
      "tensor(88.)\n",
      "Round 251, Average loss -0.073\n",
      "flag\n",
      "7\n",
      "tensor(87.4283)\n",
      "Round 252, Average loss -0.077\n",
      "flag\n",
      "28\n",
      "tensor(87.6067)\n",
      "Round 253, Average loss -0.076\n",
      "flag\n",
      "48\n",
      "tensor(89.7067)\n",
      "Round 254, Average loss -0.062\n",
      "flag\n",
      "3\n",
      "tensor(88.7433)\n",
      "Round 255, Average loss -0.069\n",
      "flag\n",
      "3\n",
      "tensor(89.7600)\n",
      "Round 256, Average loss -0.062\n",
      "flag\n",
      "3\n",
      "tensor(89.9367)\n",
      "Round 257, Average loss -0.061\n",
      "flag\n",
      "95\n",
      "tensor(85.6983)\n",
      "Round 258, Average loss -0.088\n",
      "flag\n",
      "1\n",
      "tensor(89.6467)\n",
      "Round 259, Average loss -0.063\n",
      "flag\n",
      "3\n",
      "tensor(88.5783)\n",
      "Round 260, Average loss -0.070\n",
      "saved\n",
      "flag\n",
      "3\n",
      "tensor(88.0300)\n",
      "Round 261, Average loss -0.073\n",
      "flag\n",
      "7\n",
      "tensor(86.5850)\n",
      "Round 262, Average loss -0.082\n",
      "flag\n",
      "7\n",
      "tensor(89.6383)\n",
      "Round 263, Average loss -0.063\n",
      "flag\n",
      "7\n",
      "tensor(86.0550)\n",
      "Round 264, Average loss -0.086\n",
      "flag\n",
      "7\n",
      "tensor(89.0183)\n",
      "Round 265, Average loss -0.067\n",
      "flag\n",
      "53\n",
      "tensor(84.1733)\n",
      "Round 266, Average loss -0.098\n",
      "flag\n",
      "53\n",
      "tensor(87.4117)\n",
      "Round 267, Average loss -0.077\n",
      "flag\n",
      "53\n",
      "tensor(88.4400)\n",
      "Round 268, Average loss -0.071\n",
      "flag\n",
      "53\n",
      "tensor(87.6500)\n",
      "Round 269, Average loss -0.076\n",
      "flag\n",
      "53\n",
      "tensor(86.5250)\n",
      "Round 270, Average loss -0.083\n",
      "saved\n",
      "flag\n",
      "3\n",
      "tensor(88.0800)\n",
      "Round 271, Average loss -0.073\n",
      "flag\n",
      "3\n",
      "tensor(87.0067)\n",
      "Round 272, Average loss -0.080\n",
      "flag\n",
      "7\n",
      "tensor(87.1967)\n",
      "Round 273, Average loss -0.079\n",
      "flag\n",
      "7\n",
      "tensor(88.8117)\n",
      "Round 274, Average loss -0.068\n",
      "flag\n",
      "7\n",
      "tensor(86.8167)\n",
      "Round 275, Average loss -0.081\n",
      "flag\n",
      "7\n",
      "tensor(89.2833)\n",
      "Round 276, Average loss -0.065\n",
      "flag\n",
      "53\n",
      "tensor(86.3200)\n",
      "Round 277, Average loss -0.084\n",
      "flag\n",
      "53\n",
      "tensor(87.3883)\n",
      "Round 278, Average loss -0.077\n",
      "flag\n",
      "29\n",
      "tensor(85.7117)\n",
      "Round 279, Average loss -0.088\n",
      "flag\n",
      "3\n",
      "tensor(3.6600)\n",
      "Round 280, Average loss -0.484\n",
      "saved\n",
      "flag\n",
      "7\n",
      "tensor(26.2167)\n",
      "Round 281, Average loss -0.396\n",
      "flag\n",
      "7\n",
      "tensor(68.8700)\n",
      "Round 282, Average loss -0.188\n",
      "flag\n",
      "7\n",
      "tensor(83.6767)\n",
      "Round 283, Average loss -0.101\n",
      "flag\n",
      "30\n",
      "tensor(88.7350)\n",
      "Round 284, Average loss -0.069\n",
      "flag\n",
      "7\n",
      "tensor(87.6117)\n",
      "Round 285, Average loss -0.076\n",
      "flag\n",
      "7\n",
      "tensor(89.3783)\n",
      "Round 286, Average loss -0.065\n",
      "flag\n",
      "7\n",
      "tensor(91.1000)\n",
      "Round 287, Average loss -0.053\n",
      "flag\n",
      "3\n",
      "tensor(88.4533)\n",
      "Round 288, Average loss -0.070\n",
      "flag\n",
      "3\n",
      "tensor(90.1550)\n",
      "Round 289, Average loss -0.059\n",
      "flag\n",
      "3\n",
      "tensor(91.5983)\n",
      "Round 290, Average loss -0.050\n",
      "saved\n",
      "flag\n",
      "9\n",
      "tensor(91.6950)\n",
      "Round 291, Average loss -0.049\n",
      "flag\n",
      "7\n",
      "tensor(88.9133)\n",
      "Round 292, Average loss -0.068\n",
      "flag\n",
      "7\n",
      "tensor(90.4200)\n",
      "Round 293, Average loss -0.058\n",
      "flag\n",
      "7\n",
      "tensor(90.1400)\n",
      "Round 294, Average loss -0.060\n",
      "flag\n",
      "3\n",
      "tensor(90.5050)\n",
      "Round 295, Average loss -0.057\n",
      "flag\n",
      "3\n",
      "tensor(88.4617)\n",
      "Round 296, Average loss -0.070\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag\n",
      "28\n",
      "tensor(88.7450)\n",
      "Round 297, Average loss -0.069\n",
      "flag\n",
      "3\n",
      "tensor(83.5617)\n",
      "Round 298, Average loss -0.101\n",
      "flag\n",
      "3\n",
      "tensor(88.5517)\n",
      "Round 299, Average loss -0.070\n",
      "flag\n",
      "29\n",
      "tensor(89.2167)\n",
      "Round 300, Average loss -0.066\n",
      "saved\n",
      "flag\n",
      "57\n",
      "tensor(80.1283)\n",
      "Round 301, Average loss -0.123\n",
      "flag\n",
      "7\n",
      "tensor(74.9333)\n",
      "Round 302, Average loss -0.154\n",
      "flag\n",
      "3\n",
      "tensor(87.8683)\n",
      "Round 303, Average loss -0.074\n",
      "flag\n",
      "3\n",
      "tensor(87.0233)\n",
      "Round 304, Average loss -0.080\n",
      "flag\n",
      "3\n",
      "tensor(88.5450)\n",
      "Round 305, Average loss -0.070\n",
      "flag\n",
      "3\n",
      "tensor(90.6183)\n",
      "Round 306, Average loss -0.056\n",
      "flag\n",
      "29\n",
      "tensor(88.9017)\n",
      "Round 307, Average loss -0.068\n",
      "flag\n",
      "29\n",
      "tensor(85.5100)\n",
      "Round 308, Average loss -0.089\n",
      "flag\n",
      "98\n",
      "tensor(76.5350)\n",
      "Round 309, Average loss -0.144\n",
      "flag\n",
      "30\n",
      "tensor(82.8733)\n",
      "Round 310, Average loss -0.106\n",
      "saved\n",
      "flag\n",
      "3\n",
      "tensor(74.5067)\n",
      "Round 311, Average loss -0.156\n",
      "flag\n",
      "3\n",
      "tensor(84.3467)\n",
      "Round 312, Average loss -0.097\n",
      "flag\n",
      "3\n",
      "tensor(89.4883)\n",
      "Round 313, Average loss -0.064\n",
      "flag\n",
      "3\n",
      "tensor(88.0650)\n",
      "Round 314, Average loss -0.073\n",
      "flag\n",
      "29\n",
      "tensor(87.0400)\n",
      "Round 315, Average loss -0.080\n",
      "flag\n",
      "7\n",
      "tensor(86.8450)\n",
      "Round 316, Average loss -0.081\n",
      "flag\n",
      "7\n",
      "tensor(80.7500)\n",
      "Round 317, Average loss -0.119\n",
      "flag\n",
      "3\n",
      "tensor(80.7883)\n",
      "Round 318, Average loss -0.119\n",
      "flag\n",
      "30\n",
      "tensor(82.8150)\n",
      "Round 319, Average loss -0.106\n",
      "flag\n",
      "30\n",
      "tensor(9.5317)\n",
      "Round 320, Average loss -0.462\n",
      "saved\n",
      "flag\n",
      "3\n",
      "tensor(72.5617)\n",
      "Round 321, Average loss -0.167\n",
      "flag\n",
      "3\n",
      "tensor(81.6367)\n",
      "Round 322, Average loss -0.113\n",
      "flag\n",
      "3\n",
      "tensor(88.0717)\n",
      "Round 323, Average loss -0.073\n",
      "flag\n",
      "48\n",
      "tensor(90.2050)\n",
      "Round 324, Average loss -0.059\n",
      "flag\n",
      "48\n",
      "tensor(89.9650)\n",
      "Round 325, Average loss -0.061\n",
      "flag\n",
      "3\n",
      "tensor(91.2300)\n",
      "Round 326, Average loss -0.052\n",
      "flag\n",
      "14\n",
      "tensor(90.4750)\n",
      "Round 327, Average loss -0.057\n",
      "flag\n",
      "3\n",
      "tensor(90.5633)\n",
      "Round 328, Average loss -0.057\n",
      "flag\n",
      "28\n",
      "tensor(90.9883)\n",
      "Round 329, Average loss -0.054\n",
      "flag\n",
      "3\n",
      "tensor(90.7283)\n",
      "Round 330, Average loss -0.056\n",
      "saved\n",
      "flag\n",
      "3\n",
      "tensor(92.1817)\n",
      "Round 331, Average loss -0.046\n",
      "flag\n",
      "1\n",
      "tensor(91.3433)\n",
      "Round 332, Average loss -0.052\n",
      "flag\n",
      "1\n",
      "tensor(90.6317)\n",
      "Round 333, Average loss -0.056\n",
      "flag\n",
      "20\n",
      "tensor(90.8917)\n",
      "Round 334, Average loss -0.055\n",
      "flag\n",
      "3\n",
      "tensor(91.7983)\n",
      "Round 335, Average loss -0.049\n",
      "flag\n",
      "91\n",
      "tensor(90.9600)\n",
      "Round 336, Average loss -0.054\n",
      "flag\n",
      "91\n",
      "tensor(89.9667)\n",
      "Round 337, Average loss -0.061\n",
      "flag\n",
      "7\n",
      "tensor(89.9667)\n",
      "Round 338, Average loss -0.061\n",
      "flag\n",
      "3\n",
      "tensor(85.5350)\n",
      "Round 339, Average loss -0.089\n",
      "flag\n",
      "48\n",
      "tensor(88.7350)\n",
      "Round 340, Average loss -0.069\n",
      "saved\n",
      "flag\n",
      "48\n",
      "tensor(90.4783)\n",
      "Round 341, Average loss -0.057\n",
      "flag\n",
      "14\n",
      "tensor(90.2367)\n",
      "Round 342, Average loss -0.059\n",
      "flag\n",
      "66\n",
      "tensor(87.0467)\n",
      "Round 343, Average loss -0.080\n",
      "flag\n",
      "80\n",
      "tensor(82.6833)\n",
      "Round 344, Average loss -0.107\n",
      "flag\n",
      "80\n",
      "tensor(84.2017)\n",
      "Round 345, Average loss -0.097\n",
      "flag\n",
      "30\n",
      "tensor(85.7583)\n",
      "Round 346, Average loss -0.088\n",
      "flag\n",
      "53\n",
      "tensor(86.7333)\n",
      "Round 347, Average loss -0.082\n",
      "flag\n",
      "53\n",
      "tensor(87.0067)\n",
      "Round 348, Average loss -0.080\n",
      "flag\n",
      "48\n",
      "tensor(84.6750)\n",
      "Round 349, Average loss -0.095\n",
      "flag\n",
      "1\n",
      "tensor(88.8900)\n",
      "Round 350, Average loss -0.068\n",
      "saved\n",
      "flag\n",
      "96\n",
      "tensor(75.5950)\n",
      "Round 351, Average loss -0.150\n",
      "flag\n",
      "96\n",
      "tensor(86.0150)\n",
      "Round 352, Average loss -0.086\n",
      "flag\n",
      "3\n",
      "tensor(85.8333)\n",
      "Round 353, Average loss -0.087\n",
      "flag\n",
      "3\n",
      "tensor(86.3550)\n",
      "Round 354, Average loss -0.084\n",
      "flag\n",
      "98\n",
      "tensor(82.2350)\n",
      "Round 355, Average loss -0.110\n",
      "flag\n",
      "48\n",
      "tensor(80.9350)\n",
      "Round 356, Average loss -0.118\n",
      "flag\n",
      "1\n",
      "tensor(75.6867)\n",
      "Round 357, Average loss -0.149\n",
      "flag\n",
      "20\n",
      "tensor(82.8733)\n",
      "Round 358, Average loss -0.106\n",
      "flag\n",
      "3\n",
      "tensor(80.7083)\n",
      "Round 359, Average loss -0.119\n",
      "flag\n",
      "7\n",
      "tensor(10.2183)\n",
      "Round 360, Average loss -0.460\n",
      "saved\n",
      "flag\n",
      "30\n",
      "tensor(37.9250)\n",
      "Round 361, Average loss -0.345\n",
      "flag\n",
      "30\n",
      "tensor(55.3517)\n",
      "Round 362, Average loss -0.261\n",
      "flag\n",
      "33\n",
      "tensor(65.6717)\n",
      "Round 363, Average loss -0.206\n",
      "flag\n",
      "7\n",
      "tensor(76.6167)\n",
      "Round 364, Average loss -0.144\n",
      "flag\n",
      "3\n",
      "tensor(76.9867)\n",
      "Round 365, Average loss -0.142\n",
      "flag\n",
      "46\n",
      "tensor(82.8767)\n",
      "Round 366, Average loss -0.106\n",
      "flag\n",
      "64\n",
      "tensor(79.1950)\n",
      "Round 367, Average loss -0.128\n",
      "flag\n",
      "64\n",
      "tensor(85.8600)\n",
      "Round 368, Average loss -0.087\n",
      "flag\n",
      "9\n",
      "tensor(87.4050)\n",
      "Round 369, Average loss -0.077\n",
      "flag\n",
      "7\n",
      "tensor(85.9567)\n",
      "Round 370, Average loss -0.086\n",
      "saved\n",
      "flag\n",
      "3\n",
      "tensor(87.3233)\n",
      "Round 371, Average loss -0.078\n",
      "flag\n",
      "14\n",
      "tensor(87.5950)\n",
      "Round 372, Average loss -0.076\n",
      "flag\n",
      "3\n",
      "tensor(87.8167)\n",
      "Round 373, Average loss -0.075\n",
      "flag\n",
      "3\n",
      "tensor(88.2550)\n",
      "Round 374, Average loss -0.072\n",
      "flag\n",
      "64\n",
      "tensor(89.0500)\n",
      "Round 375, Average loss -0.067\n",
      "flag\n",
      "64\n",
      "tensor(89.6450)\n",
      "Round 376, Average loss -0.063\n",
      "flag\n",
      "9\n",
      "tensor(89.4883)\n",
      "Round 377, Average loss -0.064\n",
      "flag\n",
      "30\n",
      "tensor(88.6167)\n",
      "Round 378, Average loss -0.069\n",
      "flag\n",
      "80\n",
      "tensor(88.3267)\n",
      "Round 379, Average loss -0.071\n",
      "flag\n",
      "48\n",
      "tensor(88.2017)\n",
      "Round 380, Average loss -0.072\n",
      "saved\n",
      "flag\n",
      "3\n",
      "tensor(88.3150)\n",
      "Round 381, Average loss -0.071\n",
      "flag\n",
      "41\n",
      "tensor(88.9800)\n",
      "Round 382, Average loss -0.067\n",
      "flag\n",
      "41\n",
      "tensor(85.0883)\n",
      "Round 383, Average loss -0.092\n",
      "flag\n",
      "41\n",
      "tensor(89.0967)\n",
      "Round 384, Average loss -0.066\n",
      "flag\n",
      "48\n",
      "tensor(86.6067)\n",
      "Round 385, Average loss -0.082\n",
      "flag\n",
      "80\n",
      "tensor(85.8900)\n",
      "Round 386, Average loss -0.087\n",
      "flag\n",
      "80\n",
      "tensor(82.9850)\n",
      "Round 387, Average loss -0.105\n",
      "flag\n",
      "3\n",
      "tensor(85.1550)\n",
      "Round 388, Average loss -0.092\n",
      "flag\n",
      "3\n",
      "tensor(82.8533)\n",
      "Round 389, Average loss -0.106\n",
      "flag\n",
      "48\n",
      "tensor(86.8133)\n",
      "Round 390, Average loss -0.081\n",
      "saved\n",
      "flag\n",
      "48\n",
      "tensor(88.4117)\n",
      "Round 391, Average loss -0.071\n",
      "flag\n",
      "64\n",
      "tensor(87.2533)\n",
      "Round 392, Average loss -0.078\n",
      "flag\n",
      "64\n",
      "tensor(88.1983)\n",
      "Round 393, Average loss -0.072\n",
      "flag\n",
      "53\n",
      "tensor(86.2467)\n",
      "Round 394, Average loss -0.085\n",
      "flag\n",
      "53\n",
      "tensor(87.1050)\n",
      "Round 395, Average loss -0.079\n",
      "flag\n",
      "48\n",
      "tensor(83.6800)\n",
      "Round 396, Average loss -0.101\n",
      "flag\n",
      "30\n",
      "tensor(87.7217)\n",
      "Round 397, Average loss -0.075\n",
      "flag\n",
      "30\n",
      "tensor(87.4417)\n",
      "Round 398, Average loss -0.077\n",
      "flag\n",
      "64\n",
      "tensor(85.3033)\n",
      "Round 399, Average loss -0.091\n",
      "flag\n",
      "64\n",
      "tensor(9.3850)\n",
      "Round 400, Average loss -0.463\n",
      "saved\n",
      "flag\n",
      "53\n",
      "tensor(35.8817)\n",
      "Round 401, Average loss -0.354\n",
      "flag\n",
      "30\n",
      "tensor(60.8733)\n",
      "Round 402, Average loss -0.232\n",
      "flag\n",
      "48\n",
      "tensor(74.4467)\n",
      "Round 403, Average loss -0.156\n",
      "flag\n",
      "92\n",
      "tensor(80.3117)\n",
      "Round 404, Average loss -0.121\n",
      "flag\n",
      "92\n",
      "tensor(76.9333)\n",
      "Round 405, Average loss -0.142\n",
      "flag\n",
      "3\n",
      "tensor(82.9650)\n",
      "Round 406, Average loss -0.105\n",
      "flag\n",
      "48\n",
      "tensor(83.4950)\n",
      "Round 407, Average loss -0.102\n",
      "flag\n",
      "48\n",
      "tensor(81.3483)\n",
      "Round 408, Average loss -0.115\n",
      "flag\n",
      "48\n",
      "tensor(85.6350)\n",
      "Round 409, Average loss -0.088\n",
      "flag\n",
      "48\n",
      "tensor(82.7200)\n",
      "Round 410, Average loss -0.107\n",
      "saved\n",
      "flag\n",
      "64\n",
      "tensor(80.7967)\n",
      "Round 411, Average loss -0.119\n",
      "flag\n",
      "64\n",
      "tensor(85.6067)\n",
      "Round 412, Average loss -0.089\n",
      "flag\n",
      "64\n",
      "tensor(87.8400)\n",
      "Round 413, Average loss -0.074\n",
      "flag\n",
      "64\n",
      "tensor(86.4967)\n",
      "Round 414, Average loss -0.083\n",
      "flag\n",
      "7\n",
      "tensor(86.9700)\n",
      "Round 415, Average loss -0.080\n",
      "flag\n",
      "30\n",
      "tensor(86.8983)\n",
      "Round 416, Average loss -0.080\n",
      "flag\n",
      "82\n",
      "tensor(86.8550)\n",
      "Round 417, Average loss -0.081\n",
      "flag\n",
      "14\n",
      "tensor(85.8217)\n",
      "Round 418, Average loss -0.087\n",
      "flag\n",
      "64\n",
      "tensor(86.8917)\n",
      "Round 419, Average loss -0.081\n",
      "flag\n",
      "48\n",
      "tensor(86.6517)\n",
      "Round 420, Average loss -0.082\n",
      "saved\n",
      "flag\n",
      "30\n",
      "tensor(86.4533)\n",
      "Round 421, Average loss -0.083\n",
      "flag\n",
      "30\n",
      "tensor(88.1733)\n",
      "Round 422, Average loss -0.072\n",
      "flag\n",
      "7\n",
      "tensor(85.8700)\n",
      "Round 423, Average loss -0.087\n",
      "flag\n",
      "11\n",
      "tensor(85.5467)\n",
      "Round 424, Average loss -0.089\n",
      "flag\n",
      "14\n",
      "tensor(86.3933)\n",
      "Round 425, Average loss -0.084\n",
      "flag\n",
      "64\n",
      "tensor(86.9717)\n",
      "Round 426, Average loss -0.080\n",
      "flag\n",
      "64\n",
      "tensor(83.0300)\n",
      "Round 427, Average loss -0.105\n",
      "flag\n",
      "7\n",
      "tensor(84.5333)\n",
      "Round 428, Average loss -0.095\n",
      "flag\n",
      "7\n",
      "tensor(81.1800)\n",
      "Round 429, Average loss -0.116\n",
      "flag\n",
      "30\n",
      "tensor(86.0817)\n",
      "Round 430, Average loss -0.086\n",
      "saved\n",
      "flag\n",
      "30\n",
      "tensor(87.1167)\n",
      "Round 431, Average loss -0.079\n",
      "flag\n",
      "30\n",
      "tensor(87.5617)\n",
      "Round 432, Average loss -0.076\n",
      "flag\n",
      "14\n",
      "tensor(85.6600)\n",
      "Round 433, Average loss -0.088\n",
      "flag\n",
      "14\n",
      "tensor(84.6650)\n",
      "Round 434, Average loss -0.095\n",
      "flag\n",
      "7\n",
      "tensor(86.9350)\n",
      "Round 435, Average loss -0.080\n",
      "flag\n",
      "7\n",
      "tensor(86.5433)\n",
      "Round 436, Average loss -0.083\n",
      "flag\n",
      "7\n",
      "tensor(86.4367)\n",
      "Round 437, Average loss -0.083\n",
      "flag\n",
      "64\n",
      "tensor(85.3433)\n",
      "Round 438, Average loss -0.090\n",
      "flag\n",
      "3\n",
      "tensor(86.7967)\n",
      "Round 439, Average loss -0.081\n",
      "flag\n",
      "30\n",
      "tensor(9.5750)\n",
      "Round 440, Average loss -0.462\n",
      "saved\n",
      "flag\n",
      "7\n",
      "tensor(13.3967)\n",
      "Round 441, Average loss -0.448\n",
      "flag\n",
      "96\n",
      "tensor(47.8633)\n",
      "Round 442, Average loss -0.298\n",
      "flag\n",
      "14\n",
      "tensor(62.3467)\n",
      "Round 443, Average loss -0.224\n",
      "flag\n",
      "1\n",
      "tensor(77.7933)\n",
      "Round 444, Average loss -0.137\n",
      "flag\n",
      "1\n",
      "tensor(79.9000)\n",
      "Round 445, Average loss -0.124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "flag\n",
      "7\n",
      "tensor(85.3550)\n",
      "Round 446, Average loss -0.090\n",
      "flag\n",
      "7\n",
      "tensor(87.5633)\n",
      "Round 447, Average loss -0.076\n",
      "flag\n",
      "30\n",
      "tensor(88.0233)\n",
      "Round 448, Average loss -0.073\n",
      "flag\n",
      "64\n",
      "tensor(87.)\n",
      "Round 449, Average loss -0.080\n",
      "flag\n",
      "64\n",
      "tensor(89.1533)\n",
      "Round 450, Average loss -0.066\n",
      "saved\n",
      "flag\n",
      "64\n",
      "tensor(88.9050)\n",
      "Round 451, Average loss -0.068\n",
      "flag\n",
      "64\n",
      "tensor(88.5567)\n",
      "Round 452, Average loss -0.070\n",
      "flag\n",
      "30\n",
      "tensor(87.7483)\n",
      "Round 453, Average loss -0.075\n",
      "flag\n",
      "7\n",
      "tensor(89.0483)\n",
      "Round 454, Average loss -0.067\n",
      "flag\n",
      "7\n",
      "tensor(87.4900)\n",
      "Round 455, Average loss -0.077\n",
      "flag\n",
      "64\n",
      "tensor(89.3883)\n",
      "Round 456, Average loss -0.064\n",
      "flag\n",
      "64\n",
      "tensor(90.0983)\n",
      "Round 457, Average loss -0.060\n",
      "flag\n",
      "64\n",
      "tensor(88.9900)\n",
      "Round 458, Average loss -0.067\n",
      "flag\n",
      "48\n",
      "tensor(90.1917)\n",
      "Round 459, Average loss -0.059\n",
      "flag\n",
      "7\n",
      "tensor(88.8450)\n",
      "Round 460, Average loss -0.068\n",
      "saved\n",
      "flag\n",
      "64\n",
      "tensor(89.2267)\n",
      "Round 461, Average loss -0.065\n",
      "flag\n",
      "64\n",
      "tensor(89.6250)\n",
      "Round 462, Average loss -0.063\n",
      "flag\n",
      "30\n",
      "tensor(89.7217)\n",
      "Round 463, Average loss -0.062\n",
      "flag\n",
      "30\n",
      "tensor(87.6583)\n",
      "Round 464, Average loss -0.076\n",
      "flag\n",
      "64\n",
      "tensor(84.1617)\n",
      "Round 465, Average loss -0.098\n",
      "flag\n",
      "64\n",
      "tensor(86.2833)\n",
      "Round 466, Average loss -0.084\n",
      "flag\n",
      "14\n",
      "tensor(90.0850)\n",
      "Round 467, Average loss -0.060\n",
      "flag\n",
      "14\n",
      "tensor(90.7217)\n",
      "Round 468, Average loss -0.056\n",
      "flag\n",
      "14\n",
      "tensor(88.4450)\n",
      "Round 469, Average loss -0.071\n",
      "flag\n",
      "30\n",
      "tensor(88.4633)\n",
      "Round 470, Average loss -0.070\n",
      "saved\n",
      "flag\n",
      "30\n",
      "tensor(84.8917)\n",
      "Round 471, Average loss -0.093\n",
      "flag\n",
      "64\n",
      "tensor(86.0383)\n",
      "Round 472, Average loss -0.086\n",
      "flag\n",
      "64\n",
      "tensor(89.6950)\n",
      "Round 473, Average loss -0.062\n",
      "flag\n",
      "96\n",
      "tensor(87.3900)\n",
      "Round 474, Average loss -0.077\n",
      "flag\n",
      "64\n",
      "tensor(80.7233)\n",
      "Round 475, Average loss -0.119\n",
      "flag\n",
      "64\n",
      "tensor(87.8817)\n",
      "Round 476, Average loss -0.074\n",
      "flag\n",
      "64\n",
      "tensor(86.9133)\n",
      "Round 477, Average loss -0.080\n",
      "flag\n",
      "64\n",
      "tensor(86.7233)\n",
      "Round 478, Average loss -0.082\n",
      "flag\n",
      "30\n",
      "tensor(72.6750)\n",
      "Round 479, Average loss -0.167\n",
      "flag\n",
      "64\n",
      "tensor(11.0867)\n",
      "Round 480, Average loss -0.456\n",
      "saved\n",
      "flag\n",
      "64\n",
      "tensor(9.9300)\n",
      "Round 481, Average loss -0.461\n",
      "flag\n",
      "64\n",
      "tensor(60.9900)\n",
      "Round 482, Average loss -0.232\n",
      "flag\n",
      "14\n",
      "tensor(80.0550)\n",
      "Round 483, Average loss -0.123\n",
      "flag\n",
      "14\n",
      "tensor(74.4367)\n",
      "Round 484, Average loss -0.157\n",
      "flag\n",
      "30\n",
      "tensor(80.0750)\n",
      "Round 485, Average loss -0.123\n",
      "flag\n",
      "64\n",
      "tensor(81.9217)\n",
      "Round 486, Average loss -0.112\n",
      "flag\n",
      "64\n",
      "tensor(84.0500)\n",
      "Round 487, Average loss -0.098\n",
      "flag\n",
      "48\n",
      "tensor(87.6183)\n",
      "Round 488, Average loss -0.076\n",
      "flag\n",
      "64\n",
      "tensor(86.2717)\n",
      "Round 489, Average loss -0.084\n",
      "flag\n",
      "7\n",
      "tensor(87.4817)\n",
      "Round 490, Average loss -0.077\n",
      "saved\n",
      "flag\n",
      "14\n",
      "tensor(87.4783)\n",
      "Round 491, Average loss -0.077\n",
      "flag\n",
      "14\n",
      "tensor(86.4033)\n",
      "Round 492, Average loss -0.084\n",
      "flag\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-3115270736ab>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m             \u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_glob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;31m#         if loss != 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m#             print('local loss:', loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenweilong/federated-learning/models/Update.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, net)\u001b[0m\n\u001b[1;32m     80\u001b[0m                             \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m                             \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 82\u001b[0;31m                             \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     83\u001b[0m                             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenweilong/federated-learning/models/Nets.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_pool2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2_drop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    438\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    439\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0;32m--> 440\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    441\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training rl\n",
    "loss_train = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "\n",
    "loss_save = []\n",
    "m = max(int(args.frac * args.num_users), 1)\n",
    "constant = 2\n",
    "target_acc = 0.99\n",
    "args.emb = False\n",
    "dqn = torch.load('dqn_softmax_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "# dqn = torch.load('dqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "\n",
    "last_replay_data = []\n",
    "for iter in range(args.epochs):\n",
    "    if iter==0:\n",
    "        random_n = 0\n",
    "        n_weight = []\n",
    "        while random_n<args.num_users*args.frac:\n",
    "            n_weight.append(random.random()) # 随机初始化参数\n",
    "            random_n+=1\n",
    "        action = random.randint(0,99)\n",
    "\n",
    "    #重置global参数    \n",
    "    if iter % args.reset_flag == 0:\n",
    "        if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "            net_glob = CNNCifar(args=args).to(args.device)\n",
    "        elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "            net_glob = CNNMnist(args=args).to(args.device)\n",
    "        elif args.model == 'mlp':\n",
    "            len_in = 1\n",
    "            for x in img_size:\n",
    "                len_in *= x\n",
    "            net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "        else:\n",
    "            exit('Error: unrecognized model')  \n",
    "        \n",
    "        \n",
    "    loss_locals = []\n",
    "    w_locals = []\n",
    "    p_emb_collect = []\n",
    "    for i in layer_name:\n",
    "        if i == 'conv1':\n",
    "            emb_global = layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "        else:\n",
    "            emb_global += layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "    p_emb_collect.append(emb_global)\n",
    "    \n",
    "    for idx in range(100):\n",
    "        if idx == action:\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx], flag=True)\n",
    "        else:\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "#         if loss != 0:\n",
    "#             print('local loss:', loss)\n",
    "        #把parameter转成嵌入\n",
    "        \n",
    "        ##########  求均值方式  #######\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "        \n",
    "        #分母对应local网络的层数\n",
    "        avg_emb_feature = emb_feature/4\n",
    "        \n",
    "        ##########  拼接方式  ##########\n",
    "#         for i in layer_name:\n",
    "#             if i == 'conv1':\n",
    "#                 cat_emb = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "#             else:\n",
    "#                 cat_emb = torch.cat([cat_emb, layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))], 1)\n",
    "                \n",
    "        ############ 储存嵌入 ##############\n",
    "        p_emb_collect.append(avg_emb_feature)\n",
    "        ############ 储存参数 ##############\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "    p_emb_collect = torch.cat(p_emb_collect,1).unsqueeze(0).to(args.device)\n",
    "#     print(p_emb_collect)\n",
    "    action_next = dqn.choose_action_train(p_emb_collect)\n",
    "    \n",
    "    ###########  计算当前轮的reward，然后将当前轮的reward添加到上一个replay_data中    \n",
    "    net_glob.eval()\n",
    "    global_acc, loss_train = test_img(net_glob, dataset_train, args)\n",
    "    print(global_acc)\n",
    "    reward = constant ** (global_acc.numpy()/100 - target_acc) - 1\n",
    "    loss_save.append(reward)\n",
    "    \n",
    "    if len(last_replay_data)==2:\n",
    "        last_replay_data.append(reward)#r\n",
    "        last_replay_data.append(p_emb_collect)#s_next\n",
    "        dqn.replay_buffer.add(last_replay_data[0], last_replay_data[1], last_replay_data[2], last_replay_data[3])\n",
    "    \n",
    "    last_replay_data = [p_emb_collect, action_next]#s, a\n",
    "    action = action_next\n",
    "    \n",
    "    # update global weights\n",
    "#     w_glob = FedAvg(w_locals)\n",
    "#     w_glob = FedPareto(w_locals, action_next)\n",
    "    # copy weight to net_glob\n",
    "    net_glob.train()\n",
    "    net_glob.load_state_dict(w_locals[action])\n",
    "    \n",
    "    \n",
    "    # print loss\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, reward))\n",
    "#     loss_train.append(-reward)\n",
    "    \n",
    "    if iter > 0:\n",
    "        dqn.optimize()\n",
    "    if iter % 10 == 0:\n",
    "#         torch.save(dqn, 'dqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "        torch.save(dqn, 'dqn_softmax_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "        print(\"saved\")\n",
    "#     args.lr = max(args.lr*args.lr_decay, 0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.13522605180997774, 0.13113093543615326, 0.10915445079580538, 0.10810005646881045, 0.09496720764244238, 0.09241013202788523, 0.08532814414827238, 0.08215351082241859, 0.08160765298880038, 0.0799218578594342]\n",
      "Round   0, Average loss 0.471\n",
      "[0.1464054071168125, 0.13509373123780333, 0.09731170691151551, 0.09617656799766947, 0.09419746065392928, 0.0920803910027495, 0.08722487106152513, 0.08684622894543227, 0.08573764459756913, 0.07892599047499388]\n",
      "Round   1, Average loss 0.458\n",
      "[0.14176247172602668, 0.1332077164888295, 0.10507772486789062, 0.09797671041836531, 0.09711169320654259, 0.09132140071519887, 0.08774828167748379, 0.08385428433021862, 0.08202286947047882, 0.07991684709896518]\n",
      "Round   2, Average loss 0.286\n",
      "[0.14067156882328952, 0.14019124827552715, 0.10368967915664644, 0.09522990955367919, 0.09257706727497184, 0.09171380095363302, 0.08663293515036292, 0.08497931038930742, 0.08299444885977235, 0.08132003156281015]\n",
      "Round   3, Average loss 0.114\n",
      "[0.1449761381488929, 0.13075162235441326, 0.10150867344624596, 0.09999852754576974, 0.09727599520240462, 0.09208649804078714, 0.08931043626213428, 0.08548177893156432, 0.08357297295654242, 0.07503735711124533]\n",
      "Round   4, Average loss 0.086\n",
      "[0.14679709215931036, 0.13845884105367104, 0.10513315141788286, 0.09901090076982513, 0.0908540601581644, 0.08932832492350519, 0.08839697277719441, 0.08791598546980124, 0.07940648259778267, 0.0746981886728627]\n",
      "Round   5, Average loss 0.069\n",
      "[0.14791238175741467, 0.13813202651315276, 0.10379035060655037, 0.10144737102142283, 0.09285201165060572, 0.09003537543621895, 0.0892421779512237, 0.08494769172168612, 0.0759660584376714, 0.07567455490405349]\n",
      "Round   6, Average loss 0.061\n",
      "[0.14766737733351584, 0.13987326204317135, 0.10041034504710881, 0.09466858719714631, 0.09289648763164747, 0.09182121011945289, 0.08766635992479997, 0.08503280040425824, 0.08306416599758322, 0.07689940430131588]\n",
      "Round   7, Average loss 0.051\n",
      "[0.14283583487882076, 0.13289437469342114, 0.10377500359684691, 0.10266374710238936, 0.09734594214715164, 0.09169840128298115, 0.088940380839059, 0.0855356005233547, 0.07741275853025986, 0.07689795640571548]\n",
      "Round   8, Average loss 0.050\n",
      "[0.1502052091889823, 0.13646434163050658, 0.10100474473771481, 0.0978410884585202, 0.09418139845903498, 0.09382676616208996, 0.08904549899375908, 0.08403627525464881, 0.07817007150833281, 0.07522460560641049]\n",
      "Round   9, Average loss 0.048\n"
     ]
    }
   ],
   "source": [
    "# fl\n",
    "#initialize\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob = CNNCifar(args=args).to(args.device)\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob = CNNMnist(args=args).to(args.device)\n",
    "elif args.model == 'mlp':\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "net_glob.train()\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "\n",
    "m = max(int(args.frac * args.num_users), 1)\n",
    "dqn = torch.load('dqn_softmax_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "constant = 2\n",
    "target_acc = 0.98\n",
    "for iter in range(args.validation_epochs):\n",
    "    if iter==0:\n",
    "        random_n = 0\n",
    "        n_weight = []\n",
    "        while random_n<args.num_users*args.frac:\n",
    "            n_weight.append(random.random()) # 随机初始化参数\n",
    "            random_n+=1\n",
    "        l = list(range(0,100))\n",
    "        action = random.sample(l, m)\n",
    "      \n",
    "    loss_locals = []\n",
    "    w_locals = []\n",
    "    p_emb_collect = []\n",
    "    choice = []\n",
    "    \n",
    "    for i in layer_name:\n",
    "        if i == 'conv1':\n",
    "            emb_global = layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "        else:\n",
    "            emb_global += layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "    p_emb_collect.append(emb_global)\n",
    "    \n",
    "    for idx in range(100):\n",
    "        if idx in choice:\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx], flag=True)\n",
    "        else:\n",
    "            local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        \n",
    "        #把parameter转成嵌入\n",
    "        \n",
    "        ##########  求均值方式  #######\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "        \n",
    "        #分母对应local网络的层数\n",
    "        avg_emb_feature = emb_feature/4\n",
    "        \n",
    "        \n",
    "        ##########  拼接方式  ##########\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                cat_emb = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "            else:\n",
    "                cat_emb = torch.cat([cat_emb, layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))], 1)\n",
    "                \n",
    "        ############ 储存嵌入 ##############\n",
    "        p_emb_collect.append(avg_emb_feature)\n",
    "        \n",
    "        ############ 储存参数 ##############\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        \n",
    "    p_emb_collect = torch.cat(p_emb_collect,1).unsqueeze(0).to(args.device)\n",
    "    choice, action_next = dqn.choose_action_run(p_emb_collect)\n",
    "    \n",
    "    ###########  计算当前轮的reward，然后将当前轮的reward添加到上一个replay_data中    \n",
    "#     net_glob.eval()\n",
    "    global_acc, loss_train = test_img(net_glob, dataset_train, args)\n",
    "    reward = constant ** (global_acc.numpy()/100 - target_acc) - 1\n",
    "\n",
    "    action = action_next\n",
    "    \n",
    "    # update global weights\n",
    "    #fedavg\n",
    "#     chosen_w = []\n",
    "#     for i in choice:\n",
    "#         chosen_w.append(w_locals[i])\n",
    "#     w_glob = FedAvg(chosen_w)\n",
    "    \n",
    "    #ours\n",
    "    w_glob = FedPareto(w_locals, action_next, choice)\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    # print loss\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, -reward))\n",
    "#     loss_train.append(-reward)\n",
    "\n",
    "#     args.lr = max(args.lr*args.lr_decay, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "def test_img(net_g, datatest, args):\n",
    "    net_g.eval()\n",
    "    # testing\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    data_loader = DataLoader(datatest, batch_size=args.bs)\n",
    "    l = len(data_loader)\n",
    "    for idx, (data, target) in enumerate(data_loader):\n",
    "        if args.gpu != -1:\n",
    "            data, target = data, target\n",
    "        log_probs = net_g(data)\n",
    "        # sum up batch loss\n",
    "        test_loss += F.cross_entropy(log_probs, target, reduction='sum').item()\n",
    "        # get the index of the max log-probability\n",
    "        y_pred = log_probs.data.max(1, keepdim=True)[1]\n",
    "        correct += y_pred.eq(target.data.view_as(y_pred)).long().sum()\n",
    "\n",
    "    test_loss /= len(data_loader.dataset)\n",
    "    accuracy = 100.00 * correct / len(data_loader.dataset)\n",
    "    if args.verbose:\n",
    "        print('\\nTest set: Average loss: {:.4f} \\nAccuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "            test_loss, correct, len(data_loader.dataset), accuracy))\n",
    "    return accuracy, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 91.24\n",
      "Testing accuracy: 92.02\n"
     ]
    }
   ],
   "source": [
    "# plot loss curve\n",
    "# plt.figure()\n",
    "# plt.plot(range(len(loss_train)), loss_train)\n",
    "# plt.ylabel('train_loss')\n",
    "# plt.show()\n",
    "# plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
    "\n",
    "# testing\n",
    "net_glob.eval()\n",
    "acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function matplotlib.pyplot.show(*args, **kw)>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(len(loss_save)), loss_save)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
