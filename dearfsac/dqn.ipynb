{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNNMnist(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torchvision/datasets/mnist.py:52: UserWarning: train_labels has been renamed targets\n",
      "  warnings.warn(\"train_labels has been renamed targets\")\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# Python version: 3.6\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "from torchvision import datasets, transforms\n",
    "from collections import deque\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from utils.sampling import mnist_iid, mnist_noniid, cifar_iid, mnist_iid_drl_local_divide, cifar_iid_drl_local_divide, mnist_noniid_drl_local_divide\n",
    "from models.Update import LocalUpdate\n",
    "from models.Update_divide import LocalUpdate_divide\n",
    "from models.Nets import MLP, CNNMnist, CNNCifar, CNNCifarEmb, CNNCifarEmbReverse, CNNMnistEmb, CNNMnistEmbReverse\n",
    "from models.Fed import FedAvg, FedPareto\n",
    "from models.args import args_parser\n",
    "# from models.test import test_img\n",
    "\n",
    "# parse args\n",
    "args = args_parser()\n",
    "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
    "\n",
    "# load dataset and split users\n",
    "if args.dataset == 'mnist':\n",
    "    trans_mnist = transforms.Compose([transforms.ToTensor(), transforms.Lambda(lambda x: x.repeat(1,1,1))])\n",
    "    dataset_train = datasets.MNIST('./data/mnist/', train=True, download=False, transform=trans_mnist)\n",
    "    dataset_test = datasets.MNIST('./data/mnist/', train=False, download=True, transform=trans_mnist)\n",
    "    # sample users\n",
    "    args.iid = False\n",
    "    if args.iid:\n",
    "        dict_users = mnist_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        dict_users = mnist_noniid(dataset_train, args.num_users)\n",
    "elif args.dataset == 'cifar':\n",
    "    trans_cifar = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "    dataset_train = datasets.CIFAR10('./data/cifar', train=True, download=True, transform=trans_cifar)\n",
    "    dataset_test = datasets.CIFAR10('./data/cifar', train=False, download=True, transform=trans_cifar)\n",
    "    args.iid = True\n",
    "    if args.iid:\n",
    "        dict_users = cifar_iid(dataset_train, args.num_users)\n",
    "    else:\n",
    "        exit('Error: only consider IID setting in CIFAR10')\n",
    "else:\n",
    "    exit('Error: unrecognized dataset')\n",
    "img_size = dataset_train[0][0].shape\n",
    "\n",
    "# build model\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob = CNNCifar(args=args).to(args.device)\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob = CNNMnist(args=args).to(args.device)\n",
    "elif args.model == 'mlp':\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "print(net_glob)\n",
    "net_glob.train()\n",
    "\n",
    "# copy weights\n",
    "w_glob = net_glob.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum(x.numel() for x in net_glob.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.DQN import DQN\n",
    "from models.buffer import MemoryBuffer\n",
    "from models.prioritized_buffer import PrioritizedBuffer\n",
    "from models.test import test_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10100\n"
     ]
    }
   ],
   "source": [
    "#每个local的每一层在emb之后拼接起来，再乘以(100+1)，或者用均值。分别对应是400和100\n",
    "parameter_dim = (args.num_users+1) * 100\n",
    "action_dim = args.num_users\n",
    "print(parameter_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = MemoryBuffer(500)\n",
    "# replay_buffer = PrioritizedBuffer(100)\n",
    "dqn = DQN(parameter_dim, action_dim, replay_buffer, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/torch/nn/functional.py:718: UserWarning: Named tensors and all their associated APIs are an experimental feature and subject to change. Please do not use them for anything important until they are released as stable. (Triggered internally at  /pytorch/c10/core/TensorImpl.h:1156.)\n",
      "  return torch.max_pool2d(input, kernel_size, stride, padding, dilation, ceil_mode)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss_avg:0.15005645155906677\n",
      "epoch:0, loss_avg:0.4022198021411896\n",
      "epoch:0, loss_avg:0.3488524258136749\n",
      "epoch:0, loss_avg:1.0450294017791748\n",
      "epoch:0, loss_avg:0.2595255970954895\n",
      "epoch:0, loss_avg:0.14902356266975403\n",
      "epoch:0, loss_avg:0.11621211469173431\n",
      "epoch:0, loss_avg:0.09890931099653244\n",
      "epoch:0, loss_avg:0.09907639771699905\n",
      "epoch:0, loss_avg:0.4713357090950012\n",
      "epoch:0, loss_avg:0.34891873598098755\n",
      "epoch:0, loss_avg:0.1029767170548439\n",
      "epoch:0, loss_avg:0.09997280687093735\n",
      "epoch:0, loss_avg:0.11612743884325027\n",
      "epoch:0, loss_avg:0.06369136273860931\n",
      "epoch:0, loss_avg:0.07751049846410751\n",
      "epoch:0, loss_avg:0.08532143384218216\n",
      "epoch:0, loss_avg:0.06297529488801956\n",
      "epoch:0, loss_avg:0.1431894302368164\n",
      "epoch:0, loss_avg:0.18858301639556885\n",
      "epoch:0, loss_avg:0.04950165003538132\n",
      "epoch:0, loss_avg:0.07047183811664581\n",
      "epoch:0, loss_avg:0.04150144010782242\n",
      "epoch:0, loss_avg:0.04543745145201683\n",
      "epoch:0, loss_avg:0.07671115547418594\n",
      "epoch:0, loss_avg:0.06391266733407974\n",
      "epoch:0, loss_avg:0.046754851937294006\n",
      "epoch:0, loss_avg:0.04080687463283539\n",
      "epoch:0, loss_avg:0.18121522665023804\n",
      "epoch:0, loss_avg:0.05686694383621216\n",
      "epoch:0, loss_avg:0.04702944681048393\n",
      "epoch:0, loss_avg:0.6955300569534302\n",
      "epoch:0, loss_avg:0.05450696498155594\n",
      "epoch:0, loss_avg:0.09091290831565857\n",
      "epoch:0, loss_avg:0.07022562623023987\n",
      "epoch:0, loss_avg:0.06380993127822876\n",
      "epoch:0, loss_avg:0.08109157532453537\n",
      "epoch:0, loss_avg:0.05896793678402901\n",
      "epoch:0, loss_avg:0.07210533320903778\n",
      "epoch:0, loss_avg:0.06455822288990021\n",
      "epoch:0, loss_avg:0.0708540678024292\n",
      "epoch:0, loss_avg:0.05144600570201874\n",
      "epoch:0, loss_avg:0.0886370837688446\n",
      "epoch:0, loss_avg:0.05436454340815544\n",
      "epoch:0, loss_avg:0.045560382306575775\n",
      "epoch:0, loss_avg:0.04226643592119217\n",
      "epoch:0, loss_avg:0.07057323306798935\n",
      "epoch:0, loss_avg:0.05973043292760849\n",
      "epoch:0, loss_avg:0.049222707748413086\n",
      "epoch:0, loss_avg:0.09449760615825653\n",
      "epoch:0, loss_avg:0.07194087654352188\n",
      "epoch:0, loss_avg:0.03691106289625168\n",
      "epoch:0, loss_avg:0.059885911643505096\n",
      "epoch:0, loss_avg:0.0811799168586731\n",
      "epoch:0, loss_avg:0.04219725728034973\n",
      "epoch:0, loss_avg:0.07346434891223907\n",
      "epoch:0, loss_avg:0.06894625723361969\n",
      "epoch:0, loss_avg:0.07284751534461975\n",
      "epoch:0, loss_avg:0.06056540831923485\n",
      "epoch:0, loss_avg:0.04645182192325592\n",
      "epoch:0, loss_avg:0.056449584662914276\n",
      "epoch:0, loss_avg:0.04597613215446472\n",
      "epoch:0, loss_avg:0.048256874084472656\n",
      "len:16000,w:tensor([1.8398, 1.3383, 0.3599,  ..., 0.0646, 0.4954, 0.0245],\n",
      "       grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(3.1173, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:50,w:tensor([5.1118e-03, 1.1390e-05, 6.3761e-02, 6.4613e-02, 2.4902e+00, 4.2493e-01,\n",
      "        1.8758e-01, 1.2442e+00, 1.6370e+00, 1.6341e-01, 1.8931e+00, 6.2430e-02,\n",
      "        2.3061e-05, 1.1576e+00, 2.8072e-01, 1.4414e+00, 6.2388e-01, 1.5282e+00,\n",
      "        1.1947e-01, 1.0421e+00, 1.0320e-01, 2.4370e-01, 9.7541e-02, 1.5417e-01,\n",
      "        5.8911e-01, 4.5417e-01, 7.0545e-01, 2.3960e-01, 1.5930e-01, 5.6660e-01,\n",
      "        3.0185e-02, 3.8384e-01, 8.4508e-02, 2.4157e-01, 2.4467e-01, 1.0527e-03,\n",
      "        2.1727e-02, 8.4020e-01, 4.3551e-01, 5.6223e-01, 4.3432e-01, 4.9317e-01,\n",
      "        1.2885e-01, 3.4576e-02, 2.4967e-02, 4.2986e-01, 2.4202e-01, 2.3000e-01,\n",
      "        1.5266e+00, 1.4862e-01], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(3.6029, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:500,w:tensor([4.0409e-01, 6.2817e-02, 1.9603e-01, 4.1345e-04, 2.8517e-01, 1.2297e+00,\n",
      "        8.7739e-01, 2.0597e+00, 1.6764e+00, 5.6541e-01, 5.4547e-01, 3.5085e-01,\n",
      "        4.3471e-02, 9.8967e-04, 1.6872e-01, 1.0177e+00, 1.9702e-05, 1.3314e-02,\n",
      "        3.4622e-03, 4.1452e-03, 1.6929e-01, 9.3953e-01, 2.7882e-01, 1.2594e+00,\n",
      "        3.1173e-01, 1.9319e+00, 4.7888e-01, 1.0774e+00, 3.5985e-02, 1.0568e-03,\n",
      "        7.1823e-01, 1.0010e-02, 4.1568e-01, 2.4264e-02, 5.2694e-01, 5.2563e-03,\n",
      "        2.9806e-01, 1.2234e-01, 8.9266e-02, 1.4889e+00, 1.5167e+00, 5.6740e-01,\n",
      "        8.2244e-01, 1.6678e-01, 2.0288e+00, 1.7502e-03, 4.6892e+00, 1.2311e-01,\n",
      "        1.5361e-02, 1.0001e+00, 6.7544e-04, 4.3257e+00, 1.4400e+00, 1.7287e+00,\n",
      "        3.4453e-01, 7.0564e-01, 2.1698e+00, 1.1382e+00, 9.4643e-01, 5.2779e-01,\n",
      "        3.5851e-01, 2.1183e-02, 2.1658e-02, 2.1539e-02, 2.1192e-01, 4.5428e-03,\n",
      "        1.4764e-01, 1.3092e+00, 5.7010e-01, 1.7381e-03, 2.4870e+00, 7.1340e-02,\n",
      "        1.7284e-01, 2.3134e+00, 3.2185e-01, 1.2051e+00, 2.7575e-01, 1.6374e-01,\n",
      "        2.4612e-01, 2.2859e-01, 1.0085e+00, 1.7433e-01, 2.6039e-01, 5.9342e-02,\n",
      "        2.0433e-01, 3.8085e+00, 2.3759e+00, 2.0989e+00, 2.5011e+00, 1.9685e-01,\n",
      "        4.2390e-01, 1.1640e+00, 7.7863e-02, 6.3984e-01, 3.7700e+00, 5.5934e-01,\n",
      "        1.7479e+00, 1.6175e+00, 9.1673e-01, 2.3349e-02, 6.7715e-01, 1.0819e-01,\n",
      "        5.9786e-03, 3.4209e-01, 9.4362e-01, 1.5072e-01, 1.5608e-02, 1.2420e-01,\n",
      "        1.8677e-01, 4.9036e-01, 4.4473e-02, 2.2883e-01, 3.5913e-01, 1.1152e+00,\n",
      "        5.0133e-01, 5.8295e-02, 1.7594e-01, 1.0985e-01, 1.8483e-01, 1.5208e+00,\n",
      "        1.1440e-03, 5.4930e-02, 1.6342e-02, 3.2604e-01, 4.6575e+00, 8.3202e-02,\n",
      "        6.4151e-01, 8.1008e-01, 1.4320e+00, 1.5710e+00, 1.0591e-01, 4.2496e-02,\n",
      "        6.7934e-01, 1.7352e-01, 1.9135e-01, 7.9688e-02, 4.6311e-01, 4.5250e-01,\n",
      "        1.3361e-01, 1.3429e-01, 9.4039e-03, 5.7928e-01, 2.8422e-01, 3.9784e-01,\n",
      "        1.9722e+00, 4.4354e-04, 7.0304e-01, 3.8801e-01, 4.5279e-02, 6.2249e-01,\n",
      "        7.6492e-03, 2.1902e-01, 5.5194e-01, 4.3049e-01, 1.1815e+00, 5.9923e-02,\n",
      "        2.3580e-01, 2.7549e-01, 4.0918e-01, 1.4997e-02, 4.6433e-04, 5.1778e-01,\n",
      "        1.6708e-01, 6.1007e-01, 8.9410e-02, 8.7831e-02, 8.0519e-02, 3.6069e-02,\n",
      "        3.1900e-01, 4.2081e-02, 1.2719e+00, 1.4350e+00, 5.2801e-01, 1.1615e-01,\n",
      "        7.7458e-02, 1.6501e-02, 2.3128e-01, 8.8777e-03, 6.5589e-01, 1.7929e-01,\n",
      "        1.8950e-02, 1.4377e+00, 1.5937e+00, 1.3547e-01, 1.8896e-01, 3.6579e-01,\n",
      "        1.3632e-01, 2.1739e-02, 2.5277e-02, 5.8016e-05, 1.3377e-01, 2.2586e-01,\n",
      "        6.8066e-02, 3.9412e-01, 1.9799e+00, 2.0042e-01, 9.0077e-01, 2.0280e-01,\n",
      "        3.9217e-01, 8.0574e-01, 1.5201e-01, 1.7403e+00, 1.5176e+00, 5.6102e-01,\n",
      "        3.1276e-01, 2.9787e-01, 4.7751e-01, 2.1757e-04, 1.4060e-01, 3.1299e-04,\n",
      "        3.1093e-01, 8.6120e-02, 5.6497e-01, 9.6611e-01, 1.0337e+00, 4.7130e-04,\n",
      "        9.2474e-01, 1.5300e-02, 3.4824e-01, 3.8430e-01, 2.4551e+00, 2.2141e+00,\n",
      "        4.0624e-01, 5.2719e-02, 2.8285e-02, 6.8261e-04, 6.3609e-01, 7.1132e-03,\n",
      "        1.4976e-01, 8.4420e-01, 4.0703e-01, 3.1059e-01, 3.6402e-01, 7.0116e-01,\n",
      "        1.4207e-01, 4.8379e-01, 4.4322e-01, 5.1567e-03, 6.1261e-03, 2.0241e+00,\n",
      "        1.5334e-01, 2.7173e-01, 1.1217e+00, 8.1471e-02, 1.5999e+00, 3.0634e-01,\n",
      "        1.4554e+00, 1.7177e+00, 1.3737e+00, 2.6742e-02, 8.5906e-02, 1.5800e-01,\n",
      "        3.2385e-01, 5.4877e-01, 1.2816e-03, 5.9510e-03, 2.0392e-01, 3.3061e-01,\n",
      "        1.3222e+00, 3.5285e-01, 1.2164e+00, 7.0533e-03, 3.4457e-02, 5.6131e-01,\n",
      "        8.8891e-01, 3.2402e-02, 4.4825e-02, 2.0224e+00, 1.4915e+00, 2.0395e+00,\n",
      "        8.8494e-03, 8.1122e-01, 9.4143e-02, 2.6133e-01, 9.8891e-01, 4.9664e-03,\n",
      "        1.0924e+00, 2.5576e-02, 1.6733e-01, 1.2772e+00, 3.5798e-02, 4.4082e-02,\n",
      "        4.0460e-01, 7.6066e-01, 6.5070e-03, 1.2542e-01, 5.8172e-01, 5.6941e-01,\n",
      "        1.5807e+00, 1.5754e+00, 8.1668e-01, 7.6377e-01, 1.0179e+00, 2.2709e-04,\n",
      "        4.5008e-01, 1.0259e+00, 7.3793e-03, 2.2328e+00, 2.3022e-02, 3.5424e+00,\n",
      "        8.3770e-01, 3.9955e-02, 1.3010e+00, 1.0884e+00, 4.9856e-01, 1.2390e+00,\n",
      "        1.5859e-01, 1.4648e+00, 3.2590e-01, 2.2571e-01, 5.2983e-01, 1.4345e-01,\n",
      "        5.2739e-02, 1.5786e+00, 1.6474e-02, 2.9662e-01, 2.8519e-01, 2.4210e-02,\n",
      "        3.0404e-02, 6.0488e-01, 3.3353e-01, 2.8434e-01, 2.0733e-01, 8.2870e-01,\n",
      "        6.5665e-03, 2.4415e-01, 1.0017e-03, 2.2150e-01, 1.2679e-01, 6.2763e-02,\n",
      "        6.7382e-01, 6.4291e-01, 1.5557e-01, 1.8391e+00, 2.4981e+00, 2.4031e-05,\n",
      "        3.8806e-01, 4.9094e-02, 6.8791e-01, 1.3715e-02, 2.3429e-01, 6.5241e-01,\n",
      "        3.2929e-01, 1.3259e+00, 5.2193e-02, 4.2960e-01, 2.3195e-01, 4.4143e-02,\n",
      "        5.2013e-01, 2.6050e-01, 3.3851e-01, 2.0595e-01, 1.7679e+00, 7.7446e-01,\n",
      "        6.7859e-02, 3.5399e-04, 9.6807e-02, 4.1639e-01, 4.5297e-03, 7.5698e-01,\n",
      "        1.0001e-01, 1.0674e+00, 3.0601e-01, 2.2070e-02, 1.4049e-03, 1.6480e-01,\n",
      "        1.1014e-02, 9.8641e-03, 3.3848e-01, 6.6223e-03, 8.6152e-01, 3.7029e-02,\n",
      "        7.9920e-04, 8.7592e-01, 6.0385e-01, 2.0352e-02, 1.3670e+00, 4.1848e-01,\n",
      "        4.1704e-02, 7.8395e-01, 2.2564e-01, 1.1740e-01, 3.3825e-01, 1.4115e+00,\n",
      "        1.8861e-01, 4.1052e-01, 2.0443e+00, 1.5196e-03, 1.4595e-01, 3.8340e-02,\n",
      "        7.7414e-02, 8.0185e-02, 7.2793e-01, 2.1835e-01, 1.0261e-02, 3.7495e-01,\n",
      "        2.2905e+00, 2.8956e-02, 2.2318e-02, 2.2595e-01, 3.0768e-01, 5.9662e-02,\n",
      "        6.2475e-01, 1.6013e-01, 7.4380e-01, 3.5582e-01, 1.6010e+00, 2.4123e-02,\n",
      "        2.8016e-01, 3.2225e-01, 1.7501e-01, 3.2915e-01, 4.6212e-01, 7.2494e-01,\n",
      "        1.4222e+00, 8.6894e-01, 2.6657e+00, 2.2526e-02, 2.1911e+00, 2.9259e-02,\n",
      "        8.6207e-01, 4.9307e-02, 1.2218e-03, 7.8428e-01, 1.0682e+00, 1.6180e+00,\n",
      "        6.0858e-02, 6.4256e-04, 1.6096e-01, 1.1868e-01, 5.7474e-02, 1.3642e+00,\n",
      "        3.4442e-03, 7.1410e-06, 1.2915e-01, 2.6826e-01, 9.8068e-01, 2.5493e-01,\n",
      "        1.6665e-01, 1.3247e-01, 4.4252e-01, 3.2188e-01, 1.2680e+00, 1.3284e+00,\n",
      "        1.0573e+00, 1.3445e+00, 3.9531e-01, 6.8705e-03, 5.3596e-05, 1.7255e-03,\n",
      "        1.5986e-01, 9.1270e-01, 6.7002e-01, 9.2297e-03, 4.3083e-02, 2.5875e-02,\n",
      "        2.8102e-04, 1.4293e-01, 6.6243e-02, 1.9871e-01, 5.2552e-01, 2.8184e-01,\n",
      "        1.6802e-02, 5.9609e-02, 1.3480e-03, 8.8254e-01, 5.6612e-01, 1.4434e+00,\n",
      "        3.8589e-04, 3.8393e-01, 3.8070e-02, 1.4741e-02, 6.8544e-01, 2.0991e-01,\n",
      "        1.5244e+00, 1.2338e+00, 3.7410e-01, 2.5437e-01, 2.1610e-02, 4.6401e-02,\n",
      "        4.7559e-01, 1.0803e+00, 1.0266e-01, 9.7203e-01, 2.2943e-01, 3.4706e-01,\n",
      "        3.6300e-01, 4.1914e-01, 2.3843e-01, 6.7359e-01, 2.5959e-04, 1.1307e+00,\n",
      "        5.0250e-01, 1.2980e+00, 6.5747e-01, 1.2220e-02, 7.3885e-01, 8.5010e-01,\n",
      "        2.5700e+00, 2.2245e-01], grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(4.1602, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n",
      "len:10,w:tensor([9.7574e-01, 7.0943e-01, 2.4894e-01, 9.5729e-03, 2.9337e-04, 1.8824e-01,\n",
      "        2.0599e+00, 1.8452e-01, 1.5955e-01, 3.0723e-01],\n",
      "       grad_fn=<AddBackward0>)\n",
      "loss_sum: tensor(4.6445, grad_fn=<AddBackward0>)\n",
      "\n",
      "***************\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss_avg:4.644525051116943\n",
      "epoch:0, loss_avg:0.04336868226528168\n",
      "epoch:0, loss_avg:0.060976579785346985\n",
      "epoch:0, loss_avg:0.06987003982067108\n",
      "epoch:0, loss_avg:0.058429867029190063\n",
      "epoch:0, loss_avg:0.04810609668493271\n",
      "epoch:0, loss_avg:0.0905868262052536\n",
      "epoch:0, loss_avg:0.06640742719173431\n",
      "epoch:0, loss_avg:0.08427976816892624\n",
      "epoch:0, loss_avg:0.07063858211040497\n",
      "epoch:0, loss_avg:0.09314839541912079\n",
      "epoch:0, loss_avg:0.08681177347898483\n",
      "epoch:0, loss_avg:0.06721099466085434\n",
      "epoch:0, loss_avg:0.051178231835365295\n",
      "epoch:0, loss_avg:0.05336005985736847\n",
      "epoch:0, loss_avg:0.0630878284573555\n",
      "epoch:0, loss_avg:0.048411283642053604\n",
      "epoch:0, loss_avg:0.055836305022239685\n",
      "epoch:0, loss_avg:0.04145362228155136\n",
      "epoch:0, loss_avg:0.18913817405700684\n",
      "epoch:0, loss_avg:0.05807746946811676\n",
      "epoch:0, loss_avg:0.0625775009393692\n",
      "epoch:0, loss_avg:0.043869126588106155\n",
      "epoch:0, loss_avg:0.0521882064640522\n",
      "epoch:0, loss_avg:0.08007463812828064\n",
      "epoch:0, loss_avg:0.0765778049826622\n",
      "epoch:0, loss_avg:0.062087539583444595\n",
      "epoch:0, loss_avg:0.03853235021233559\n",
      "epoch:0, loss_avg:0.04951990395784378\n",
      "epoch:0, loss_avg:0.06914609670639038\n",
      "epoch:0, loss_avg:0.06562605500221252\n",
      "epoch:0, loss_avg:0.034885063767433167\n",
      "epoch:0, loss_avg:0.06449802219867706\n",
      "epoch:0, loss_avg:0.04237211495637894\n",
      "epoch:0, loss_avg:0.05273803323507309\n",
      "epoch:0, loss_avg:0.051382627338171005\n",
      "epoch:0, loss_avg:0.039142195135354996\n",
      "epoch:1, loss_avg:0.03837645798921585\n",
      "epoch:1, loss_avg:0.06521931290626526\n",
      "epoch:1, loss_avg:0.05284550040960312\n",
      "epoch:1, loss_avg:0.0992661789059639\n",
      "epoch:1, loss_avg:0.043314673006534576\n",
      "epoch:1, loss_avg:0.06683830171823502\n",
      "epoch:1, loss_avg:0.04509880393743515\n",
      "epoch:1, loss_avg:0.03950507193803787\n",
      "epoch:1, loss_avg:0.06235282123088837\n",
      "epoch:1, loss_avg:0.03766044229269028\n",
      "epoch:1, loss_avg:0.0455913171172142\n",
      "epoch:1, loss_avg:0.04950261861085892\n",
      "epoch:1, loss_avg:0.08001729846000671\n",
      "epoch:1, loss_avg:0.04883454740047455\n",
      "epoch:1, loss_avg:0.06704667210578918\n",
      "epoch:1, loss_avg:0.07817630469799042\n",
      "epoch:1, loss_avg:0.056235093623399734\n",
      "epoch:1, loss_avg:0.06547119468450546\n",
      "epoch:1, loss_avg:0.051366426050662994\n",
      "epoch:1, loss_avg:0.09312157332897186\n",
      "epoch:1, loss_avg:0.06406102329492569\n",
      "epoch:1, loss_avg:0.07972951233386993\n",
      "epoch:1, loss_avg:0.07612716406583786\n",
      "epoch:1, loss_avg:0.05872253328561783\n",
      "epoch:1, loss_avg:0.04103195294737816\n",
      "epoch:1, loss_avg:0.05259884521365166\n",
      "epoch:1, loss_avg:0.05720936506986618\n",
      "epoch:1, loss_avg:0.05375321954488754\n",
      "epoch:1, loss_avg:0.05693765729665756\n",
      "epoch:1, loss_avg:0.0666903629899025\n",
      "epoch:1, loss_avg:0.05559356510639191\n",
      "epoch:1, loss_avg:0.07575223594903946\n",
      "epoch:1, loss_avg:0.05038507282733917\n",
      "epoch:1, loss_avg:0.0592707097530365\n",
      "epoch:1, loss_avg:0.07361723482608795\n",
      "epoch:1, loss_avg:0.04887592792510986\n",
      "epoch:1, loss_avg:0.05354531109333038\n",
      "epoch:1, loss_avg:0.04615997523069382\n",
      "epoch:1, loss_avg:0.06640226393938065\n",
      "epoch:1, loss_avg:0.05810992419719696\n",
      "epoch:1, loss_avg:0.03703498840332031\n",
      "epoch:1, loss_avg:0.06966961920261383\n",
      "epoch:1, loss_avg:0.05841827020049095\n",
      "epoch:1, loss_avg:0.04593924060463905\n",
      "epoch:1, loss_avg:0.05501990020275116\n",
      "epoch:1, loss_avg:0.05875152349472046\n",
      "epoch:1, loss_avg:0.09511975198984146\n",
      "epoch:1, loss_avg:0.04857899993658066\n",
      "epoch:1, loss_avg:0.06183836609125137\n",
      "epoch:1, loss_avg:0.054317060858011246\n",
      "epoch:1, loss_avg:0.047550320625305176\n",
      "epoch:1, loss_avg:0.07108557224273682\n",
      "epoch:1, loss_avg:0.04508141428232193\n",
      "epoch:1, loss_avg:0.045029833912849426\n",
      "epoch:1, loss_avg:0.03540228679776192\n",
      "epoch:1, loss_avg:0.05912933498620987\n",
      "epoch:1, loss_avg:0.06456463783979416\n",
      "epoch:1, loss_avg:0.036193251609802246\n",
      "epoch:1, loss_avg:0.06008947268128395\n",
      "epoch:1, loss_avg:0.05544804781675339\n",
      "epoch:1, loss_avg:0.04568047821521759\n",
      "epoch:1, loss_avg:0.06822855025529861\n",
      "epoch:1, loss_avg:0.057927485555410385\n",
      "epoch:1, loss_avg:0.07377892732620239\n",
      "epoch:1, loss_avg:0.05444812774658203\n",
      "epoch:1, loss_avg:0.05279766023159027\n",
      "epoch:1, loss_avg:0.05752711370587349\n",
      "epoch:1, loss_avg:0.04309786111116409\n",
      "epoch:1, loss_avg:0.04087536036968231\n",
      "epoch:1, loss_avg:0.056524112820625305\n",
      "epoch:1, loss_avg:0.07113280892372131\n",
      "epoch:1, loss_avg:0.044556550681591034\n",
      "epoch:1, loss_avg:0.07918554544448853\n",
      "epoch:1, loss_avg:0.04558134824037552\n",
      "epoch:1, loss_avg:0.06310433149337769\n",
      "epoch:1, loss_avg:0.0835607573390007\n",
      "epoch:1, loss_avg:0.04213347285985947\n",
      "epoch:1, loss_avg:0.05327022448182106\n",
      "epoch:1, loss_avg:0.05282081663608551\n",
      "epoch:1, loss_avg:0.06522073596715927\n",
      "epoch:1, loss_avg:0.044626764953136444\n",
      "epoch:1, loss_avg:0.09497452527284622\n",
      "epoch:1, loss_avg:0.07035188376903534\n",
      "epoch:1, loss_avg:0.05442439019680023\n",
      "epoch:1, loss_avg:0.05236972123384476\n",
      "epoch:1, loss_avg:0.06144401803612709\n",
      "epoch:1, loss_avg:0.0671173632144928\n",
      "epoch:1, loss_avg:0.05966435372829437\n",
      "epoch:1, loss_avg:0.06691040098667145\n",
      "epoch:1, loss_avg:0.05460697412490845\n",
      "epoch:1, loss_avg:0.05155545473098755\n",
      "epoch:1, loss_avg:0.062060385942459106\n",
      "epoch:1, loss_avg:0.05677875503897667\n",
      "epoch:1, loss_avg:0.08348222076892853\n",
      "epoch:1, loss_avg:0.055600330233573914\n",
      "epoch:1, loss_avg:0.048178862780332565\n",
      "epoch:1, loss_avg:0.09670679271221161\n",
      "epoch:1, loss_avg:0.057698383927345276\n",
      "epoch:1, loss_avg:0.0451880544424057\n",
      "epoch:1, loss_avg:0.05059082806110382\n",
      "epoch:2, loss_avg:0.039689451456069946\n",
      "epoch:2, loss_avg:0.05022059381008148\n",
      "epoch:2, loss_avg:0.061176858842372894\n",
      "epoch:2, loss_avg:0.05785004049539566\n",
      "epoch:2, loss_avg:0.042110517621040344\n",
      "epoch:2, loss_avg:0.07787438482046127\n",
      "epoch:2, loss_avg:0.04442758485674858\n",
      "epoch:2, loss_avg:0.05324636399745941\n",
      "epoch:2, loss_avg:0.04107135161757469\n",
      "epoch:2, loss_avg:0.04209015518426895\n",
      "epoch:2, loss_avg:0.057475022971630096\n",
      "epoch:2, loss_avg:0.047742657363414764\n",
      "epoch:2, loss_avg:0.05725816637277603\n",
      "epoch:2, loss_avg:0.07118310779333115\n",
      "epoch:2, loss_avg:0.046747904270887375\n",
      "epoch:2, loss_avg:2.310421943664551\n",
      "epoch:2, loss_avg:0.05939376354217529\n",
      "epoch:2, loss_avg:0.13954520225524902\n",
      "epoch:2, loss_avg:0.11244875192642212\n",
      "epoch:2, loss_avg:0.15163204073905945\n",
      "epoch:2, loss_avg:0.11612828820943832\n",
      "epoch:2, loss_avg:0.0885520875453949\n",
      "epoch:2, loss_avg:0.0742122009396553\n",
      "epoch:2, loss_avg:0.055320855230093\n",
      "epoch:2, loss_avg:0.061117276549339294\n",
      "epoch:2, loss_avg:0.13042610883712769\n",
      "epoch:2, loss_avg:0.09935946762561798\n",
      "epoch:2, loss_avg:0.10565904527902603\n",
      "epoch:2, loss_avg:0.07890482246875763\n",
      "epoch:2, loss_avg:0.06522933393716812\n",
      "epoch:2, loss_avg:0.052121471613645554\n",
      "epoch:2, loss_avg:0.04632945731282234\n",
      "epoch:2, loss_avg:0.0701819583773613\n",
      "epoch:2, loss_avg:0.06352182477712631\n",
      "epoch:2, loss_avg:0.09286142885684967\n",
      "epoch:2, loss_avg:0.06927885115146637\n",
      "epoch:2, loss_avg:0.07488782703876495\n",
      "epoch:2, loss_avg:0.05165763944387436\n",
      "epoch:2, loss_avg:0.06494078785181046\n",
      "epoch:2, loss_avg:0.07275223731994629\n",
      "epoch:2, loss_avg:0.08358189463615417\n",
      "epoch:2, loss_avg:0.07326085865497589\n",
      "epoch:2, loss_avg:0.06291036307811737\n",
      "epoch:2, loss_avg:0.05558369308710098\n",
      "epoch:2, loss_avg:0.06333805620670319\n",
      "epoch:2, loss_avg:0.058909595012664795\n",
      "epoch:2, loss_avg:0.05563943088054657\n",
      "epoch:2, loss_avg:0.08044195920228958\n",
      "epoch:2, loss_avg:0.03912777826189995\n",
      "epoch:2, loss_avg:0.052791155874729156\n",
      "epoch:2, loss_avg:0.10037103295326233\n",
      "epoch:2, loss_avg:0.05196252837777138\n",
      "epoch:2, loss_avg:0.0668897032737732\n",
      "epoch:2, loss_avg:0.07741834968328476\n",
      "epoch:2, loss_avg:0.07531487196683884\n",
      "epoch:2, loss_avg:0.05579480528831482\n",
      "epoch:2, loss_avg:0.05647401139140129\n",
      "epoch:2, loss_avg:0.05577393248677254\n",
      "epoch:2, loss_avg:0.047339387238025665\n",
      "epoch:2, loss_avg:0.05932331830263138\n",
      "epoch:2, loss_avg:0.05810560658574104\n",
      "epoch:2, loss_avg:0.04447786882519722\n",
      "epoch:2, loss_avg:0.0659240186214447\n",
      "epoch:2, loss_avg:0.04350326582789421\n",
      "epoch:2, loss_avg:0.04933685436844826\n",
      "epoch:2, loss_avg:0.07132428884506226\n",
      "epoch:2, loss_avg:0.06348694115877151\n",
      "epoch:2, loss_avg:0.05199115723371506\n",
      "epoch:2, loss_avg:0.061537016183137894\n",
      "epoch:2, loss_avg:0.05844990909099579\n",
      "epoch:2, loss_avg:0.0621684268116951\n",
      "epoch:2, loss_avg:0.04816873371601105\n",
      "epoch:2, loss_avg:0.06302463263273239\n",
      "epoch:2, loss_avg:0.055049829185009\n",
      "epoch:2, loss_avg:0.05028282478451729\n",
      "epoch:2, loss_avg:0.0697992742061615\n",
      "epoch:2, loss_avg:0.09448660165071487\n",
      "epoch:2, loss_avg:0.05735068768262863\n",
      "epoch:2, loss_avg:0.06428765505552292\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:2, loss_avg:0.053771037608385086\n",
      "epoch:2, loss_avg:0.04329332709312439\n",
      "epoch:2, loss_avg:0.03486855328083038\n",
      "epoch:2, loss_avg:0.06222553923726082\n",
      "epoch:2, loss_avg:0.05538228899240494\n",
      "epoch:2, loss_avg:0.053843121975660324\n",
      "epoch:2, loss_avg:0.07116333395242691\n",
      "epoch:2, loss_avg:0.05350669473409653\n",
      "epoch:2, loss_avg:0.04448404163122177\n",
      "epoch:2, loss_avg:0.04406748339533806\n",
      "epoch:2, loss_avg:0.07138630002737045\n",
      "epoch:2, loss_avg:0.06176557019352913\n",
      "epoch:2, loss_avg:0.047291770577430725\n",
      "epoch:2, loss_avg:0.05378703027963638\n",
      "epoch:2, loss_avg:0.040329791605472565\n",
      "epoch:2, loss_avg:0.07213686406612396\n",
      "epoch:2, loss_avg:0.04487380012869835\n",
      "epoch:2, loss_avg:0.04402980953454971\n",
      "epoch:2, loss_avg:0.047428593039512634\n",
      "epoch:2, loss_avg:0.050896983593702316\n",
      "epoch:2, loss_avg:0.04606828838586807\n"
     ]
    }
   ],
   "source": [
    "layer_dict = {}\n",
    "layer_name = []\n",
    "count = 0\n",
    "for name in w_glob.keys():\n",
    "    if count % 2 == 0:\n",
    "        layer_name.append(name.split('.',1)[0])\n",
    "    count += 1\n",
    "    \n",
    "for i in layer_name:\n",
    "#     layer_dict[i] = CNNCifarEmb(torch.cat([w_glob[i+'.weight'].reshape(1,-1), w_glob[i+'.bias'].reshape(1,-1)], 1).numel())\n",
    "    layer_dict[i] = CNNMnistEmb(torch.cat([w_glob[i+'.weight'].reshape(1,-1), w_glob[i+'.bias'].reshape(1,-1)], 1).numel())\n",
    "    \n",
    "# emb_reverse = CNNCifarEmbReverse(args)\n",
    "emb_reverse = CNNMnistEmbReverse(args)\n",
    "\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params':layer_dict[layer_name[0]].parameters()},\n",
    "    {'params':layer_dict[layer_name[1]].parameters()},\n",
    "    {'params':layer_dict[layer_name[2]].parameters()},\n",
    "    {'params':layer_dict[layer_name[3]].parameters()},\n",
    "#     {'params':layer_dict[layer_name[4]].parameters()},\n",
    "    {'params':emb_reverse.parameters()}\n",
    "] ,0.01)\n",
    "\n",
    "w_save = []\n",
    "for iter in range(args.emb_train_epochs):\n",
    "    idxs_users = np.random.choice(range(args.num_users), 100, replace=False)\n",
    "    \n",
    "    for idx in idxs_users:\n",
    "#         local = LocalUpdate_divide(args=args, dataset=dataset_train, idxs=dict_users[0][idx])\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "#         w, loss, loss_list = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        if iter == 0:\n",
    "            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "            w_save.append(w)\n",
    "        if iter > 0:\n",
    "#             net_self = CNNMnist(args=args).to(args.device)\n",
    "#             net_self.load_state_dict(w_save[idx])\n",
    "            w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "#             w_save[idx] = w\n",
    "            \n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1).to(args.device))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1).to(args.device))\n",
    "        avg_emb_feature = emb_feature/4\n",
    "        transform_w = emb_reverse.forward(avg_emb_feature)\n",
    "        loss_w = [sum((w[i].reshape(1,-1) - transform_w[i].reshape(1,-1)) ** 2) for i in w_glob.keys()]\n",
    "        loss_avg = 0\n",
    "        loss_check_dict = {}\n",
    "        for i in range(len(loss_w)):\n",
    "            loss_avg += sum(loss_w[i])/len(loss_w[i])\n",
    "            loss_check_dict[len(loss_w[i])] = loss_w[i]\n",
    "            if loss_avg.item() > 3.0:\n",
    "                print('len:{},w:{}'.format(len(loss_w[i]), loss_check_dict[len(loss_w[i])]))\n",
    "                print('loss_sum:', loss_avg)\n",
    "                print('\\n***************')\n",
    "        optimizer.zero_grad()\n",
    "        loss_avg.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        print('epoch:{}, loss_avg:{}'.format(iter, loss_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Round   0, Average loss -0.976\n",
      "saved\n",
      "Round   1, Average loss -0.972\n",
      "Round   2, Average loss -0.902\n",
      "Round   3, Average loss -0.664\n",
      "Round   4, Average loss -0.485\n",
      "Round   5, Average loss -0.420\n",
      "Round   6, Average loss -0.388\n",
      "Round   7, Average loss -0.351\n",
      "Round   8, Average loss -0.331\n",
      "Round   9, Average loss -0.295\n",
      "Round  10, Average loss -0.283\n",
      "Round  11, Average loss -0.273\n",
      "Round  12, Average loss -0.266\n",
      "Round  13, Average loss -0.252\n",
      "Round  14, Average loss -0.246\n",
      "Round  15, Average loss -0.240\n",
      "Round  16, Average loss -0.237\n",
      "Round  17, Average loss -0.228\n",
      "Round  18, Average loss -0.219\n",
      "Round  19, Average loss -0.222\n",
      "Round  20, Average loss -0.215\n",
      "Round  21, Average loss -0.221\n",
      "Round  22, Average loss -0.221\n",
      "Round  23, Average loss -0.221\n",
      "Round  24, Average loss -0.223\n",
      "Round  25, Average loss -0.227\n",
      "Round  26, Average loss -0.230\n",
      "Round  27, Average loss -0.222\n",
      "Round  28, Average loss -0.229\n",
      "Round  29, Average loss -0.237\n",
      "Round  30, Average loss -0.218\n",
      "Round  31, Average loss -0.244\n",
      "Round  32, Average loss -0.236\n",
      "Round  33, Average loss -0.226\n",
      "Round  34, Average loss -0.216\n",
      "Round  35, Average loss -0.238\n",
      "Round  36, Average loss -0.237\n",
      "Round  37, Average loss -0.247\n",
      "Round  38, Average loss -0.246\n",
      "Round  39, Average loss -0.252\n",
      "Round  40, Average loss -0.238\n",
      "loss tensor(77.2785, grad_fn=<MeanBackward0>)\n",
      "loss tensor(217009.8750, grad_fn=<MeanBackward0>)\n",
      "loss tensor(15790.5898, grad_fn=<MeanBackward0>)\n",
      "loss tensor(43772.4805, grad_fn=<MeanBackward0>)\n",
      "loss tensor(77593.7734, grad_fn=<MeanBackward0>)\n",
      "Round  41, Average loss -0.254\n",
      "Round  42, Average loss -0.236\n",
      "Round  43, Average loss -0.247\n",
      "Round  44, Average loss -0.249\n",
      "Round  45, Average loss -0.242\n",
      "Round  46, Average loss -0.238\n",
      "Round  47, Average loss -0.233\n",
      "Round  48, Average loss -0.257\n",
      "Round  49, Average loss -0.236\n",
      "Round  50, Average loss -0.254\n",
      "Round  51, Average loss -0.255\n",
      "Round  52, Average loss -0.256\n",
      "Round  53, Average loss -0.271\n",
      "Round  54, Average loss -0.284\n",
      "Round  55, Average loss -0.261\n",
      "Round  56, Average loss -0.287\n",
      "Round  57, Average loss -0.292\n",
      "Round  58, Average loss -0.285\n",
      "Round  59, Average loss -0.270\n",
      "Round  60, Average loss -0.306\n",
      "Round  61, Average loss -0.293\n",
      "Round  62, Average loss -0.296\n",
      "Round  63, Average loss -0.312\n",
      "Round  64, Average loss -0.312\n",
      "Round  65, Average loss -0.341\n",
      "Round  66, Average loss -0.315\n",
      "Round  67, Average loss -0.348\n",
      "Round  68, Average loss -0.292\n",
      "Round  69, Average loss -0.359\n",
      "Round  70, Average loss -0.337\n",
      "Round  71, Average loss -0.337\n",
      "Round  72, Average loss -0.324\n",
      "Round  73, Average loss -0.336\n",
      "Round  74, Average loss -0.341\n",
      "Round  75, Average loss -0.336\n",
      "Round  76, Average loss -0.371\n",
      "Round  77, Average loss -0.351\n",
      "Round  78, Average loss -0.347\n",
      "Round  79, Average loss -0.330\n",
      "Round  80, Average loss -0.332\n",
      "loss tensor(410597.8750, grad_fn=<MeanBackward0>)\n",
      "loss tensor(139333.3281, grad_fn=<MeanBackward0>)\n",
      "loss tensor(35739.8438, grad_fn=<MeanBackward0>)\n",
      "loss tensor(301696.6875, grad_fn=<MeanBackward0>)\n",
      "loss tensor(575808.1250, grad_fn=<MeanBackward0>)\n",
      "Round  81, Average loss -0.337\n",
      "Round  82, Average loss -0.348\n",
      "Round  83, Average loss -0.393\n",
      "Round  84, Average loss -0.363\n",
      "Round  85, Average loss -0.342\n",
      "Round  86, Average loss -0.365\n",
      "Round  87, Average loss -0.352\n",
      "Round  88, Average loss -0.358\n",
      "Round  89, Average loss -0.363\n",
      "Round  90, Average loss -0.385\n",
      "Round  91, Average loss -0.381\n",
      "Round  92, Average loss -0.397\n",
      "Round  93, Average loss -0.365\n",
      "Round  94, Average loss -0.383\n",
      "Round  95, Average loss -0.381\n",
      "Round  96, Average loss -0.396\n",
      "Round  97, Average loss -0.364\n",
      "Round  98, Average loss -0.366\n",
      "Round  99, Average loss -0.386\n",
      "Round 100, Average loss -0.386\n",
      "Round 101, Average loss -0.412\n",
      "Round 102, Average loss -0.392\n",
      "Round 103, Average loss -0.414\n",
      "Round 104, Average loss -0.397\n",
      "Round 105, Average loss -0.394\n",
      "Round 106, Average loss -0.401\n",
      "Round 107, Average loss -0.407\n",
      "Round 108, Average loss -0.379\n",
      "Round 109, Average loss -0.437\n",
      "Round 110, Average loss -0.400\n",
      "Round 111, Average loss -0.419\n",
      "Round 112, Average loss -0.428\n",
      "Round 113, Average loss -0.378\n",
      "Round 114, Average loss -0.447\n",
      "Round 115, Average loss -0.428\n",
      "Round 116, Average loss -0.440\n",
      "Round 117, Average loss -0.433\n",
      "Round 118, Average loss -0.412\n",
      "Round 119, Average loss -0.471\n",
      "Round 120, Average loss -0.456\n",
      "loss tensor(86753.0391, grad_fn=<MeanBackward0>)\n",
      "loss tensor(36392.0078, grad_fn=<MeanBackward0>)\n",
      "loss tensor(56523.5000, grad_fn=<MeanBackward0>)\n",
      "loss tensor(186489.2031, grad_fn=<MeanBackward0>)\n",
      "loss tensor(228375.5469, grad_fn=<MeanBackward0>)\n",
      "Round 121, Average loss -0.460\n",
      "Round 122, Average loss -0.460\n",
      "Round 123, Average loss -0.446\n",
      "Round 124, Average loss -0.456\n",
      "Round 125, Average loss -0.498\n",
      "Round 126, Average loss -0.457\n",
      "Round 127, Average loss -0.455\n",
      "Round 128, Average loss -0.492\n",
      "Round 129, Average loss -0.489\n",
      "Round 130, Average loss -0.493\n",
      "Round 131, Average loss -0.505\n",
      "Round 132, Average loss -0.516\n",
      "Round 133, Average loss -0.533\n",
      "Round 134, Average loss -0.484\n",
      "Round 135, Average loss -0.501\n",
      "Round 136, Average loss -0.465\n",
      "Round 137, Average loss -0.511\n",
      "Round 138, Average loss -0.482\n",
      "Round 139, Average loss -0.557\n",
      "Round 140, Average loss -0.482\n",
      "Round 141, Average loss -0.500\n",
      "Round 142, Average loss -0.504\n",
      "Round 143, Average loss -0.495\n",
      "Round 144, Average loss -0.557\n",
      "Round 145, Average loss -0.544\n",
      "Round 146, Average loss -0.528\n",
      "Round 147, Average loss -0.573\n",
      "Round 148, Average loss -0.565\n",
      "Round 149, Average loss -0.524\n",
      "Round 150, Average loss -0.555\n",
      "Round 151, Average loss -0.586\n",
      "Round 152, Average loss -0.597\n",
      "Round 153, Average loss -0.611\n",
      "Round 154, Average loss -0.630\n",
      "Round 155, Average loss -0.654\n",
      "Round 156, Average loss -0.660\n",
      "Round 157, Average loss -0.583\n",
      "Round 158, Average loss -0.637\n",
      "Round 159, Average loss -0.669\n",
      "Round 160, Average loss -0.647\n",
      "loss tensor(883701.9375, grad_fn=<MeanBackward0>)\n",
      "loss tensor(132400.8125, grad_fn=<MeanBackward0>)\n",
      "loss tensor(145808.5000, grad_fn=<MeanBackward0>)\n",
      "loss tensor(554039.5625, grad_fn=<MeanBackward0>)\n",
      "loss tensor(702593.9375, grad_fn=<MeanBackward0>)\n",
      "Round 161, Average loss -0.633\n",
      "Round 162, Average loss -0.667\n",
      "Round 163, Average loss -0.640\n",
      "Round 164, Average loss -0.704\n",
      "Round 165, Average loss -0.685\n",
      "Round 166, Average loss -0.638\n",
      "Round 167, Average loss -0.661\n",
      "Round 168, Average loss -0.686\n",
      "Round 169, Average loss -0.653\n",
      "Round 170, Average loss -0.667\n",
      "Round 171, Average loss -0.672\n",
      "Round 172, Average loss -0.674\n",
      "Round 173, Average loss -0.681\n",
      "Round 174, Average loss -0.691\n",
      "Round 175, Average loss -0.699\n",
      "Round 176, Average loss -0.692\n",
      "Round 177, Average loss -0.676\n",
      "Round 178, Average loss -0.712\n",
      "Round 179, Average loss -0.703\n",
      "Round 180, Average loss -0.673\n",
      "Round 181, Average loss -0.702\n",
      "Round 182, Average loss -0.690\n",
      "Round 183, Average loss -0.695\n",
      "Round 184, Average loss -0.684\n",
      "Round 185, Average loss -0.719\n",
      "Round 186, Average loss -0.709\n",
      "Round 187, Average loss -0.702\n",
      "Round 188, Average loss -0.717\n",
      "Round 189, Average loss -0.724\n",
      "Round 190, Average loss -0.750\n",
      "Round 191, Average loss -0.695\n",
      "Round 192, Average loss -0.746\n",
      "Round 193, Average loss -0.707\n",
      "Round 194, Average loss -0.733\n",
      "Round 195, Average loss -0.724\n",
      "Round 196, Average loss -0.710\n",
      "Round 197, Average loss -0.709\n",
      "Round 198, Average loss -0.734\n",
      "Round 199, Average loss -0.727\n",
      "Round 200, Average loss -0.753\n",
      "loss tensor(180334.6875, grad_fn=<MeanBackward0>)\n",
      "loss tensor(50155.2383, grad_fn=<MeanBackward0>)\n",
      "loss tensor(301456.3750, grad_fn=<MeanBackward0>)\n",
      "loss tensor(284700.3125, grad_fn=<MeanBackward0>)\n",
      "loss tensor(265758.9688, grad_fn=<MeanBackward0>)\n",
      "Round 201, Average loss -0.713\n",
      "Round 202, Average loss -0.752\n",
      "Round 203, Average loss -0.788\n",
      "Round 204, Average loss -0.728\n",
      "Round 205, Average loss -0.761\n",
      "Round 206, Average loss -0.779\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c8681b9e2b3e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;31m#             w, loss = local.train(net=copy.deepcopy(net_self).to(args.device))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflag\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_glob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;31m#         if loss != 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;31m#             print('local loss:', loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/chenweilong/federated-learning/models/Update.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, net)\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m                     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m                     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_idx\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# training rl\n",
    "\n",
    "#先全部训练一次，然后找topk个客户端，这topk个再训练一次，fedavg。之后再从100个里面找topk\n",
    "#（其他90个就不更新了，也不接收global，除非被选中）\n",
    "\n",
    "\n",
    "loss_train = []\n",
    "cv_loss, cv_acc = [], []\n",
    "val_loss_pre, counter = 0, 0\n",
    "net_best = None\n",
    "best_loss = None\n",
    "val_acc_list, net_list = [], []\n",
    "\n",
    "#用来装w和更新被选择后训练了的w\n",
    "w_save = []\n",
    "loss_save = []\n",
    "\n",
    "#使用网络采样前可以先多进行一段时间的随机采样来丰富buffer\n",
    "threshold = 10\n",
    "m = max(int(args.frac * args.num_users), 1)\n",
    "constant = 64\n",
    "target_acc = 0.99\n",
    "args.emb = False\n",
    "\n",
    "beta_start = 0.4\n",
    "beta_frames = 1000 \n",
    "beta_by_frame = lambda frame_idx: min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)\n",
    "\n",
    "# dqn = torch.load('doubledqn2_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "# dqn = torch.load('dqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "\n",
    "last_replay_data = []\n",
    "for iter in range(args.epochs):\n",
    "    #判断7次（2**6=64），视情况k值减半\n",
    "#     if iter % int(args.epochs/7) == 0:\n",
    "#         decay_flag = np.mean(loss_save)\n",
    "#         if np.mean(loss_save) > decay_flag:\n",
    "#             args.k = int(args.k * args.k_frac)\n",
    "#             #buffer要清空，不然optimize时拼接会有问题\n",
    "#             replay_buffer = PrioritizedBuffer(100)\n",
    "#             print('dacay')\n",
    "    \n",
    "    if iter == 0:\n",
    "        random_n = 0\n",
    "        n_weight = []\n",
    "        while random_n<args.num_users*args.frac:\n",
    "            n_weight.append(random.random()) # 随机初始化参数\n",
    "            random_n+=1\n",
    "#         action = random.sample(range(0,100),1)[0]\n",
    "        action = random.sample(range(0,100),10)\n",
    "\n",
    "    #重置global参数    \n",
    "    if iter % args.reset_flag == 0:\n",
    "        if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "            net_glob = CNNCifar(args=args).to(args.device)\n",
    "        elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "            net_glob = CNNMnist(args=args).to(args.device)\n",
    "        elif args.model == 'mlp':\n",
    "            len_in = 1\n",
    "            for x in img_size:\n",
    "                len_in *= x\n",
    "            net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "        else:\n",
    "            exit('Error: unrecognized model')\n",
    "        \n",
    "        \n",
    "    loss_locals = []\n",
    "    w_locals = []\n",
    "    p_emb_collect = []\n",
    "    for i in layer_name:\n",
    "        if i == 'conv1':\n",
    "            emb_global = layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "        else:\n",
    "            emb_global += layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "    emb_global = emb_global/4  #(1*100)\n",
    "    \n",
    "#拼接\n",
    "#     for i in layer_name:\n",
    "#         if i == 'conv1':\n",
    "#             emb_global = layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "#         else:\n",
    "#             emb_global = torch.cat([emb_global, layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))], 1)\n",
    "                \n",
    "    p_emb_collect.append(emb_global)\n",
    "        \n",
    "#     #先全部训练一次\n",
    "#     if iter == 0:\n",
    "#         for idx in range(100):\n",
    "#             net_init = CNNMnist(args=args).to(args.device)\n",
    "#             local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx], flag=True)\n",
    "#             w, loss = local.train(net=copy.deepcopy(net_init).to(args.device))\n",
    "            \n",
    "#             for i in layer_name:\n",
    "#                 if i == 'conv1':\n",
    "#                     emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "#                 else:\n",
    "#                     emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "           \n",
    "#             avg_emb_feature = emb_feature/4\n",
    "            \n",
    "    \n",
    "#             ##########  拼接方式  ##########\n",
    "# #             for i in layer_name:\n",
    "# #                 if i == 'conv1':\n",
    "# #                     cat_emb = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "# #                 else:\n",
    "# #                     cat_emb = torch.cat([cat_emb, layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))], 1)                    \n",
    "\n",
    "#             ############ 储存嵌入 ##############\n",
    "#             p_emb_collect.append(avg_emb_feature)\n",
    "# #             p_emb_collect.append(cat_emb)\n",
    "#             ############ 储存参数 ##############\n",
    "#             w_locals.append(copy.deepcopy(w))\n",
    "#             w_save.append(w)\n",
    "# #             net_tmp = CNNMnist(args=args).to(args.device)\n",
    "# #             net_tmp.load_state_dict(w)\n",
    "# #             global_acc, loss_train = test_img(net_tmp, dataset_train, args)\n",
    "# #             print(global_acc)\n",
    "\n",
    "\n",
    "#     elif iter >= 1:\n",
    "\n",
    "#之后用global赋值然后训练\n",
    "    for idx in range(100):\n",
    "#         if idx == action:\n",
    "#             local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx], flag=True)\n",
    "#             w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "#         else:\n",
    "#             local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "#             net_self = CNNMnist(args=args).to(args.device)\n",
    "#             net_self.load_state_dict(w_save[idx])\n",
    "#             w, loss = local.train(net=copy.deepcopy(net_self).to(args.device))\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx], flag=True)\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "#         if loss != 0:\n",
    "#             print('local loss:', loss)\n",
    "    #把parameter转成嵌入\n",
    "\n",
    "        ##########  求均值方式  #######\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "\n",
    "#             #分母对应local网络的层数\n",
    "        avg_emb_feature = emb_feature/4\n",
    "\n",
    "        ##########  拼接方式  ##########\n",
    "#             for i in layer_name:\n",
    "#                 if i == 'conv1':\n",
    "#                     cat_emb = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "#                 else:\n",
    "#                     cat_emb = torch.cat([cat_emb, layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))], 1)\n",
    "\n",
    "        ############ 储存嵌入 ##############\n",
    "        p_emb_collect.append(avg_emb_feature)\n",
    "#             p_emb_collect.append(cat_emb)\n",
    "        ############ 储存参数 ##############\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "    p_emb_collect = torch.squeeze(torch.cat(p_emb_collect,1).unsqueeze(0),1).to(args.device)\n",
    "#     print(p_emb_collect.shape)\n",
    "    \n",
    "    action_next = dqn.choose_action_train(p_emb_collect)# list\n",
    "    \n",
    "#     if iter > threshold:\n",
    "#         action_next = dqn.choose_action_train(p_emb_collect)# list\n",
    "#     #开始更新之前没必要用错误的网络一直选择一个client，可能污染buffer\n",
    "#     else:\n",
    "#         action_next = random.sample(range(0,100),10)\n",
    "#         print(action_next)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ###########  计算当前轮的reward，然后将当前轮的reward添加到上一个replay_data中    \n",
    "    net_glob.eval()\n",
    "    global_acc, loss_train = test_img(net_glob, dataset_test, args)\n",
    "#     print(global_acc)\n",
    "    reward = constant ** (global_acc.numpy()/100 - target_acc) - 1\n",
    "    if reward >= 0:\n",
    "        print('well done')\n",
    "        break\n",
    "    loss_save.append(reward)\n",
    "    \n",
    "    if len(last_replay_data)==3:\n",
    "#         last_replay_data.append(reward)#r\n",
    "        last_replay_data.append(p_emb_collect)#s_next\n",
    "        dqn.replay_buffer.add(last_replay_data[0], last_replay_data[1], last_replay_data[2], last_replay_data[3])\n",
    "    \n",
    "    last_replay_data = [p_emb_collect, torch.LongTensor(action_next).unsqueeze(0), reward]#s, a\n",
    "    action = action_next\n",
    "    \n",
    "    # update global weights\n",
    "    w_chosen = []\n",
    "    for i in action:\n",
    "        w_chosen.append(w_locals[i])\n",
    "        \n",
    "    w_glob = FedAvg(w_locals)\n",
    "#     w_glob = FedPareto(w_locals, action_next)\n",
    "#     w_glob = w_locals[action]\n",
    "    \n",
    "    #ours\n",
    "#     w_glob = FedPareto(w_locals, weight, action)\n",
    "    #更新w_save\n",
    "#     w_save[action] = w_glob\n",
    "        \n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    \n",
    "    # print loss\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, reward))\n",
    "#     loss_train.append(-reward)\n",
    "    \n",
    "    \n",
    "    if iter > 30 and iter % 40 == 0:\n",
    "        beta = beta_by_frame(iter)\n",
    "        dqn.optimize(beta)\n",
    "    \n",
    "    #debug\n",
    "#     if iter > 10:\n",
    "#         beta = beta_by_frame(iter)\n",
    "#         dqn.optimize(beta)\n",
    "#     if iter > 10:\n",
    "#         #如果刚好遇上buffer清空，就跳过一次更新\n",
    "#         if len(dqn.replay_buffer.buffer) == 0:\n",
    "#             print('buffer=0')\n",
    "#             continue\n",
    "#         else:\n",
    "#             beta = beta_by_frame(iter)\n",
    "#             dqn.optimize(beta)\n",
    "    if iter % 500 == 0:\n",
    "#         torch.save(dqn, 'dqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "        torch.save(dqn, 'dqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "        print(\"saved\")\n",
    "#     args.lr = max(args.lr*args.lr_decay, 0.001)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[75, 50, 99, 74, 43, 71, 69, 78, 20, 70]\n",
      "[83, 5, 2, 85, 59]\n",
      "Round   0, Average loss 0.975\n",
      "[83, 72, 5, 59, 85]\n",
      "Round   1, Average loss 0.965\n",
      "[83, 5, 72, 2, 85]\n",
      "Round   2, Average loss 0.962\n",
      "[83, 5, 72, 2, 59]\n",
      "Round   3, Average loss 0.949\n",
      "[83, 2, 72, 85, 5]\n",
      "Round   4, Average loss 0.950\n",
      "[2, 83, 5, 85, 72]\n",
      "Round   5, Average loss 0.945\n",
      "[5, 83, 72, 2, 85]\n",
      "Round   6, Average loss 0.940\n",
      "[83, 5, 72, 2, 85]\n",
      "Round   7, Average loss 0.940\n",
      "[83, 5, 72, 2, 8]\n",
      "Round   8, Average loss 0.936\n",
      "[5, 83, 85, 72, 2]\n",
      "Round   9, Average loss 0.937\n"
     ]
    }
   ],
   "source": [
    "# fl\n",
    "#initialize\n",
    "if args.model == 'cnn' and args.dataset == 'cifar':\n",
    "    net_glob = CNNCifar(args=args).to(args.device)\n",
    "elif args.model == 'cnn' and args.dataset == 'mnist':\n",
    "    net_glob = CNNMnist(args=args).to(args.device)\n",
    "elif args.model == 'mlp':\n",
    "    len_in = 1\n",
    "    for x in img_size:\n",
    "        len_in *= x\n",
    "    net_glob = MLP(dim_in=len_in, dim_hidden=200, dim_out=args.num_classes).to(args.device)\n",
    "else:\n",
    "    exit('Error: unrecognized model')\n",
    "# net_glob.train()\n",
    "w_glob = net_glob.state_dict()\n",
    "\n",
    "w_save = []\n",
    "m = max(int(args.frac * args.num_users), 1)\n",
    "dqn = torch.load('dqn_{}_{}_{}_clientnumber{}_localep{}.pt'.format(args.dataset, args.epochs, args.model, m, args.local_ep))\n",
    "constant = 64\n",
    "target_acc = 0.99\n",
    "for iter in range(args.validation_epochs):\n",
    "    if iter == 0:\n",
    "        random_n = 0\n",
    "        n_weight = []\n",
    "        while random_n<args.num_users*args.frac:\n",
    "            n_weight.append(random.random()) # 随机初始化参数\n",
    "            random_n+=1\n",
    "        action = random.sample(range(0,100),10)\n",
    "        print(action)\n",
    "        \n",
    "    loss_locals = []\n",
    "    w_locals = []\n",
    "    p_emb_collect = []\n",
    "    for i in layer_name:\n",
    "        if i == 'conv1':\n",
    "            emb_global = layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "        else:\n",
    "            emb_global += layer_dict[i].forward(torch.cat([net_glob.state_dict()[i+'.weight'].reshape(1,-1), net_glob.state_dict()[i+'.bias'].reshape(1,-1)], 1))\n",
    "    emb_global = emb_global/4\n",
    "    p_emb_collect.append(emb_global)\n",
    "    \n",
    "    for idx in range(100):\n",
    "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
    "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
    "        \n",
    "        ##########  求均值方式  #######\n",
    "        for i in layer_name:\n",
    "            if i == 'conv1':\n",
    "                emb_feature = layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "            else:\n",
    "                emb_feature += layer_dict[i].forward(torch.cat([w[i+'.weight'].reshape(1,-1), w[i+'.bias'].reshape(1,-1)], 1))\n",
    "\n",
    "        #分母对应local网络的层数\n",
    "        avg_emb_feature = emb_feature/4\n",
    "        \n",
    "        ############ 储存嵌入 ##############\n",
    "        p_emb_collect.append(avg_emb_feature)\n",
    "        ############ 储存参数 ##############\n",
    "        w_locals.append(copy.deepcopy(w))\n",
    "        \n",
    "    p_emb_collect = torch.squeeze(torch.cat(p_emb_collect,1).unsqueeze(0),1).to(args.device)\n",
    "    action_next = dqn.choose_action_run(p_emb_collect)\n",
    "    \n",
    "    action = action_next\n",
    "    # update global weights\n",
    "    #fedavg\n",
    "    chosen_w = []\n",
    "    for i in action:\n",
    "        chosen_w.append(w_locals[i])\n",
    "    w_glob = FedAvg(chosen_w)\n",
    "    \n",
    "    #ours\n",
    "#     w_glob = FedPareto(w_locals, weight, action_next)\n",
    "    # copy weight to net_glob\n",
    "    net_glob.load_state_dict(w_glob)\n",
    "    \n",
    "    #计算当前轮的reward   \n",
    "#     net_glob.eval()\n",
    "    global_acc, loss_train = test_img(net_glob, dataset_train, args)\n",
    "    reward = constant ** (global_acc.numpy()/100 - target_acc) - 1\n",
    "    \n",
    "    # print loss\n",
    "    print('Round {:3d}, Average loss {:.3f}'.format(iter, -reward))\n",
    "#     loss_train.append(-reward)\n",
    "\n",
    "#     args.lr = max(args.lr*args.lr_decay, 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy: 32.56\n",
      "Testing accuracy: 32.33\n"
     ]
    }
   ],
   "source": [
    "# plot loss curve\n",
    "# plt.figure()\n",
    "# plt.plot(range(len(loss_train)), loss_train)\n",
    "# plt.ylabel('train_loss')\n",
    "# plt.show()\n",
    "# plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
    "\n",
    "# testing\n",
    "net_glob.eval()\n",
    "acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
    "acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
    "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
    "print(\"Testing accuracy: {:.2f}\".format(acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-0.975371031855105,\n",
       " -0.8102594432248442,\n",
       " -0.975580118651963,\n",
       " -0.9007438634890736,\n",
       " -0.7761971886804484,\n",
       " -0.6804087589944856,\n",
       " -0.5142451953292079,\n",
       " -0.5662005594182116,\n",
       " -0.5218947723519878,\n",
       " -0.8102594432248442,\n",
       " -0.5218947723519878,\n",
       " -0.4699501956687363,\n",
       " -0.519869068681007,\n",
       " -0.9750860083538706,\n",
       " -0.519869068681007,\n",
       " -0.4451382740224047,\n",
       " -0.4999999471170642,\n",
       " -0.4421303270368043,\n",
       " -0.9750860083538706,\n",
       " -0.4421303270368043,\n",
       " -0.4680361833106459,\n",
       " -0.9750860083538706,\n",
       " -0.4680361833106459,\n",
       " -0.43590901479795074,\n",
       " -0.523581896570082,\n",
       " -0.43856139831845153,\n",
       " -0.4304082993402708,\n",
       " -0.9750860083538706,\n",
       " -0.4304082993402708,\n",
       " -0.9750860083538706,\n",
       " -0.4304082993402708,\n",
       " -0.4048603887714919,\n",
       " -0.4176332067657721,\n",
       " -0.46470721366917545,\n",
       " -0.9750860083538706,\n",
       " -0.46470721366917545,\n",
       " -0.9750860083538706,\n",
       " -0.46470721366917545,\n",
       " -0.44502294516640284,\n",
       " -0.41056687074612686,\n",
       " -0.4535733233867021,\n",
       " -0.9750860083538706,\n",
       " -0.4535733233867021,\n",
       " -0.9750860083538706,\n",
       " -0.4535733233867021,\n",
       " -0.4123618233597025,\n",
       " -0.4118320491415227,\n",
       " -0.9750860083538706,\n",
       " -0.4118320491415227,\n",
       " -0.9750860083538706,\n",
       " -0.9754477337242985,\n",
       " -0.975580118651963,\n",
       " -0.8102594432248442,\n",
       " -0.7335198332252053,\n",
       " -0.975580118651963,\n",
       " -0.8528152610005035,\n",
       " -0.6605667502334998,\n",
       " -0.5179348272144673,\n",
       " -0.7335198332252053,\n",
       " -0.5179348272144673,\n",
       " -0.5019024837881024,\n",
       " -0.43902829533375376,\n",
       " -0.3854936399739267,\n",
       " -0.3610388141491313,\n",
       " -0.364572309585699,\n",
       " -0.3951705808197421,\n",
       " -0.34905842058437686,\n",
       " -0.31854035320370366,\n",
       " -0.28762930547704924,\n",
       " -0.3506357281520054,\n",
       " -0.3428475829838281,\n",
       " -0.30859650930416116,\n",
       " -0.31953167410250516,\n",
       " -0.34047460248833483,\n",
       " -0.3508157836513238,\n",
       " -0.3463002120602451,\n",
       " -0.38489690234093066,\n",
       " -0.3440308695541041,\n",
       " -0.3524336272055322,\n",
       " -0.32936485004529714,\n",
       " -0.3098892065115767,\n",
       " -0.3436669444131213,\n",
       " -0.7335198332252053,\n",
       " -0.5019024837881024,\n",
       " -0.42807409957993114,\n",
       " -0.5059599553006087,\n",
       " -0.42660557112160746,\n",
       " -0.42241708388719046,\n",
       " -0.3987652865247635,\n",
       " -0.4206127699323766,\n",
       " -0.3997646638605937,\n",
       " -0.35761959708320123,\n",
       " -0.3377720625136471,\n",
       " -0.348290870685663,\n",
       " -0.34507571898126244,\n",
       " -0.34923891343898894,\n",
       " -0.34702494116048244,\n",
       " -0.37954441640173564,\n",
       " -0.3314533261056185,\n",
       " -0.37384144986774237,\n",
       " -0.9755648796356599,\n",
       " -0.9330373750502098,\n",
       " -0.3436669444131213,\n",
       " -0.35104067986243304,\n",
       " -0.33408953587784695,\n",
       " -0.31607981619242276,\n",
       " -0.3668583853402593,\n",
       " -0.3359332464807905,\n",
       " -0.3299689040603667,\n",
       " -0.2932362618235992,\n",
       " -0.33140708054681445,\n",
       " -0.36241030117236384,\n",
       " -0.4181577557693844,\n",
       " -0.35158015409381216,\n",
       " -0.39813999799420385,\n",
       " -0.3796733522415746,\n",
       " -0.35373398026092673,\n",
       " -0.3602855937666195,\n",
       " -0.3811764762497065,\n",
       " -0.4209740830812321,\n",
       " -0.4992022598362974,\n",
       " -0.4730273963876921,\n",
       " -0.9750860083538706,\n",
       " -0.8637243159541047,\n",
       " -0.5966232350848266,\n",
       " -0.4729907767513778,\n",
       " -0.5258551369550505,\n",
       " -0.36205677517489343,\n",
       " -0.42779656353951134,\n",
       " -0.40654939488869624,\n",
       " -0.3710575539943778,\n",
       " -0.3632495192618388,\n",
       " -0.3946673730785112,\n",
       " -0.4292622228243196,\n",
       " -0.4490092143202642,\n",
       " -0.36514465156286335,\n",
       " -0.37035969818310766,\n",
       " -0.38297537891668365,\n",
       " -0.31617464134143036,\n",
       " -0.34289303722041975,\n",
       " -0.3749254942587623,\n",
       " -0.3565053355135037,\n",
       " -0.4730273963876921,\n",
       " -0.4091761927795168,\n",
       " -0.40876643541374047,\n",
       " -0.4581369363427532,\n",
       " -0.5956994956458164,\n",
       " -0.4254516608263844,\n",
       " -0.39508690186964057,\n",
       " -0.6767997922579818,\n",
       " -0.9708366551591051,\n",
       " -0.814704466751471,\n",
       " -0.6370858929881975,\n",
       " -0.4914717340914515,\n",
       " -0.43665131253447353,\n",
       " -0.4210543646991036,\n",
       " -0.40687826462029164,\n",
       " -0.3859620016819959,\n",
       " -0.36001943183523333,\n",
       " -0.34316632185911167,\n",
       " -0.33344309201042155,\n",
       " -0.32395077814836637,\n",
       " -0.9457750742286363,\n",
       " -0.9326183304863176,\n",
       " -0.8268613256422155,\n",
       " -0.9457750742286363,\n",
       " -0.9388795054974558,\n",
       " -0.9457750742286363,\n",
       " -0.8268613256422155,\n",
       " -0.8048710254401447,\n",
       " -0.6351437467250185,\n",
       " -0.9457750742286363,\n",
       " -0.9198970072816367,\n",
       " -0.9457750742286363,\n",
       " -0.6351437467250185,\n",
       " -0.9457750742286363,\n",
       " -0.812378122981826,\n",
       " -0.7427042046347339,\n",
       " -0.6710135645392992,\n",
       " -0.5449489876444042,\n",
       " -0.6351437467250185,\n",
       " -0.5449489876444042,\n",
       " -0.3565053355135037,\n",
       " -0.2900446641470986,\n",
       " -0.31877664670909467,\n",
       " -0.32395077814836637,\n",
       " -0.3988486473797922,\n",
       " -0.3917653589996718,\n",
       " -0.31546324072970255,\n",
       " -0.38489690234093066,\n",
       " -0.3717111785583923,\n",
       " -0.43844470244880396,\n",
       " -0.36386705609690806,\n",
       " -0.38040394144807765,\n",
       " -0.34761288139001556,\n",
       " -0.36192417770264285,\n",
       " -0.3892727995898194,\n",
       " -0.3231068032000166,\n",
       " -0.3625429998877143,\n",
       " -0.4076177902771435,\n",
       " -0.9750410685973537,\n",
       " -0.9740082064341005,\n",
       " -0.32395077814836637,\n",
       " -0.32815510780337664,\n",
       " -0.9330373750502098,\n",
       " -0.32815510780337664,\n",
       " -0.308069136534472,\n",
       " -0.34756775364047354,\n",
       " -0.3509955832974059,\n",
       " -0.3392849008554225,\n",
       " -0.33878093586753877,\n",
       " -0.9330373750502098,\n",
       " -0.33878093586753877,\n",
       " -0.3022414194747539,\n",
       " -0.3206627542071814,\n",
       " -0.39045701615983885,\n",
       " -0.9330373750502098,\n",
       " -0.39045701615983885,\n",
       " -0.35837595649622733,\n",
       " -0.37375462129804016,\n",
       " -0.9330373750502098,\n",
       " -0.37375462129804016,\n",
       " -0.9740082064341005,\n",
       " -0.9734012912886574,\n",
       " -0.9621142261705588,\n",
       " -0.8525804228489562,\n",
       " -0.7574758863236579,\n",
       " -0.642007769263687,\n",
       " -0.6008231119286591,\n",
       " -0.5801633305287053,\n",
       " -0.5951947698266734,\n",
       " -0.5378605523653632,\n",
       " -0.521231528271787,\n",
       " -0.5741256018964145,\n",
       " -0.46161884054781177,\n",
       " -0.4879701132227128,\n",
       " -0.5154222360247909,\n",
       " -0.5022821215244282,\n",
       " -0.49302031372613375,\n",
       " -0.5880332017826232,\n",
       " -0.45171413611814215,\n",
       " -0.48380076501613556,\n",
       " -0.37375462129804016,\n",
       " -0.3234821285778543,\n",
       " -0.4191650770319999,\n",
       " -0.32950421361711557,\n",
       " -0.3051372457687387,\n",
       " -0.4545570611003217,\n",
       " -0.3760510204668587,\n",
       " -0.38199079754367726,\n",
       " -0.9765261219980375,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.9750860083538706,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.48380076501613556,\n",
       " -0.49164790300832373,\n",
       " -0.5187695609360796,\n",
       " -0.5163952979415827,\n",
       " -0.49037786460309096,\n",
       " -0.5133691085988701,\n",
       " -0.5282154419318525,\n",
       " -0.49333647262185565,\n",
       " -0.5076690998641799,\n",
       " -0.5052746058919835,\n",
       " -0.5022476927518149,\n",
       " -0.4767399664708396,\n",
       " -0.48590752767490286,\n",
       " -0.5180016652796726,\n",
       " -0.4876506057096741,\n",
       " -0.6021765580556535,\n",
       " -0.5373477761945114,\n",
       " -0.5228879918124374,\n",
       " -0.5017989534596754,\n",
       " -0.7779587049850238,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.9753983310880908,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.975580118651963,\n",
       " -0.9750860083538706,\n",
       " -0.9750860083538706,\n",
       " -0.9786097077475368,\n",
       " -0.9749665666562847,\n",
       " -0.7779587049850238,\n",
       " -0.5243409020808993,\n",
       " -0.5732095249717089,\n",
       " -0.6234534940198762,\n",
       " -0.5378926644760292,\n",
       " -0.5150526210012099,\n",
       " -0.5061995797227598,\n",
       " -0.5591377628261651,\n",
       " -0.5572083222163606,\n",
       " -0.5595957864521627,\n",
       " -0.5681805711712431,\n",
       " -0.5933950090319706,\n",
       " -0.5666513828155203,\n",
       " -0.6212546905697924,\n",
       " -0.6452683135952904,\n",
       " -0.8448753415514281,\n",
       " -0.5767446916755461,\n",
       " -0.7252293343499722,\n",
       " -0.4884666928489656,\n",
       " -0.5994373548941887,\n",
       " -0.9749665666562847,\n",
       " -0.8438288411591219,\n",
       " -0.7582144252182217,\n",
       " -0.6211496430746647,\n",
       " -0.5624862668824768,\n",
       " -0.5075667681356992,\n",
       " -0.5127277436383162,\n",
       " -0.5128965488579666,\n",
       " -0.4439447389664495,\n",
       " -0.42241708388719046,\n",
       " -0.4496581432714142,\n",
       " -0.4218963748012575,\n",
       " -0.3977644363377376,\n",
       " -0.4219365447709146,\n",
       " -0.42401624143747774,\n",
       " -0.4074536772003873,\n",
       " -0.42445524216152075,\n",
       " -0.41613768407251084,\n",
       " -0.4350872349131649,\n",
       " -0.36545257568288336,\n",
       " -0.6351437467250185,\n",
       " -0.9198970072816367,\n",
       " -0.6333437593505706,\n",
       " -0.6351437467250185,\n",
       " -0.6333437593505706,\n",
       " -0.561636406833703,\n",
       " -0.895845283286498,\n",
       " -0.561636406833703,\n",
       " -0.9749665666562847,\n",
       " -0.5994373548941887,\n",
       " -0.974853524880406,\n",
       " -0.5994373548941887,\n",
       " -0.561636406833703,\n",
       " -0.895845283286498,\n",
       " -0.561636406833703,\n",
       " -0.45948749444319303,\n",
       " -0.45791165880461926,\n",
       " -0.42129495915119275,\n",
       " -0.5994373548941887,\n",
       " -0.974853524880406,\n",
       " -0.36545257568288336,\n",
       " -0.4646701857173593,\n",
       " -0.4296181420657882,\n",
       " -0.4496581432714142,\n",
       " -0.974853524880406,\n",
       " -0.4496581432714142,\n",
       " -0.6351437467250185,\n",
       " -0.4496581432714142,\n",
       " -0.42417595001360897,\n",
       " -0.4113834199901506,\n",
       " -0.44402183571579557,\n",
       " -0.4090942640314127,\n",
       " -0.3934913363654682,\n",
       " -0.5094405125008408,\n",
       " -0.40733032749082676,\n",
       " -0.4228573034499805,\n",
       " -0.39755554104411384,\n",
       " -0.974853524880406,\n",
       " -0.39755554104411384,\n",
       " -0.40013897815007116,\n",
       " -0.5449489876444042,\n",
       " -0.40013897815007116,\n",
       " -0.974853524880406,\n",
       " -0.5994373548941887,\n",
       " -0.974853524880406,\n",
       " -0.5994373548941887,\n",
       " -0.974853524880406,\n",
       " -0.895845283286498,\n",
       " -0.5994373548941887,\n",
       " -0.974853524880406,\n",
       " -0.5994373548941887,\n",
       " -0.974853524880406,\n",
       " -0.5994373548941887,\n",
       " -0.974853524880406,\n",
       " -0.5994373548941887,\n",
       " -0.974853524880406,\n",
       " -0.895845283286498,\n",
       " -0.5994373548941887,\n",
       " -0.9754477337242985,\n",
       " -0.5994373548941887,\n",
       " -0.40013897815007116,\n",
       " -0.42129495915119275,\n",
       " -0.3621009007407784,\n",
       " -0.36554055532654306,\n",
       " -0.3949191252552601,\n",
       " -0.36373483489430813,\n",
       " -0.40013897815007116,\n",
       " -0.38387277054130486,\n",
       " -0.36373483489430813,\n",
       " -0.38387277054130486,\n",
       " -0.36373483489430813,\n",
       " -0.38387277054130486,\n",
       " -0.36373483489430813,\n",
       " -0.38387277054130486,\n",
       " -0.42513308953347484,\n",
       " -0.4175929225510211,\n",
       " -0.5061311050781157,\n",
       " -0.36373483489430813,\n",
       " -0.383360163214983,\n",
       " -0.34652687994159626,\n",
       " -0.5994373548941887,\n",
       " -0.7251531249040927,\n",
       " -0.54871838362191,\n",
       " -0.6726513711031248,\n",
       " -0.6112808934417455,\n",
       " -0.5617275024407599,\n",
       " -0.932944471546461,\n",
       " -0.5449489876444042,\n",
       " -0.932944471546461,\n",
       " -0.7384062304224741,\n",
       " -0.732000913583567,\n",
       " -0.6046506100038886,\n",
       " -0.5767741018922469,\n",
       " -0.6630048191258162,\n",
       " -0.6523517883496912,\n",
       " -0.658584557382961,\n",
       " -0.8804239751796911,\n",
       " -0.6404909772814018,\n",
       " -0.8214995100910707,\n",
       " -0.6770461275462609,\n",
       " -0.5061311050781157,\n",
       " -0.5449489876444042,\n",
       " -0.5061311050781157,\n",
       " -0.6351437467250185,\n",
       " -0.6446776939060632,\n",
       " -0.5061311050781157,\n",
       " -0.37284229355542775,\n",
       " -0.40167548041525736,\n",
       " -0.9750151041881632,\n",
       " -0.9705787985276451,\n",
       " -0.7443928997889291,\n",
       " -0.5476849412836129,\n",
       " -0.6446776939060632,\n",
       " -0.5476849412836129,\n",
       " -0.5139084612967961,\n",
       " -0.46202918149110583,\n",
       " -0.45327017130820724,\n",
       " -0.41443546435326306,\n",
       " -0.37118825347493667,\n",
       " -0.6446776939060632,\n",
       " -0.6770461275462609,\n",
       " -0.8287471406357046,\n",
       " -0.9388795054974558,\n",
       " -0.6465200807470592,\n",
       " -0.5368344310624881,\n",
       " -0.6770461275462609,\n",
       " -0.9750860083538706,\n",
       " -0.6770461275462609,\n",
       " -0.846607493492213,\n",
       " -0.974010008370345,\n",
       " -0.5368344310624881,\n",
       " -0.3620124441248118,\n",
       " -0.32460664220428104,\n",
       " -0.32521498202893107,\n",
       " -0.33887261351906517,\n",
       " -0.846607493492213,\n",
       " -0.974010008370345,\n",
       " -0.846607493492213,\n",
       " -0.974010008370345,\n",
       " -0.33887261351906517,\n",
       " -0.34652687994159626,\n",
       " -0.4194467996054525,\n",
       " -0.29166675894193617,\n",
       " -0.37118825347493667,\n",
       " -0.29166675894193617,\n",
       " -0.34652687994159626,\n",
       " -0.3407944923312095,\n",
       " -0.29166675894193617,\n",
       " -0.27868576596795425,\n",
       " -0.28748123814170756,\n",
       " -0.3407944923312095,\n",
       " -0.28748123814170756,\n",
       " -0.3407944923312095,\n",
       " -0.28748123814170756,\n",
       " -0.2816294360269107,\n",
       " -0.3407944923312095,\n",
       " -0.37118825347493667,\n",
       " -0.3407944923312095,\n",
       " -0.9745766140441587,\n",
       " -0.862271416787721,\n",
       " -0.2816294360269107,\n",
       " -0.3138479624746221,\n",
       " -0.3259160343183749,\n",
       " -0.3615701795908274,\n",
       " -0.2872343154688779,\n",
       " -0.3589538796481373,\n",
       " -0.3008374581424219,\n",
       " -0.895845283286498,\n",
       " -0.7792018455723,\n",
       " -0.3008374581424219,\n",
       " -0.2757799004823932,\n",
       " -0.4321035499336655,\n",
       " -0.7792018455723,\n",
       " -0.4321035499336655,\n",
       " -0.7792018455723,\n",
       " -0.4321035499336655,\n",
       " -0.3030629829408925,\n",
       " -0.7792018455723,\n",
       " -0.3030629829408925,\n",
       " -0.35005030592603803,\n",
       " -0.862271416787721,\n",
       " -0.6850270443473528,\n",
       " -0.535773774276991,\n",
       " -0.4886794347660993,\n",
       " -0.5314418369075009,\n",
       " -0.4793449331962959,\n",
       " -0.497463562877123,\n",
       " -0.39813999799420385,\n",
       " -0.4605728767570503,\n",
       " -0.3896536567375446,\n",
       " -0.38744988755714294,\n",
       " -0.41007647245501777,\n",
       " -0.3845984138863098,\n",
       " -0.43351881479695065,\n",
       " -0.4220966466372965,\n",
       " -0.3902455861675427,\n",
       " -0.43151275306300296,\n",
       " -0.44617585874920385,\n",
       " -0.36830497735199563,\n",
       " -0.40687826462029164,\n",
       " -0.35005030592603803,\n",
       " -0.3015638368965563,\n",
       " -0.30547427530256543,\n",
       " -0.30955431939257216,\n",
       " -0.3179733904225791,\n",
       " -0.2781355785729863,\n",
       " -0.28718478496204314,\n",
       " -0.2926480220298032,\n",
       " -0.9681110585205206,\n",
       " -0.906286544019391,\n",
       " -0.7495143279443267,\n",
       " -0.6277355199496302,\n",
       " -0.42517285220630485,\n",
       " -0.3512206230768822,\n",
       " -0.35538934569904046,\n",
       " -0.3367614393992425,\n",
       " -0.2909791324449127,\n",
       " -0.29738797611568246,\n",
       " -0.28925691884541993,\n",
       " -0.3058111413668355,\n",
       " -0.974853524880406,\n",
       " -0.40687826462029164,\n",
       " -0.974853524880406,\n",
       " -0.40687826462029164,\n",
       " -0.974853524880406,\n",
       " -0.40687826462029164,\n",
       " -0.974853524880406,\n",
       " -0.40687826462029164,\n",
       " -0.974853524880406,\n",
       " -0.40687826462029164,\n",
       " -0.974853524880406,\n",
       " -0.40687826462029164,\n",
       " -0.974853524880406,\n",
       " -0.40687826462029164,\n",
       " -0.974853524880406,\n",
       " -0.40687826462029164,\n",
       " -0.974853524880406,\n",
       " -0.40687826462029164,\n",
       " -0.974853524880406,\n",
       " -0.40687826462029164,\n",
       " -0.3058111413668355,\n",
       " -0.3037872704441671,\n",
       " -0.24235177529252971,\n",
       " -0.2557805251028017,\n",
       " -0.35054568167587175,\n",
       " -0.30243489373114085,\n",
       " -0.29475328244529697,\n",
       " -0.31560549317176356,\n",
       " -0.3022896823781258,\n",
       " -0.3682173810470454,\n",
       " -0.3437123419757252,\n",
       " -0.29504658795766026,\n",
       " -0.28703662523243445,\n",
       " -0.3311753803312639,\n",
       " -0.3051853083721956,\n",
       " -0.33754235788012166,\n",
       " -0.3585094948618369,\n",
       " -0.33344309201042155,\n",
       " -0.9756679787903476,\n",
       " -0.9762824304031055,\n",
       " -0.7792018455723,\n",
       " -0.5489060674244959,\n",
       " -0.46862578032308444,\n",
       " -0.5065759423177321,\n",
       " -0.37118825347493667,\n",
       " -0.40687826462029164,\n",
       " -0.5065759423177321,\n",
       " -0.4284704756729213,\n",
       " -0.42862894918053385,\n",
       " -0.40382825939891254,\n",
       " -0.37118825347493667,\n",
       " -0.40382825939891254,\n",
       " -0.3593981598537114,\n",
       " -0.40687826462029164,\n",
       " -0.3593981598537114,\n",
       " -0.38074730008267765,\n",
       " -0.35162521003949343,\n",
       " -0.9342743598928781,\n",
       " -0.37118825347493667,\n",
       " -0.40687826462029164,\n",
       " -0.9762824304031055,\n",
       " -0.9762824304031055,\n",
       " -0.951359562404973,\n",
       " -0.7493927758314514,\n",
       " -0.7013971384386235,\n",
       " -0.6475721274011319,\n",
       " -0.6578028189714519,\n",
       " -0.6201240826737895,\n",
       " -0.5503108534384765,\n",
       " -0.6046506100038886,\n",
       " -0.5460202283680511,\n",
       " -0.58074493563038,\n",
       " -0.5027993717647115,\n",
       " -0.47251566143328105,\n",
       " -0.4950892884566632,\n",
       " -0.47320995495724183,\n",
       " -0.4749232836941616,\n",
       " -0.5035913294460759,\n",
       " -0.4774286998152528,\n",
       " -0.43017131291996846,\n",
       " -0.5449489876444042,\n",
       " -0.5739189396451003,\n",
       " -0.5033160844558678,\n",
       " -0.45186616463167517,\n",
       " -0.43801637228184853,\n",
       " -0.4397664883514598,\n",
       " -0.48390803551808015,\n",
       " -0.44717291419649297,\n",
       " -0.9737820266144275,\n",
       " -0.9458426838295948,\n",
       " -0.40687826462029164,\n",
       " -0.3360714557480685,\n",
       " -0.37118825347493667,\n",
       " -0.9458426838295948,\n",
       " -0.8816443811488932,\n",
       " -0.8410329212726311,\n",
       " -0.3360714557480685,\n",
       " -0.8410329212726311,\n",
       " -0.7729786498053356,\n",
       " -0.8000792565985169,\n",
       " -0.43017131291996846,\n",
       " -0.4822598378641626,\n",
       " -0.4876150022487802,\n",
       " -0.47288123708054786,\n",
       " -0.4764860463079149,\n",
       " -0.4605728767570503,\n",
       " -0.45986192794197545,\n",
       " -0.4349305526534497,\n",
       " -0.49323100103838224,\n",
       " -0.4291830793791702,\n",
       " -0.5076350437264675,\n",
       " -0.44182082649789145,\n",
       " -0.4574229491894899,\n",
       " -0.4702439308766564,\n",
       " -0.49578874771565007,\n",
       " -0.5119841196118877,\n",
       " -0.46064766802443946,\n",
       " -0.4698031828797945,\n",
       " -0.4499632981559558,\n",
       " -0.46236466684587507,\n",
       " -0.9750860083538706,\n",
       " -0.9281167223425204,\n",
       " -0.8141771308768665,\n",
       " -0.7134020754655618,\n",
       " -0.6595299445543892,\n",
       " -0.6058272709124182,\n",
       " -0.5313118792284774,\n",
       " -0.45000134336089237,\n",
       " -0.4457917619703522,\n",
       " -0.4353612347566028,\n",
       " -0.4164614254623884,\n",
       " -0.444830618416464,\n",
       " -0.4034975092543124,\n",
       " -0.3680860640661213,\n",
       " -0.46618928226399536,\n",
       " -0.4093400161991755,\n",
       " -0.4619171927036274,\n",
       " -0.43261488519017055,\n",
       " -0.9753829786288266,\n",
       " -0.8003008685259243,\n",
       " -0.46236466684587507,\n",
       " -0.48114615313073417,\n",
       " -0.43115801606199233,\n",
       " -0.43265431037598867,\n",
       " -0.4422076753533445,\n",
       " -0.4501538467970049,\n",
       " -0.49009513131753824,\n",
       " -0.45019187882198064,\n",
       " -0.611846240850319,\n",
       " -0.6456614770698724,\n",
       " -0.6138053168588522,\n",
       " -0.5794643126153003,\n",
       " -0.4635557723716881,\n",
       " -0.44359775954561287,\n",
       " -0.5666214066751232,\n",
       " -0.4449459872395669,\n",
       " -0.5953068856718902,\n",
       " -0.4753599418628619,\n",
       " -0.5721135235374103,\n",
       " -0.5804833197630157,\n",
       " -0.8000792565985169,\n",
       " -0.7430606397870334,\n",
       " -0.7108680781455795,\n",
       " -0.6887167407330836,\n",
       " -0.6244179916923572,\n",
       " -0.5318314937768149,\n",
       " -0.5120178749306135,\n",
       " -0.49871610194362015,\n",
       " -0.5504043025898923,\n",
       " -0.5073278071632337,\n",
       " -0.974853524880406,\n",
       " -0.974853524880406,\n",
       " -0.5073278071632337,\n",
       " -0.5234497588850656,\n",
       " -0.57068749607904,\n",
       " -0.4835859932617017,\n",
       " -0.47149103561752104,\n",
       " -0.974853524880406,\n",
       " -0.47149103561752104,\n",
       " -0.48304875486355414,\n",
       " -0.3360714557480685,\n",
       " -0.5804833197630157,\n",
       " -0.5408936864462183,\n",
       " -0.3360714557480685,\n",
       " -0.3358873108127226,\n",
       " -0.4044064419382042,\n",
       " -0.37088311268279583,\n",
       " -0.37101384841371265,\n",
       " -0.9753693242812435,\n",
       " -0.9737329137194711,\n",
       " -0.5408936864462183,\n",
       " -0.6009061908434252,\n",
       " -0.5146827240512267,\n",
       " -0.5304339450900946,\n",
       " -0.6711959817341779,\n",
       " -0.45595417523279924,\n",
       " -0.4832637500504121,\n",
       " -0.5798140335940145,\n",
       " -0.6467160791481052,\n",
       " -0.6410138419693567,\n",
       " -0.8003008685259243,\n",
       " -0.6410138419693567,\n",
       " -0.8003008685259243,\n",
       " -0.37118825347493667,\n",
       " -0.6410138419693567,\n",
       " -0.8003008685259243,\n",
       " -0.37118825347493667,\n",
       " -0.42401624143747774,\n",
       " -0.3997646638605937,\n",
       " -0.3561930696044381,\n",
       " -0.6410138419693567,\n",
       " -0.609471372432468,\n",
       " -0.8003008685259243,\n",
       " -0.3561930696044381,\n",
       " -0.30955431939257216,\n",
       " -0.609471372432468,\n",
       " -0.8003008685259243,\n",
       " -0.609471372432468,\n",
       " -0.8003008685259243,\n",
       " -0.609471372432468,\n",
       " -0.9737329137194711,\n",
       " -0.9491743720972841,\n",
       " -0.6867470979328579,\n",
       " -0.5448229206888258,\n",
       " -0.5582200056835569,\n",
       " -0.484086824719162,\n",
       " -0.48637041770736034,\n",
       " -0.39079480372929254,\n",
       " -0.3784251918445898,\n",
       " -0.3975138679915945,\n",
       " -0.3605514421082888,\n",
       " -0.4071659466708265,\n",
       " -0.34548413798319544,\n",
       " -0.3039803160675356,\n",
       " -0.3581536023179869,\n",
       " -0.3735809280357605,\n",
       " -0.3508606867395536,\n",
       " -0.3247939150394926,\n",
       " -0.9723561954421015,\n",
       " -0.9161880399988074,\n",
       " -0.609471372432468,\n",
       " -0.611711726548649,\n",
       " -0.5690476911102126,\n",
       " -0.5561020762902975,\n",
       " -0.5958395570479408,\n",
       " -0.5721728497568281,\n",
       " -0.6709223179941521,\n",
       " -0.5279865010531806,\n",
       " -0.6776052452552145,\n",
       " -0.5503419577551181,\n",
       " -0.6432958297148546,\n",
       " -0.5591071270498849,\n",
       " -0.6211234367570755,\n",
       " -0.58760462242599,\n",
       " -0.6537706441345413,\n",
       " -0.6502005527716397,\n",
       " -0.8183793668826524,\n",
       " -0.7120480201754127,\n",
       " -0.6273740196639958,\n",
       " -0.743114033926876,\n",
       " -0.9161880399988074,\n",
       " -0.7002981521663654,\n",
       " -0.4891399910617812,\n",
       " -0.437548811506951,\n",
       " -0.3858341780742247,\n",
       " -0.35556808311693955,\n",
       " -0.3123244499800285,\n",
       " -0.35672826069909425,\n",
       " -0.38545093756537097,\n",
       " -0.3854936399739267,\n",
       " -0.3545846184305448,\n",
       " -0.3110839689798133,\n",
       " -0.381305072933911,\n",
       " -0.3527029440020999,\n",
       " -0.3051372457687387,\n",
       " -0.29592554939844606,\n",
       " -0.33353550975099766,\n",
       " -0.3013217945344091,\n",
       " -0.3045589111695127,\n",
       " -0.33224070640881154,\n",
       " -0.8003008685259243,\n",
       " -0.6473033106164479,\n",
       " -0.5745091951812387,\n",
       " -0.5723507790663611,\n",
       " -0.4918593218899119,\n",
       " -0.48182908689658255,\n",
       " -0.47452277781604,\n",
       " -0.47280814222583767,\n",
       " -0.971246129099124,\n",
       " -0.8526927824426866,\n",
       " -0.6910383074412956,\n",
       " -0.5514005028269651,\n",
       " -0.5252630673119063,\n",
       " -0.44456121374041446,\n",
       " -0.39454155376877154,\n",
       " -0.43151275306300296,\n",
       " -0.45633122890053945,\n",
       " -0.3911325973007924,\n",
       " -0.36681458886513585,\n",
       " -0.4208937903308161,\n",
       " -0.30955431939257216,\n",
       " -0.32315383763811956,\n",
       " -0.3268966257970761,\n",
       " -0.32132159277174,\n",
       " -0.36386705609690806,\n",
       " -0.3473867972787331,\n",
       " -0.34525731615798894,\n",
       " -0.36571627676389906,\n",
       " -0.3754453114216685,\n",
       " -0.32235574266340294,\n",
       " -0.3068689256212087,\n",
       " -0.33061871728375825,\n",
       " -0.3420727096509967,\n",
       " -0.3402004072758299,\n",
       " -0.35279269137502467,\n",
       " -0.40092853642184545,\n",
       " -0.3495995426506312,\n",
       " -0.39218670618844,\n",
       " -0.42445524216152075,\n",
       " -0.3721464180380337,\n",
       " -0.35162521003949343,\n",
       " -0.3904991773241028,\n",
       " -0.36773548450749605,\n",
       " -0.3690489556528085,\n",
       " -0.404819221019168,\n",
       " -0.3831891341127409,\n",
       " -0.3949609777822015,\n",
       " -0.3459377534602983,\n",
       " -0.41052609772981474,\n",
       " -0.3716675183984828,\n",
       " -0.37583479156457145,\n",
       " -0.3581092037045672,\n",
       " -0.41244329906254285,\n",
       " -0.43614343605141226,\n",
       " -0.3951287427917278,\n",
       " -0.41268765839796595,\n",
       " -0.4485889423074274,\n",
       " -0.42604875664983,\n",
       " -0.9754000366509938,\n",
       " -0.799120820217921,\n",
       " -0.3721464180380337,\n",
       " -0.35632685804870246,\n",
       " -0.44313484238954204,\n",
       " -0.47572337991572866,\n",
       " -0.3900342763779723,\n",
       " -0.5117473022287173,\n",
       " -0.40320804703681123,\n",
       " -0.3903724916485184,\n",
       " -0.4411628335870774,\n",
       " -0.42141540246391995,\n",
       " -0.4485124788726512,\n",
       " -0.4196077751948244,\n",
       " -0.5271023379047295,\n",
       " -0.37790785483757006,\n",
       " -0.6223033251542214,\n",
       " -0.46027360796105876,\n",
       " -0.4856936324678982,\n",
       " -0.5186693837303386,\n",
       " -0.564030291313779,\n",
       " -0.5450436953927722,\n",
       " -0.4953343456115237,\n",
       " -0.4328116237913707,\n",
       " -0.5466804899376697,\n",
       " -0.44709642983799325,\n",
       " -0.472223185837636,\n",
       " -0.4932662142673331,\n",
       " -0.47017047031021775,\n",
       " -0.387407321089383,\n",
       " -0.6835608180389676,\n",
       " -0.5074301885497954,\n",
       " -0.6474011062317413,\n",
       " -0.6376388906678822,\n",
       " -0.5311494558233694,\n",
       " -0.574803948788436,\n",
       " -0.5854264845439519,\n",
       " -0.7011279386081524,\n",
       " -0.526249289450617,\n",
       " -0.5204343903311371,\n",
       " -0.7102059612798499,\n",
       " -0.48390803551808015,\n",
       " -0.6446776939060632,\n",
       " -0.6827043074226031,\n",
       " -0.6667215107557245,\n",
       " -0.5756577685782879,\n",
       " -0.5836410299737026,\n",
       " -0.5588931768473886,\n",
       " -0.5511516242933936,\n",
       " -0.5380527518383955,\n",
       " -0.9760411907254803,\n",
       " -0.9378929850387657,\n",
       " -0.7368969644893066,\n",
       " -0.5516180872038603,\n",
       " -0.46777803950001917,\n",
       " -0.3927341753459457,\n",
       " -0.4089303724507788,\n",
       " -0.3641316178373577,\n",
       " -0.38280424309199335,\n",
       " -0.34811011488517685,\n",
       " -0.32315383763811956,\n",
       " -0.3109884379140696,\n",
       " -0.799120820217921,\n",
       " -0.50427916801012,\n",
       " -0.48390803551808015,\n",
       " -0.8287471406357046,\n",
       " -0.50427916801012,\n",
       " -0.35574656655455517,\n",
       " -0.4490092143202642,\n",
       " -0.35431608453036834,\n",
       " -0.28693775955365697,\n",
       " -0.32815510780337664,\n",
       " -0.48390803551808015,\n",
       " -0.5256578624974564,\n",
       " -0.32815510780337664,\n",
       " -0.2627120208420013,\n",
       " -0.305329917071419,\n",
       " -0.29607186193555146,\n",
       " -0.31022393120016,\n",
       " -0.2793352389464111,\n",
       " -0.26714489907971783,\n",
       " -0.3306652297508029,\n",
       " -0.8287471406357046,\n",
       " -0.5790560704704621,\n",
       " -0.6223818136701812,\n",
       " -0.42851000750381074,\n",
       " -0.34937414701484537,\n",
       " -0.33790967899666646,\n",
       " -0.3547187411243644,\n",
       " -0.38271865738085464,\n",
       " -0.3200973414959769,\n",
       " -0.28148012160955305,\n",
       " -0.3210392193346283,\n",
       " -0.32950421361711557,\n",
       " -0.29753420763907557,\n",
       " -0.2867399870654861,\n",
       " -0.2815298205404292,\n",
       " -0.2882217188773132,\n",
       " -0.3282482587199903,\n",
       " -0.2741718627442409]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-e614e9720dc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_save\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "plt.plot(range(len(loss_save)), loss_save)\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
